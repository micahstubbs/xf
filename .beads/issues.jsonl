{"id":"xf-10","title":"Test Issue","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T19:03:06.322242-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:03:13.179271-05:00","closed_at":"2026-01-10T19:03:13.179271-05:00","close_reason":"Test issue - deleting"}
{"id":"xf-11","title":"xf UX \u0026 Reliability Improvements","description":"## Overview\n\nThis epic encompasses 5 high-impact improvements to xf identified through systematic analysis of the codebase, user workflows, and feature gaps. These improvements were prioritized based on: (1) filling explicit feature gaps where users encounter 'not implemented' errors, (2) transforming power-user workflows, (3) building reliability/trust, and (4) improving ergonomics.\n\n## Background \u0026 Motivation\n\nxf is a CLI tool for indexing and searching X (Twitter) data archives locally. While core search functionality is excellent (sub-millisecond queries via Tantivy), analysis revealed several key gaps:\n\n1. **DM Context (--context flag)** - CLI flag exists but throws 'not implemented yet' at line 287 of main.rs. Users who download their archive often want to revisit old DM conversations, making this the #1 most requested missing feature.\n\n2. **Detailed Stats (--detailed, --hashtags, --mentions flags)** - CLI flags exist but throw 'not implemented yet' at line 629. Users are curious about their posting patterns when downloading years of Twitter data.\n\n3. **Interactive Mode** - Power users need to run multiple searches iteratively. Currently each search requires typing the full 'xf search \"query\"' command. A REPL mode would dramatically improve this workflow.\n\n4. **Health Checks** - No way to verify database/index integrity. Users need confidence that their indexed data is healthy, especially before relying on search results for important lookups.\n\n5. **Natural Language Dates** - --since and --until require ISO format (YYYY-MM-DD) which is unfriendly. Users naturally want to say 'since last month' or 'from december 2023'.\n\n## Technical Context\n\n- **Search engine**: Tantivy (Lucene-like, Rust)\n- **Storage**: SQLite with FTS5 virtual tables\n- **CLI**: clap with derive macros\n- **Parsing**: serde_json for JavaScript-wrapped JSON (window.YTD.*)\n- **Parallelism**: rayon for archive parsing\n\n## Success Criteria\n\n- All 5 features implemented with comprehensive tests\n- No regressions in existing functionality\n- All quality gates pass: cargo check, clippy, fmt, test\n- Documentation updated (help text, README if needed)\n- Each feature fills an explicit gap or provides obvious user value\n\n## Priority Rationale\n\nP2 (medium) because these are enhancements rather than critical bugs. However, features #1 and #2 fill explicit gaps where users encounter errors, making them high-value.\n\n## Child Epics (to be created)\n\n- DM Conversation Viewer (priority #1 - fills biggest functional gap)\n- Enhanced Stats Dashboard (priority #2 - fills explicit gap, differentiating feature)\n- Interactive REPL Mode (priority #3 - biggest workflow improvement)\n- xf doctor Health Check (priority #4 - reliability/trust)\n- Natural Language Date Filtering (priority #5 - ergonomic improvement)","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-10T19:03:43.51052-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:03:43.51052-05:00"}
{"id":"xf-11.1","title":"DM Conversation Viewer (--context flag)","description":"## Overview\n\nImplement the --context flag for DM search results to display full conversation context around matched messages. This is the #1 most impactful improvement identified because it fills the single biggest functional gap in xf.\n\n## Background\n\nWhen users download their X archive, one of the primary use cases is revisiting old DM conversations. Currently:\n\n- The --context flag exists in cli.rs (SearchArgs, line 151)\n- Using it throws: 'DM context output is not implemented yet' (main.rs line 287)\n- DM search results show isolated messages without conversation context\n- The data model already has conversation_id linking messages\n\nThis is the most requested missing feature because searching for a DM and finding a single message is nearly useless without seeing the surrounding conversation.\n\n## Current State\n\n**cli.rs (lines 149-151)**:\n```rust\n/// Include surrounding context (for DMs)\n#[arg(long, short = 'c')]\npub context: bool,\n```\n\n**main.rs (lines 286-288)**:\n```rust\nif args.context {\n    anyhow::bail!(\"DM context output is not implemented yet.\");\n}\n```\n\n**Data Model (storage.rs)**:\n- direct_messages table has conversation_id column\n- DirectMessage model exists but doesn't carry conversation_id (noted in xf-9rf.4)\n\n## Implementation Approach\n\n1. **Storage Layer**: Add get_conversation_messages(conversation_id) method to fetch all messages in a conversation, ordered by timestamp\n\n2. **Search Integration**: When --context is used with --types dm:\n   - Perform normal search to find matching messages\n   - For each result, extract conversation_id from metadata\n   - Fetch full conversation context\n   - Display with matched message highlighted\n\n3. **Display Format**: Show conversation in natural reading order:\n```\nConversation with @friend (2023-12-15 to 2023-12-15):\n  [14:32] friend: Hey, want to grab dinner?\n  [14:35] you: Sure! When were you thinking?  â† matched\n  [14:36] friend: How about 7pm?\n  [14:40] you: Perfect, see you then\n```\n\n## Technical Considerations\n\n1. **DirectMessage model**: May need to add conversation_id field, or retrieve it from stored metadata JSON\n2. **Performance**: Fetching full conversations for many results could be slow. Consider limiting context to N messages before/after match\n3. **Output formats**: Need to handle --format json, text, etc. JSON should include conversation structure\n4. **Multiple matches in same conversation**: Deduplicate to show conversation once with all matches highlighted\n\n## Acceptance Criteria\n\n- [ ] xf search \"query\" --types dm --context shows full conversation context\n- [ ] Matched message is visually highlighted in text output\n- [ ] JSON output includes structured conversation with match indicators\n- [ ] Performance is acceptable (\u003c 100ms for typical results)\n- [ ] Tests cover various scenarios (single match, multiple matches, empty conversations)\n- [ ] Help text updated to explain context behavior\n\n## Dependencies\n\n- None (this is a self-contained feature)\n\n## Child Tasks (to be created)\n\n1. Add get_conversation_messages to storage.rs\n2. Update DirectMessage handling for conversation_id\n3. Implement context display in cmd_search\n4. Add context formatting for different output modes\n5. Add tests for DM context functionality\n6. Update CLI help text","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-10T19:04:10.48158-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:04:10.48158-05:00","dependencies":[{"issue_id":"xf-11.1","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.1","title":"Add get_conversation_messages to storage.rs","description":"## Goal\n\nAdd a new method to Storage that retrieves all messages in a DM conversation by conversation_id, ordered by timestamp.\n\n## Implementation\n\nAdd to storage.rs in the Storage impl block (after get_all_dms around line 1084):\n\n```rust\n/// Get all messages in a specific DM conversation.\n///\n/// Returns messages ordered by creation timestamp (oldest first).\n///\n/// # Arguments\n/// * `conversation_id` - The conversation ID to fetch messages for\n///\n/// # Errors\n/// Returns an error if the database query fails.\npub fn get_conversation_messages(\u0026self, conversation_id: \u0026str) -\u003e Result\u003cVec\u003cDirectMessage\u003e\u003e {\n    let query = r\"\n        SELECT id, conversation_id, sender_id, recipient_id, text,\n               created_at, urls_json, media_urls_json\n        FROM direct_messages \n        WHERE conversation_id = ?\n        ORDER BY created_at ASC\n    \";\n    \n    let mut stmt = self.conn.prepare(query)?;\n    let messages = stmt\n        .query_map([conversation_id], |row| {\n            Ok(DirectMessage {\n                id: row.get(0)?,\n                sender_id: row.get(2)?,\n                recipient_id: row.get(3)?,\n                text: row.get(4)?,\n                created_at: row\n                    .get::\u003c_, String\u003e(5)\n                    .ok()\n                    .and_then(|s| DateTime::parse_from_rfc3339(\u0026s).ok())\n                    .map_or_else(Utc::now, |dt| dt.with_timezone(\u0026Utc)),\n                urls: serde_json::from_str(\u0026row.get::\u003c_, String\u003e(6)?).unwrap_or_default(),\n                media_urls: serde_json::from_str(\u0026row.get::\u003c_, String\u003e(7)?).unwrap_or_default(),\n            })\n        })?\n        .filter_map(std::result::Result::ok)\n        .collect();\n\n    Ok(messages)\n}\n```\n\n## Key Points\n\n1. **Column selection**: Includes conversation_id (column 1) but we don't store it in DirectMessage struct currently. This is noted - parent epic may need to update the model.\n\n2. **Ordering**: ASC by created_at so messages appear in chronological order (reading order).\n\n3. **Pattern**: Follows existing get_all_* methods pattern for consistency.\n\n4. **Error handling**: Uses filter_map like other methods to skip any malformed rows.\n\n## Testing\n\nAdd test in storage.rs tests module:\n\n```rust\n#[test]\nfn test_get_conversation_messages() {\n    let storage = create_test_storage();\n    // Setup: store a conversation with multiple messages\n    let conv = DmConversation {\n        conversation_id: \"conv-123\".to_string(),\n        messages: vec\\![\n            create_test_dm(\"msg-1\", \"Hello\"),\n            create_test_dm(\"msg-2\", \"Hi there\"),\n            create_test_dm(\"msg-3\", \"How are you?\"),\n        ],\n    };\n    storage.store_dm_conversations(\u0026[conv]).unwrap();\n    \n    // Test: fetch by conversation_id\n    let messages = storage.get_conversation_messages(\"conv-123\").unwrap();\n    assert_eq\\!(messages.len(), 3);\n    assert_eq\\!(messages[0].text, \"Hello\");\n    \n    // Test: nonexistent conversation returns empty\n    let empty = storage.get_conversation_messages(\"nonexistent\").unwrap();\n    assert\\!(empty.is_empty());\n}\n```\n\n## Acceptance Criteria\n\n- [ ] Method added to Storage impl\n- [ ] Returns messages in chronological order\n- [ ] Handles nonexistent conversation_id gracefully (empty vec)\n- [ ] Test added and passing\n- [ ] cargo check, clippy pass","notes":"Started implementation of --context DM viewer: added get_conversation_messages to storage.rs and context output pipeline in cmd_search; added conversation context structs, grouping by conversation_id, and text/JSON rendering; added unit test for get_conversation_messages; fixed stats JSON optional fields and filtered empty top_counts entries.","status":"in_progress","priority":1,"issue_type":"task","assignee":"SageDune","created_at":"2026-01-10T19:04:32.991463-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:09:56.389530922-05:00","dependencies":[{"issue_id":"xf-11.1.1","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.2","title":"Implement DM context display in cmd_search","description":"## Goal\n\nRemove the 'not implemented' error and implement proper DM context display when --context flag is used with DM search.\n\n## Current Code (main.rs lines 286-288)\n\n```rust\nif args.context {\n    anyhow::bail\\!(\"DM context output is not implemented yet.\");\n}\n```\n\n## Implementation\n\nReplace the bail\\! with actual context handling logic. The implementation should:\n\n1. **Detect when context applies**: Only when --types includes 'dm' and --context is set\n\n2. **Extract conversation_id from results**: Each DM search result's metadata JSON contains conversation_id\n\n3. **Fetch conversation context**: Use the new get_conversation_messages method\n\n4. **Display with highlighting**: Show full conversation with the matched message highlighted\n\n## Pseudocode\n\n```rust\n// In cmd_search, after getting search results:\nif args.context \u0026\u0026 args.types.as_ref().map_or(false, |t| t.iter().any(|x| matches\\!(x, DataType::Dm))) {\n    // Group results by conversation_id\n    let mut conversations_shown: HashSet\u003cString\u003e = HashSet::new();\n    \n    for result in \u0026results {\n        if result.result_type \\!= SearchResultType::DirectMessage {\n            continue;\n        }\n        \n        // Extract conversation_id from metadata\n        let conv_id = result.metadata[\"conversation_id\"].as_str().unwrap_or_default();\n        \n        // Skip if we already showed this conversation\n        if conversations_shown.contains(conv_id) {\n            continue;\n        }\n        conversations_shown.insert(conv_id.to_string());\n        \n        // Fetch full conversation\n        let messages = storage.get_conversation_messages(conv_id)?;\n        \n        // Find matched message IDs in this conversation\n        let matched_ids: HashSet\u003c_\u003e = results.iter()\n            .filter(|r| r.metadata[\"conversation_id\"].as_str() == Some(conv_id))\n            .map(|r| r.id.as_str())\n            .collect();\n        \n        // Display conversation with highlights\n        display_dm_conversation(\u0026messages, \u0026matched_ids, \u0026cli.format);\n    }\n} else {\n    // Normal result display\n    for result in \u0026results {\n        print_result(\u0026result, \u0026cli.format, ...);\n    }\n}\n```\n\n## Display Format (text mode)\n\n```\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nConversation (2023-12-15)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n  [14:32] 12345678: Hey, want to grab dinner?\nâ–º [14:35] 87654321: Sure\\! When were you thinking?\n  [14:36] 12345678: How about 7pm?\n  [14:40] 87654321: Perfect, see you then\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n```\n\nThe â–º indicates matched messages, and the line should be colored (cyan or yellow).\n\n## Edge Cases\n\n1. **No DM types in search**: If --context is used without --types dm, show warning or ignore\n2. **Multiple matches in conversation**: Show conversation once with all matches highlighted\n3. **Empty conversations**: Skip display\n4. **Very long conversations**: Consider limiting to N messages around matches\n\n## Dependencies\n\n- xf-11.1.1: get_conversation_messages must exist first\n\n## Acceptance Criteria\n\n- [ ] --context no longer throws 'not implemented' error\n- [ ] DM results show full conversation context\n- [ ] Matched messages are visually highlighted\n- [ ] Conversations are deduplicated (shown once even if multiple matches)\n- [ ] Works with all output formats (text, json, compact, csv)","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T19:05:01.194389-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:05:01.194389-05:00","dependencies":[{"issue_id":"xf-11.1.2","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.2","depends_on_id":"xf-11.1.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.3","title":"Add unit tests for DM context functionality","description":"## Goal\n\nComprehensive test coverage for the DM context feature to ensure reliability and catch regressions.\n\n## Unit Tests (storage.rs)\n\n### Test: get_conversation_messages_returns_chronological_order\n- Create conversation with 5 messages, store in random order\n- Verify returned messages are sorted by created_at ASC\n- Assert first message has earliest timestamp\n\n### Test: get_conversation_messages_empty_conversation\n- Query nonexistent conversation_id\n- Verify empty Vec is returned (not error)\n\n### Test: get_conversation_messages_single_message\n- Create conversation with exactly one message\n- Verify single message returned correctly\n\n### Test: get_conversation_messages_preserves_all_fields\n- Store message with URLs, media, full metadata\n- Retrieve and verify all fields preserved\n\n## Integration Tests (cmd_search context)\n\n### Test: context_flag_with_dm_search\n- Index test archive with known DM conversations\n- Run: xf search 'keyword' --types dm --context\n- Verify full conversation displayed, matched message highlighted\n\n### Test: context_flag_without_dm_type_ignored\n- Run: xf search 'keyword' --context (no --types dm)\n- Verify context flag has no effect on non-DM results\n\n### Test: context_deduplicates_conversations\n- Create conversation with 3 messages containing search term\n- Search should show conversation once, all 3 messages marked as matched\n\n### Test: context_output_formats\n- Test --context with --format json\n- Verify JSON includes conversation structure with match indicators\n- Test --context with --format csv\n- Verify reasonable CSV output (or appropriate error)\n\n## E2E Test Script (tests/e2e/dm_context_test.sh)\n\n```bash\n#\\!/bin/bash\nset -euo pipefail\n\n# E2E test for DM conversation context feature\n# Requires: test archive at ./test-archives/dm-test/\n\nLOG_FILE=\"./test-output/dm_context_e2e_$(date +%Y%m%d_%H%M%S).log\"\nmkdir -p ./test-output\n\nlog() { echo \"[$(date +%H:%M:%S)] $*\" | tee -a \"$LOG_FILE\"; }\n\nlog \"=== DM Context E2E Test Suite ===\"\nlog \"Testing xf binary: $(which xf)\"\nlog \"Archive: ./test-archives/dm-test/\"\n\n# Test 1: Basic context display\nlog \"[TEST 1] Basic --context flag\"\nOUTPUT=$(xf search \"meeting\" --types dm --context 2\u003e\u00261)\nif echo \"$OUTPUT\" | grep -q \"Conversation\"; then\n    log \"[PASS] Context header displayed\"\nelse\n    log \"[FAIL] No conversation context shown\"\n    exit 1\nfi\n\n# Test 2: JSON format with context\nlog \"[TEST 2] JSON format with context\"\nOUTPUT=$(xf search \"meeting\" --types dm --context --format json 2\u003e\u00261)\nif echo \"$OUTPUT\" | jq -e '.conversations' \u003e /dev/null 2\u003e\u00261; then\n    log \"[PASS] JSON contains conversations array\"\nelse\n    log \"[FAIL] JSON missing conversations structure\"\n    exit 1\nfi\n\n# Test 3: Multiple matches in same conversation\nlog \"[TEST 3] Deduplication of conversations\"\n# ... additional tests\n\nlog \"=== All E2E tests passed ===\"\n```\n\n## Logging Requirements\n\n- All test functions should use tracing::debug\\! for setup steps\n- Log conversation_id and message counts during test execution\n- On failure, dump full conversation state to help debugging\n- Test runner should output timing information\n\n## Acceptance Criteria\n\n- [ ] All unit tests pass (cargo test)\n- [ ] Integration tests cover all output formats\n- [ ] E2E script runs successfully on fresh test archive\n- [ ] Tests are documented in tests/README.md\n- [ ] No regressions in existing test suite","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T19:06:59.983466-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:22:47.659198-05:00","dependencies":[{"issue_id":"xf-11.1.3","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.3","depends_on_id":"xf-11.1.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.3","depends_on_id":"xf-11.1.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.4","title":"Update CLI help text and README for --context flag","description":"## Goal\n\nUpdate CLI help text and README to document the --context flag for DM searches.\n\n## CLI Help Updates (cli.rs)\n\n### SearchArgs context field (line ~151)\nCurrent:\n```rust\n/// Include surrounding context (for DMs)\n#[arg(long, short = 'c')]\npub context: bool,\n```\n\nUpdated:\n```rust\n/// Show full conversation context for DM results.\n/// When searching DMs, displays the complete conversation thread\n/// with matched messages highlighted. Use with --types dm.\n/// Example: xf search \"meeting\" --types dm --context\n#[arg(long, short = 'c')]\npub context: bool,\n```\n\n### xf search --help output\nShould include usage example:\n```\nOPTIONS:\n    -c, --context\n            Show full conversation context for DM results.\n            Displays complete conversation threads with matched\n            messages highlighted.\n\n            Example:\n                xf search \"dinner\" --types dm --context\n```\n\n## README.md Updates\n\n### Features section\nAdd to DM search capabilities:\n```markdown\n- **DM Conversation Context**: Use `--context` to view full conversation\n  threads around matched messages, with highlights\n```\n\n### Usage Examples section\nAdd new example:\n```markdown\n### View DM Conversation Context\n\nWhen searching DMs, use `--context` to see the full conversation:\n\n```bash\n# Search for a keyword and show surrounding conversation\nxf search \"vacation plans\" --types dm --context\n\n# Output shows the full conversation with matched messages marked:\n# Conversation with @friend (2023-07-15)\n#   [10:32] friend: Hey, want to plan that trip?\n# â–º [10:35] you: Yes\\! Let's discuss vacation plans\n#   [10:36] friend: How about Hawaii?\n```\n```\n\n### JSON Output Documentation\nDocument the conversation structure in JSON format:\n```markdown\nWhen using `--context` with `--format json`, output includes:\n```json\n{\n  \"conversations\": [\n    {\n      \"conversation_id\": \"123-456\",\n      \"participants\": [\"12345\", \"67890\"],\n      \"messages\": [\n        {\"id\": \"msg1\", \"text\": \"...\", \"matched\": false},\n        {\"id\": \"msg2\", \"text\": \"...\", \"matched\": true},\n        ...\n      ]\n    }\n  ]\n}\n```\n```\n\n## Acceptance Criteria\n\n- [ ] xf search --help shows --context with clear description\n- [ ] README.md includes DM context example\n- [ ] JSON output format documented\n- [ ] Help text fits within 80 columns for readability","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:07:06.472161-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:23:07.154321-05:00","dependencies":[{"issue_id":"xf-11.1.4","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.4","depends_on_id":"xf-11.1.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2","title":"Enhanced Stats Dashboard (--detailed flag)","description":"## Overview\n\nAdd a --detailed flag to `xf stats` that shows comprehensive archive analytics including temporal patterns, engagement distributions, content type breakdowns, and user interaction metrics. Transforms the basic stats command into a powerful analytics dashboard.\n\n## Background\n\nThe current `xf stats` command shows basic counts:\n- Total tweets, likes, DMs, followers, following\n- Date range of archive\n\nUsers want deeper insights when reviewing years of Twitter data:\n- When was I most active?\n- What content performed best?\n- Who do I interact with most?\n- What are my posting patterns?\n\n## Current State (main.rs cmd_stats)\n\nThe stats command currently:\n- Queries SQLite for row counts\n- Displays in simple text or JSON format\n- Has --hashtags, --mentions, --detailed flags that throw 'not implemented'\n\n## Implementation Architecture\n\n### New Module: stats_analytics.rs\n\nCreate dedicated module for analytics calculations:\n\n```rust\npub struct TemporalStats {\n    pub tweets_per_day: Vec\u003c(NaiveDate, u64)\u003e,\n    pub tweets_per_hour: [u64; 24],\n    pub tweets_per_dow: [u64; 7],\n    pub longest_gap: Duration,\n    pub most_active_day: NaiveDate,\n    pub most_active_hour: u8,\n}\n\npub struct EngagementStats {\n    pub likes_histogram: Vec\u003c(u64, u64)\u003e,  // (bucket, count)\n    pub top_tweets: Vec\u003cTweetEngagement\u003e,\n    pub avg_engagement: f64,\n    pub total_likes_received: u64,\n    pub total_retweets_received: u64,\n}\n\npub struct ContentStats {\n    pub media_ratio: f64,\n    pub thread_count: u64,\n    pub top_hashtags: Vec\u003c(String, u64)\u003e,\n    pub top_mentions: Vec\u003c(String, u64)\u003e,\n    pub avg_tweet_length: f64,\n}\n\npub fn compute_temporal_stats(storage: \u0026Storage) -\u003e Result\u003cTemporalStats\u003e;\npub fn compute_engagement_stats(storage: \u0026Storage) -\u003e Result\u003cEngagementStats\u003e;\npub fn compute_content_stats(storage: \u0026Storage) -\u003e Result\u003cContentStats\u003e;\n```\n\n### Output Format\n\nText output with colored sections and ASCII sparklines:\n\n```\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                    ARCHIVE ANALYTICS\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nğŸ“Š TEMPORAL PATTERNS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nActivity Trend (12 months):  â–â–‚â–ƒâ–…â–‡â–ˆâ–†â–„â–ƒâ–‚â–â–\nMost Active Day: Saturday\nMost Active Hour: 9 PM (21:00)\nLongest Gap: 45 days (2023-08-15 to 2023-09-29)\nTweets/Day Average: 3.2\n\nğŸ“ˆ ENGAGEMENT\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTotal Likes Received: 15,432\nTotal Retweets: 1,234\nAvg Engagement/Tweet: 12.3\n\nTop Performing Tweets:\n1. [523 likes] \"My hot take on...\" (2023-05-12)\n2. [412 likes] \"Thread about...\" (2023-03-01)\n3. [389 likes] \"Unpopular opinion:...\" (2023-07-22)\n\nğŸ“ CONTENT BREAKDOWN\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nText-only: 65% | With Media: 35%\nThreads: 142 | Standalone: 4,523\nAvg Tweet Length: 187 chars\n\nTop Hashtags: #rust (45), #programming (32), #tech (28)\nTop Mentions: @friend (156), @colleague (89), @brand (45)\n```\n\n## Performance Requirements\n\n- All queries must complete in \u003c 2 seconds on 100k tweet archives\n- Use SQL aggregations, not Rust iteration\n- Cache expensive calculations if repeated\n- Show progress indicator for large archives\n\n## Dependencies\n\n- None (uses existing Storage and SQLite)\n\n## Child Tasks\n\n1. Add temporal analytics to stats command (SQL + sparklines)\n2. Add engagement analytics to stats command (histograms + top tweets)\n3. Add content analysis to stats command (ratios + top lists)\n4. Add --detailed flag and output formatting\n5. Add unit and integration tests\n6. Add E2E test script with performance benchmarks","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-10T19:07:22.685312-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:23:48.918856-05:00","dependencies":[{"issue_id":"xf-11.2","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.1","title":"Add temporal analytics to stats command","description":"## Goal\n\nAdd temporal analytics showing when and how frequently the user posted.\n\n## Implementation\n\n### New Functions in stats_analytics.rs\n\n```rust\nuse chrono::{NaiveDate, Weekday, Duration, Timelike};\n\n/// Activity statistics over time\npub struct TemporalStats {\n    /// Tweets per day for the entire archive period\n    pub daily_counts: Vec\u003c(NaiveDate, u64)\u003e,\n    /// Tweets per hour of day (0-23), aggregated\n    pub hourly_distribution: [u64; 24],\n    /// Tweets per day of week (Mon=0, Sun=6)\n    pub dow_distribution: [u64; 7],\n    /// Longest period with no tweets\n    pub longest_gap: Duration,\n    /// The gap's start and end dates\n    pub longest_gap_range: (NaiveDate, NaiveDate),\n    /// Day with most tweets\n    pub most_active_day: (NaiveDate, u64),\n    /// Hour with most tweets overall\n    pub most_active_hour: (u8, u64),\n    /// Average tweets per day (excluding zero days)\n    pub avg_tweets_per_active_day: f64,\n}\n\npub fn compute_temporal_stats(storage: \u0026Storage) -\u003e Result\u003cTemporalStats\u003e {\n    // Use SQL for efficiency:\n    // SELECT DATE(created_at) as day, COUNT(*) as count\n    // FROM tweets GROUP BY day ORDER BY day\n    \n    // For hourly: \n    // SELECT CAST(strftime('%H', created_at) AS INTEGER) as hour, COUNT(*)\n    // FROM tweets GROUP BY hour\n    \n    // For DOW:\n    // SELECT CAST(strftime('%w', created_at) AS INTEGER) as dow, COUNT(*)\n    // FROM tweets GROUP BY dow\n}\n```\n\n### Sparkline Generation\n\n```rust\n/// Generate ASCII sparkline from values\n/// Uses: â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ\npub fn sparkline(values: \u0026[u64], width: usize) -\u003e String {\n    let blocks = ['â–', 'â–‚', 'â–ƒ', 'â–„', 'â–…', 'â–†', 'â–‡', 'â–ˆ'];\n    let max = *values.iter().max().unwrap_or(\u00261);\n    \n    // Bucket values into 'width' buckets\n    let bucket_size = (values.len() + width - 1) / width;\n    let buckets: Vec\u003cu64\u003e = values\n        .chunks(bucket_size)\n        .map(|chunk| chunk.iter().sum::\u003cu64\u003e() / chunk.len() as u64)\n        .collect();\n    \n    buckets.iter()\n        .map(|\u0026v| {\n            let idx = ((v as f64 / max as f64) * 7.0) as usize;\n            blocks[idx.min(7)]\n        })\n        .collect()\n}\n```\n\n### Gap Detection\n\n```rust\nfn find_longest_gap(daily_counts: \u0026[(NaiveDate, u64)]) -\u003e (Duration, NaiveDate, NaiveDate) {\n    let mut max_gap = Duration::zero();\n    let mut gap_start = daily_counts[0].0;\n    let mut gap_end = daily_counts[0].0;\n    \n    for window in daily_counts.windows(2) {\n        let gap = window[1].0 - window[0].0;\n        if gap \u003e max_gap {\n            max_gap = gap;\n            gap_start = window[0].0;\n            gap_end = window[1].0;\n        }\n    }\n    (max_gap, gap_start, gap_end)\n}\n```\n\n## SQL Queries Required\n\n```sql\n-- Daily counts\nSELECT DATE(created_at) as day, COUNT(*) as count\nFROM tweets \nWHERE created_at IS NOT NULL\nGROUP BY day \nORDER BY day;\n\n-- Hourly distribution\nSELECT CAST(strftime('%H', created_at) AS INTEGER) as hour, COUNT(*) as count\nFROM tweets\nWHERE created_at IS NOT NULL\nGROUP BY hour\nORDER BY hour;\n\n-- Day of week distribution  \nSELECT CAST(strftime('%w', created_at) AS INTEGER) as dow, COUNT(*) as count\nFROM tweets\nWHERE created_at IS NOT NULL\nGROUP BY dow\nORDER BY dow;\n```\n\n## Logging\n\n- Log query execution times with tracing::debug!\n- Log total tweets processed\n- Log when sparkline is generated\n\n## Acceptance Criteria\n\n- [ ] TemporalStats struct defined in stats_analytics.rs\n- [ ] compute_temporal_stats function implemented\n- [ ] Sparkline generation works for various data ranges\n- [ ] Gap detection handles edge cases (single tweet, no tweets)\n- [ ] All SQL queries execute in \u003c 500ms on 100k tweets\n- [ ] Unit tests for sparkline and gap detection","status":"closed","priority":1,"issue_type":"task","assignee":"StormyLantern","created_at":"2026-01-10T19:08:11.877953-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:54:12.699305-05:00","closed_at":"2026-01-10T19:54:12.699305-05:00","close_reason":"Implemented temporal analytics with SQL aggregations, sparklines, gap detection, and CLI integration. All tests pass.","dependencies":[{"issue_id":"xf-11.2.1","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.2","title":"Add engagement analytics to stats command","description":"## Goal\n\nAdd engagement analytics showing how the user's tweets performed.\n\n## Implementation\n\n### New Structs in stats_analytics.rs\n\n```rust\n/// Engagement metrics for the archive\npub struct EngagementStats {\n    /// Histogram of likes: (min_likes, max_likes, count)\n    pub likes_histogram: Vec\u003cLikesBucket\u003e,\n    /// Top N tweets by total engagement (likes + retweets)\n    pub top_tweets: Vec\u003cTopTweet\u003e,\n    /// Average engagement per tweet\n    pub avg_engagement: f64,\n    /// Median engagement\n    pub median_engagement: u64,\n    /// Total likes received across all tweets\n    pub total_likes: u64,\n    /// Total retweets received\n    pub total_retweets: u64,\n    /// Engagement trend over time (monthly averages)\n    pub monthly_trend: Vec\u003c(String, f64)\u003e,  // (YYYY-MM, avg_engagement)\n}\n\n#[derive(Debug)]\npub struct LikesBucket {\n    pub range: (u64, u64),  // (min, max) inclusive\n    pub count: u64,\n}\n\n#[derive(Debug)]\npub struct TopTweet {\n    pub id: String,\n    pub text_preview: String,  // First 50 chars\n    pub created_at: DateTime\u003cUtc\u003e,\n    pub likes: u64,\n    pub retweets: u64,\n    pub total_engagement: u64,\n}\n```\n\n### Histogram Bucketing\n\n```rust\nfn compute_likes_histogram(storage: \u0026Storage) -\u003e Result\u003cVec\u003cLikesBucket\u003e\u003e {\n    // Buckets: 0, 1-5, 6-10, 11-25, 26-50, 51-100, 101-500, 500+\n    let bucket_ranges = [\n        (0, 0), (1, 5), (6, 10), (11, 25), \n        (26, 50), (51, 100), (101, 500), (501, u64::MAX)\n    ];\n    \n    // SQL: SELECT favorite_count, COUNT(*) FROM tweets GROUP BY \n    //      CASE WHEN favorite_count = 0 THEN 0\n    //           WHEN favorite_count \u003c= 5 THEN 1 ... END\n}\n```\n\n### Top Tweets Query\n\n```sql\nSELECT id, full_text, created_at, favorite_count, retweet_count,\n       (favorite_count + retweet_count) as total_engagement\nFROM tweets\nWHERE favorite_count IS NOT NULL\nORDER BY total_engagement DESC\nLIMIT 10;\n```\n\n### Engagement Trend\n\n```sql\nSELECT strftime('%Y-%m', created_at) as month,\n       AVG(favorite_count + retweet_count) as avg_engagement\nFROM tweets\nWHERE created_at IS NOT NULL\nGROUP BY month\nORDER BY month;\n```\n\n## Display Format\n\n```\nğŸ“ˆ ENGAGEMENT ANALYTICS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTotal Likes: 15,432 | Total Retweets: 1,234\nAverage per Tweet: 12.3 | Median: 3\n\nLikes Distribution:\n  0     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45%\n  1-5   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 28%\n  6-10  â–ˆâ–ˆâ–ˆâ–ˆ 9%\n  11-25 â–ˆâ–ˆâ–ˆ 7%\n  26-50 â–ˆâ–ˆ 5%\n  50+   â–ˆ 6%\n\nTrend (12mo): â–â–‚â–ƒâ–…â–‡â–ˆâ–†â–„â–ƒâ–‚â–â–\n\nTop Performing:\n1. [523 â¤ï¸ 45 ğŸ”] \"My hot take on...\" (May 12)\n2. [412 â¤ï¸ 23 ğŸ”] \"Thread about...\" (Mar 1)\n```\n\n## Acceptance Criteria\n\n- [ ] EngagementStats struct implemented\n- [ ] Histogram buckets are intuitive and meaningful\n- [ ] Top tweets include preview text (truncated)\n- [ ] Trend sparkline shows monthly patterns\n- [ ] Handles tweets with NULL engagement values\n- [ ] Performance \u003c 500ms on 100k tweets","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:08:12.244568-05:00","created_by":"jemanuel","updated_at":"2026-01-10T20:35:58.174246-05:00","closed_at":"2026-01-10T20:35:58.174246-05:00","close_reason":"Implemented engagement analytics with histogram, top tweets, trends sparkline, and CLI integration. All tests pass.","dependencies":[{"issue_id":"xf-11.2.2","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.3","title":"Add content analysis to stats command","description":"## Goal\n\nAdd content analysis showing what types of content the user posts and who they interact with.\n\n## Implementation\n\n### New Structs in stats_analytics.rs\n\n```rust\n/// Content breakdown and interaction patterns\npub struct ContentStats {\n    /// Percentage of tweets with media attachments\n    pub media_ratio: f64,\n    /// Number of tweets that are part of threads\n    pub thread_count: u64,\n    /// Number of standalone tweets\n    pub standalone_count: u64,\n    /// Top hashtags with counts\n    pub top_hashtags: Vec\u003c(String, u64)\u003e,\n    /// Top mentioned users with counts\n    pub top_mentions: Vec\u003c(String, u64)\u003e,\n    /// Average tweet length in characters\n    pub avg_tweet_length: f64,\n    /// Distribution of tweet lengths (buckets)\n    pub length_distribution: Vec\u003c(String, u64)\u003e,  // (\"0-50\", count)\n    /// Tweets with links vs without\n    pub link_ratio: f64,\n    /// Reply ratio (tweets that are replies vs original)\n    pub reply_ratio: f64,\n}\n```\n\n### Hashtag/Mention Extraction\n\nOption A: Parse from stored JSON (if available)\n```rust\n// If entities JSON is stored in metadata column\nfn extract_hashtags_from_metadata(storage: \u0026Storage) -\u003e Result\u003cVec\u003c(String, u64)\u003e\u003e {\n    let rows = storage.conn.prepare(\"SELECT metadata FROM tweets\")?;\n    let mut counts: HashMap\u003cString, u64\u003e = HashMap::new();\n    for row in rows {\n        if let Ok(meta) = serde_json::from_str::\u003cValue\u003e(\u0026row.get::\u003c_, String\u003e(0)?) {\n            if let Some(hashtags) = meta[\"entities\"][\"hashtags\"].as_array() {\n                for ht in hashtags {\n                    if let Some(text) = ht[\"text\"].as_str() {\n                        *counts.entry(text.to_lowercase()).or_default() += 1;\n                    }\n                }\n            }\n        }\n    }\n    // Sort by count, take top 20\n    let mut sorted: Vec\u003c_\u003e = counts.into_iter().collect();\n    sorted.sort_by(|a, b| b.1.cmp(\u0026a.1));\n    Ok(sorted.into_iter().take(20).collect())\n}\n```\n\nOption B: Parse from tweet text using regex\n```rust\nuse regex::Regex;\n\nfn extract_hashtags_from_text(storage: \u0026Storage) -\u003e Result\u003cVec\u003c(String, u64)\u003e\u003e {\n    let hashtag_re = Regex::new(r\"#(\\w+)\")?;\n    let mut counts: HashMap\u003cString, u64\u003e = HashMap::new();\n    \n    let mut stmt = storage.conn.prepare(\"SELECT full_text FROM tweets\")?;\n    for row in stmt.query_map([], |r| r.get::\u003c_, String\u003e(0))? {\n        if let Ok(text) = row {\n            for cap in hashtag_re.captures_iter(\u0026text) {\n                let tag = cap[1].to_lowercase();\n                *counts.entry(tag).or_default() += 1;\n            }\n        }\n    }\n    // Sort and return top 20\n}\n```\n\n### Thread Detection\n\n```sql\n-- Threads are identified by in_reply_to_user_id matching own user_id\n-- and in_reply_to_status_id being non-null\nSELECT COUNT(*) FROM tweets\nWHERE in_reply_to_user_id = (SELECT user_id FROM account_info LIMIT 1)\n  AND in_reply_to_status_id IS NOT NULL;\n```\n\n### Media Detection\n\n```sql\n-- Count tweets with media\nSELECT COUNT(*) FROM tweets\nWHERE media_urls_json IS NOT NULL \n  AND media_urls_json != '[]';\n```\n\n## Display Format\n\n```\nğŸ“ CONTENT ANALYSIS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nContent Type:\n  Text Only:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 65%\n  With Media:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35%\n  With Links:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 22%\n\nTweet Type:\n  Original:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 72%\n  Replies:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 25%\n  Threads:     â–ˆ 3% (142 threads)\n\nAvg Length: 187 chars\nLength Distribution:\n  0-50:   â–ˆâ–ˆ 8%\n  51-140: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45%\n  141-280: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 42%\n  280+:   â–ˆ 5%\n\nTop Hashtags:\n  #rust (156)  #programming (89)  #tech (45)\n  #ai (34)     #opensource (28)   #webdev (22)\n\nTop Mentions:\n  @friend (156)  @colleague (89)  @brand (45)\n```\n\n## Acceptance Criteria\n\n- [ ] ContentStats struct implemented\n- [ ] Hashtag extraction works (choose Option A or B based on data)\n- [ ] Mention extraction works\n- [ ] Thread detection is accurate\n- [ ] Media/link ratios computed correctly\n- [ ] Top lists limited to 20 items\n- [ ] Performance \u003c 1s on 100k tweets","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:08:12.592229-05:00","created_by":"jemanuel","updated_at":"2026-01-10T20:43:51.924873-05:00","closed_at":"2026-01-10T20:43:51.924873-05:00","close_reason":"Closed","dependencies":[{"issue_id":"xf-11.2.3","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.4","title":"Add --detailed flag and output formatting","description":"## Goal\n\nAdd --detailed flag to CLI and wire up all analytics with proper output formatting.\n\n## CLI Changes (cli.rs)\n\n### Add flag to StatsArgs\n\n```rust\n/// Show detailed analytics including temporal patterns, engagement metrics,\n/// and content analysis. Provides comprehensive archive insights.\n#[arg(long)]\npub detailed: bool,\n```\n\n### Already existing (remove 'not implemented' error)\n\n```rust\n/// Show top hashtags with counts\n#[arg(long)]\npub hashtags: bool,\n\n/// Show top mentions with counts\n#[arg(long)]\npub mentions: bool,\n```\n\n## Main.rs Integration (cmd_stats)\n\n```rust\nfn cmd_stats(cli: \u0026Cli, args: \u0026StatsArgs) -\u003e Result\u003c()\u003e {\n    let storage = Storage::open(\u0026cli.db)?;\n    \n    // Basic stats (always computed)\n    let basic = compute_basic_stats(\u0026storage)?;\n    \n    if args.detailed {\n        // Show progress for large archives\n        if basic.total_tweets \u003e 10_000 {\n            eprintln\\!(\"Computing detailed analytics...\");\n        }\n        \n        let temporal = compute_temporal_stats(\u0026storage)?;\n        let engagement = compute_engagement_stats(\u0026storage)?;\n        let content = compute_content_stats(\u0026storage)?;\n        \n        if cli.format.is_json() {\n            print_detailed_stats_json(\u0026basic, \u0026temporal, \u0026engagement, \u0026content)?;\n        } else {\n            print_detailed_stats_text(\u0026basic, \u0026temporal, \u0026engagement, \u0026content)?;\n        }\n    } else if args.hashtags {\n        let content = compute_content_stats(\u0026storage)?;\n        print_hashtags(\u0026content.top_hashtags, \u0026cli.format)?;\n    } else if args.mentions {\n        let content = compute_content_stats(\u0026storage)?;\n        print_mentions(\u0026content.top_mentions, \u0026cli.format)?;\n    } else {\n        // Basic stats only\n        print_basic_stats(\u0026basic, \u0026cli.format)?;\n    }\n    \n    Ok(())\n}\n```\n\n## Output Formatting (stats_output.rs)\n\n### Text Format\n\n```rust\nfn print_detailed_stats_text(\n    basic: \u0026BasicStats,\n    temporal: \u0026TemporalStats,\n    engagement: \u0026EngagementStats,\n    content: \u0026ContentStats,\n) -\u003e Result\u003c()\u003e {\n    use colored::*;\n    \n    println\\!(\"{}\", \"â•\".repeat(65).bright_blue());\n    println\\!(\"{}               ARCHIVE ANALYTICS\", \" \".repeat(16).on_bright_blue());\n    println\\!(\"{}\", \"â•\".repeat(65).bright_blue());\n    println\\!();\n    \n    // Section: Basic Overview\n    println\\!(\"{}\", \"ğŸ“Š OVERVIEW\".cyan().bold());\n    println\\!(\"{}\", \"â”€\".repeat(65).dimmed());\n    // ... format basic stats\n    \n    // Section: Temporal\n    println\\!();\n    println\\!(\"{}\", \"ğŸ“… TEMPORAL PATTERNS\".cyan().bold());\n    println\\!(\"{}\", \"â”€\".repeat(65).dimmed());\n    println\\!(\"Activity Trend: {}\", sparkline(\u0026temporal.daily_counts, 30));\n    // ... format temporal stats\n    \n    // Section: Engagement\n    println\\!();\n    println\\!(\"{}\", \"ğŸ“ˆ ENGAGEMENT\".cyan().bold());\n    // ... format engagement stats\n    \n    // Section: Content\n    println\\!();\n    println\\!(\"{}\", \"ğŸ“ CONTENT\".cyan().bold());\n    // ... format content stats\n}\n```\n\n### JSON Format\n\n```rust\nfn print_detailed_stats_json(...) -\u003e Result\u003c()\u003e {\n    let output = json\\!({\n        \"basic\": {\n            \"total_tweets\": basic.total_tweets,\n            \"total_likes\": basic.total_likes,\n            // ...\n        },\n        \"temporal\": {\n            \"daily_counts\": temporal.daily_counts,\n            \"hourly_distribution\": temporal.hourly_distribution,\n            \"most_active_day\": temporal.most_active_day,\n            // ...\n        },\n        \"engagement\": {\n            \"total_likes\": engagement.total_likes,\n            \"top_tweets\": engagement.top_tweets,\n            // ...\n        },\n        \"content\": {\n            \"media_ratio\": content.media_ratio,\n            \"top_hashtags\": content.top_hashtags,\n            // ...\n        }\n    });\n    println\\!(\"{}\", serde_json::to_string_pretty(\u0026output)?);\n    Ok(())\n}\n```\n\n## Acceptance Criteria\n\n- [ ] --detailed flag added to CLI\n- [ ] --hashtags and --mentions work (no longer throw error)\n- [ ] Text output is colorful and well-formatted\n- [ ] JSON output includes all computed metrics\n- [ ] Progress indicator for large archives\n- [ ] Help text clearly explains --detailed\n- [ ] Output fits in 80-column terminal","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T19:08:12.922482-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:05:10.662004-05:00","closed_at":"2026-01-10T21:05:10.662004-05:00","close_reason":"Closed","dependencies":[{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.5","title":"Add tests for enhanced stats analytics","description":"## Goal\n\nComprehensive test coverage for the enhanced stats analytics feature.\n\n## Unit Tests (stats_analytics.rs)\n\n### Temporal Analytics Tests\n\n```rust\n#[test]\nfn test_sparkline_generation() {\n    let values = vec![1, 5, 10, 8, 3, 1];\n    let spark = sparkline(\u0026values, 6);\n    assert_eq!(spark.chars().count(), 6);\n    // Highest value (10) should be â–ˆ\n    assert!(spark.contains('â–ˆ'));\n}\n\n#[test]\nfn test_sparkline_empty_input() {\n    let values: Vec\u003cu64\u003e = vec![];\n    let spark = sparkline(\u0026values, 10);\n    assert_eq!(spark, \"\");\n}\n\n#[test]\nfn test_sparkline_single_value() {\n    let values = vec![5];\n    let spark = sparkline(\u0026values, 1);\n    assert_eq!(spark, \"â–ˆ\");  // Single value is max\n}\n\n#[test]\nfn test_gap_detection_normal() {\n    let counts = vec![\n        (NaiveDate::from_ymd(2023, 1, 1), 5),\n        (NaiveDate::from_ymd(2023, 1, 5), 3),  // 4 day gap\n        (NaiveDate::from_ymd(2023, 1, 20), 2), // 15 day gap (longest)\n        (NaiveDate::from_ymd(2023, 1, 22), 1), // 2 day gap\n    ];\n    let (gap, start, end) = find_longest_gap(\u0026counts);\n    assert_eq!(gap.num_days(), 15);\n    assert_eq!(start, NaiveDate::from_ymd(2023, 1, 5));\n}\n\n#[test]\nfn test_gap_detection_single_day() {\n    let counts = vec![(NaiveDate::from_ymd(2023, 1, 1), 5)];\n    let (gap, _, _) = find_longest_gap(\u0026counts);\n    assert_eq!(gap.num_days(), 0);\n}\n\n#[test]\nfn test_hourly_distribution() {\n    let storage = create_test_storage_with_tweets(vec![\n        (\"tweet1\", \"2023-01-01T09:00:00Z\"),\n        (\"tweet2\", \"2023-01-01T09:30:00Z\"),\n        (\"tweet3\", \"2023-01-01T21:00:00Z\"),\n    ]);\n    let stats = compute_temporal_stats(\u0026storage).unwrap();\n    assert_eq!(stats.hourly_distribution[9], 2);  // 9 AM\n    assert_eq!(stats.hourly_distribution[21], 1); // 9 PM\n}\n```\n\n### Engagement Analytics Tests\n\n```rust\n#[test]\nfn test_likes_histogram_buckets() {\n    let storage = create_test_storage_with_engagement(vec![\n        (0, 0), (1, 0), (3, 0), (5, 0),  // 4 in 0-5 bucket\n        (10, 0), (15, 0),                // 2 in 6-25 bucket\n        (100, 0),                        // 1 in 51-100 bucket\n    ]);\n    let stats = compute_engagement_stats(\u0026storage).unwrap();\n    // Verify bucket counts\n}\n\n#[test]\nfn test_top_tweets_ordering() {\n    let storage = create_test_storage_with_engagement(vec![\n        (10, 5),   // total 15\n        (100, 20), // total 120 (should be first)\n        (50, 10),  // total 60\n    ]);\n    let stats = compute_engagement_stats(\u0026storage).unwrap();\n    assert_eq!(stats.top_tweets[0].total_engagement, 120);\n}\n\n#[test]\nfn test_engagement_with_nulls() {\n    // Tweets with NULL favorite_count should be handled gracefully\n    let storage = create_test_storage_with_null_engagement();\n    let stats = compute_engagement_stats(\u0026storage).unwrap();\n    // Should not panic, should skip nulls\n}\n```\n\n### Content Analytics Tests\n\n```rust\n#[test]\nfn test_hashtag_extraction() {\n    let storage = create_test_storage_with_tweets(vec![\n        \"Hello #rust #programming\",\n        \"More #rust content\",\n        \"#Tech news\",\n    ]);\n    let stats = compute_content_stats(\u0026storage).unwrap();\n    assert_eq!(stats.top_hashtags[0], (\"rust\".to_string(), 2));\n}\n\n#[test]\nfn test_media_ratio() {\n    let storage = create_test_storage_mixed_media(3, 7); // 3 with media, 7 without\n    let stats = compute_content_stats(\u0026storage).unwrap();\n    assert!((stats.media_ratio - 0.3).abs() \u003c 0.01);\n}\n\n#[test]\nfn test_thread_detection() {\n    // Create tweets where some reply to self\n    let storage = create_test_storage_with_threads();\n    let stats = compute_content_stats(\u0026storage).unwrap();\n    assert_eq!(stats.thread_count, expected_thread_count);\n}\n```\n\n## Integration Tests\n\n### Test: detailed_flag_produces_all_sections\n\n```rust\n#[test]\nfn test_detailed_flag_text_output() {\n    let output = run_xf(\u0026[\"stats\", \"--detailed\"]);\n    assert!(output.contains(\"TEMPORAL PATTERNS\"));\n    assert!(output.contains(\"ENGAGEMENT\"));\n    assert!(output.contains(\"CONTENT\"));\n}\n```\n\n### Test: detailed_json_schema\n\n```rust\n#[test]\nfn test_detailed_flag_json_output() {\n    let output = run_xf(\u0026[\"stats\", \"--detailed\", \"--format\", \"json\"]);\n    let json: Value = serde_json::from_str(\u0026output).unwrap();\n    assert!(json[\"temporal\"].is_object());\n    assert!(json[\"engagement\"].is_object());\n    assert!(json[\"content\"].is_object());\n}\n```\n\n## Edge Case Tests\n\n```rust\n#[test]\nfn test_empty_archive() {\n    let storage = create_empty_storage();\n    let stats = compute_temporal_stats(\u0026storage).unwrap();\n    assert!(stats.daily_counts.is_empty());\n}\n\n#[test]\nfn test_single_tweet_archive() {\n    // Edge case: archive with exactly one tweet\n}\n\n#[test]\nfn test_massive_archive_performance() {\n    // Generate 100k fake tweets, verify \u003c 2s execution\n    let storage = create_large_test_storage(100_000);\n    let start = Instant::now();\n    let _ = compute_temporal_stats(\u0026storage).unwrap();\n    assert!(start.elapsed() \u003c Duration::from_secs(2));\n}\n```\n\n## Logging Requirements\n\n- Each test should log setup and teardown with tracing::debug!\n- On assertion failure, log full computed state\n- Log timing for performance-sensitive tests\n\n## Acceptance Criteria\n\n- [ ] All unit tests pass\n- [ ] Edge cases covered (empty, single item, nulls)\n- [ ] Performance test validates \u003c 2s on 100k tweets\n- [ ] Integration tests verify CLI output\n- [ ] Test coverage \u003e 80% for new code","status":"in_progress","priority":2,"issue_type":"task","assignee":"MagentaFox","created_at":"2026-01-10T19:08:23.225389-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:25:00.900211348-05:00","dependencies":[{"issue_id":"xf-11.2.5","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.5","depends_on_id":"xf-11.2.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.6","title":"Create E2E test script for stats --detailed","description":"## Goal\n\nCreate a comprehensive E2E test script that validates the stats --detailed feature against real archive data.\n\n## E2E Script: tests/e2e/stats_detailed_test.sh\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\n# E2E Test Suite for xf stats --detailed\n# Tests the full stats analytics pipeline against test archives\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" \u0026\u0026 pwd)\"\nPROJECT_ROOT=\"$(cd \"$SCRIPT_DIR/../..\" \u0026\u0026 pwd)\"\nTEST_ARCHIVE=\"$PROJECT_ROOT/test-archives/large-archive\"\nLOG_DIR=\"$PROJECT_ROOT/test-output\"\nTIMESTAMP=\"$(date +%Y%m%d_%H%M%S)\"\nLOG_FILE=\"$LOG_DIR/stats_detailed_e2e_$TIMESTAMP.log\"\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\n# Ensure log directory exists\nmkdir -p \"$LOG_DIR\"\n\n# Logging function with timestamps\nlog() {\n    local level=\"$1\"\n    shift\n    local msg=\"$*\"\n    local ts=\"$(date '+%Y-%m-%d %H:%M:%S.%3N')\"\n    echo -e \"[$ts] [$level] $msg\" | tee -a \"$LOG_FILE\"\n}\n\nlog_info() { log \"INFO\" \"$*\"; }\nlog_pass() { log \"PASS\" \"${GREEN}$*${NC}\"; }\nlog_fail() { log \"FAIL\" \"${RED}$*${NC}\"; }\nlog_warn() { log \"WARN\" \"${YELLOW}$*${NC}\"; }\n\n# Test counter\nTESTS_RUN=0\nTESTS_PASSED=0\nTESTS_FAILED=0\n\n# Run a test and track result\nrun_test() {\n    local name=\"$1\"\n    shift\n    local cmd=\"$*\"\n    \n    ((TESTS_RUN++))\n    log_info \"Running test: $name\"\n    log_info \"Command: $cmd\"\n    \n    local start_time=$(date +%s%N)\n    local output\n    local exit_code=0\n    \n    if output=$(eval \"$cmd\" 2\u003e\u00261); then\n        exit_code=0\n    else\n        exit_code=$?\n    fi\n    \n    local end_time=$(date +%s%N)\n    local duration_ms=$(( (end_time - start_time) / 1000000 ))\n    \n    log_info \"Duration: ${duration_ms}ms\"\n    log_info \"Exit code: $exit_code\"\n    \n    if [ $exit_code -eq 0 ]; then\n        log_pass \"$name\"\n        ((TESTS_PASSED++))\n    else\n        log_fail \"$name\"\n        log_info \"Output: $output\"\n        ((TESTS_FAILED++))\n    fi\n    \n    echo \"$output\"\n    return $exit_code\n}\n\n# Assertion helpers\nassert_contains() {\n    local haystack=\"$1\"\n    local needle=\"$2\"\n    local msg=\"${3:-Output should contain '$needle'}\"\n    \n    if echo \"$haystack\" | grep -q \"$needle\"; then\n        log_pass \"Assert: $msg\"\n        return 0\n    else\n        log_fail \"Assert: $msg\"\n        log_info \"Searched for: $needle\"\n        log_info \"In output: $haystack\"\n        return 1\n    fi\n}\n\nassert_json_valid() {\n    local json=\"$1\"\n    local msg=\"${2:-JSON should be valid}\"\n    \n    if echo \"$json\" | jq . \u003e/dev/null 2\u003e\u00261; then\n        log_pass \"Assert: $msg\"\n        return 0\n    else\n        log_fail \"Assert: $msg\"\n        return 1\n    fi\n}\n\nassert_performance() {\n    local actual_ms=\"$1\"\n    local max_ms=\"$2\"\n    local msg=\"${3:-Should complete within ${max_ms}ms}\"\n    \n    if [ \"$actual_ms\" -le \"$max_ms\" ]; then\n        log_pass \"Assert: $msg (actual: ${actual_ms}ms)\"\n        return 0\n    else\n        log_fail \"Assert: $msg (actual: ${actual_ms}ms, max: ${max_ms}ms)\"\n        return 1\n    fi\n}\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TEST CASES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nlog_info \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\nlog_info \"           XF STATS --DETAILED E2E TEST SUITE\"\nlog_info \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\nlog_info \"Test Archive: $TEST_ARCHIVE\"\nlog_info \"Log File: $LOG_FILE\"\nlog_info \"\"\n\n# Test 1: Basic --detailed flag works\ntest_basic_detailed() {\n    local output\n    output=$(xf stats --detailed --db \"$TEST_ARCHIVE/xf.db\" 2\u003e\u00261)\n    \n    assert_contains \"$output\" \"TEMPORAL\" \"Should contain TEMPORAL section\"\n    assert_contains \"$output\" \"ENGAGEMENT\" \"Should contain ENGAGEMENT section\"\n    assert_contains \"$output\" \"CONTENT\" \"Should contain CONTENT section\"\n}\nrun_test \"Basic --detailed output\" test_basic_detailed\n\n# Test 2: JSON output is valid\ntest_json_output() {\n    local output\n    output=$(xf stats --detailed --format json --db \"$TEST_ARCHIVE/xf.db\" 2\u003e\u00261)\n    \n    assert_json_valid \"$output\" \"JSON output should be valid\"\n    \n    # Check for required fields\n    local has_temporal=$(echo \"$output\" | jq 'has(\"temporal\")')\n    local has_engagement=$(echo \"$output\" | jq 'has(\"engagement\")')\n    local has_content=$(echo \"$output\" | jq 'has(\"content\")')\n    \n    [ \"$has_temporal\" = \"true\" ] \u0026\u0026 log_pass \"Has temporal section\" || log_fail \"Missing temporal\"\n    [ \"$has_engagement\" = \"true\" ] \u0026\u0026 log_pass \"Has engagement section\" || log_fail \"Missing engagement\"\n    [ \"$has_content\" = \"true\" ] \u0026\u0026 log_pass \"Has content section\" || log_fail \"Missing content\"\n}\nrun_test \"JSON output format\" test_json_output\n\n# Test 3: Performance benchmark\ntest_performance() {\n    local start=$(date +%s%N)\n    xf stats --detailed --db \"$TEST_ARCHIVE/xf.db\" \u003e/dev/null 2\u003e\u00261\n    local end=$(date +%s%N)\n    local duration_ms=$(( (end - start) / 1000000 ))\n    \n    assert_performance \"$duration_ms\" 5000 \"Should complete in under 5 seconds\"\n}\nrun_test \"Performance benchmark\" test_performance\n\n# Test 4: Sparklines are generated\ntest_sparklines() {\n    local output\n    output=$(xf stats --detailed --db \"$TEST_ARCHIVE/xf.db\" 2\u003e\u00261)\n    \n    # Sparkline chars: â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ\n    if echo \"$output\" | grep -q '[â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ]'; then\n        log_pass \"Sparklines present in output\"\n    else\n        log_fail \"No sparklines found in output\"\n        return 1\n    fi\n}\nrun_test \"Sparkline generation\" test_sparklines\n\n# Test 5: --hashtags flag\ntest_hashtags_flag() {\n    local output\n    output=$(xf stats --hashtags --db \"$TEST_ARCHIVE/xf.db\" 2\u003e\u00261)\n    \n    # Should not error\n    if [ $? -eq 0 ]; then\n        log_pass \"--hashtags flag works\"\n    else\n        log_fail \"--hashtags flag failed\"\n        return 1\n    fi\n}\nrun_test \"Hashtags flag\" test_hashtags_flag\n\n# Test 6: --mentions flag\ntest_mentions_flag() {\n    local output\n    output=$(xf stats --mentions --db \"$TEST_ARCHIVE/xf.db\" 2\u003e\u00261)\n    \n    if [ $? -eq 0 ]; then\n        log_pass \"--mentions flag works\"\n    else\n        log_fail \"--mentions flag failed\"\n        return 1\n    fi\n}\nrun_test \"Mentions flag\" test_mentions_flag\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SUMMARY\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nlog_info \"\"\nlog_info \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\nlog_info \"                    TEST SUMMARY\"\nlog_info \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\nlog_info \"Tests Run:    $TESTS_RUN\"\nlog_info \"Tests Passed: $TESTS_PASSED\"\nlog_info \"Tests Failed: $TESTS_FAILED\"\nlog_info \"Log File:     $LOG_FILE\"\n\nif [ $TESTS_FAILED -eq 0 ]; then\n    log_pass \"All tests passed!\"\n    exit 0\nelse\n    log_fail \"Some tests failed!\"\n    exit 1\nfi\n```\n\n## Test Data Requirements\n\nCreate test archive at test-archives/large-archive/ with:\n- 1000+ tweets for realistic stats\n- Variety of engagement levels (0 to 500 likes)\n- Mix of media and text-only tweets\n- Some threads (self-replies)\n- Various hashtags and mentions\n- Date range spanning 6+ months\n\n## Logging Specifications\n\n- Timestamp format: YYYY-MM-DD HH:MM:SS.mmm\n- Log levels: INFO, PASS, FAIL, WARN\n- Each test logs: name, command, duration, exit code\n- Failed assertions log: expected vs actual\n- Performance tests log: actual duration vs threshold\n- Summary at end: total/passed/failed counts\n\n## Acceptance Criteria\n\n- [ ] Script is executable and runs to completion\n- [ ] All 6+ test cases implemented\n- [ ] Performance test validates \u003c 5s threshold\n- [ ] JSON validation using jq\n- [ ] Clear pass/fail output with colors\n- [ ] Detailed log file generated\n- [ ] Exit code 0 only if all tests pass","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:26:54.052373-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:26:54.052373-05:00","dependencies":[{"issue_id":"xf-11.2.6","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.6","depends_on_id":"xf-11.2.5","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3","title":"Interactive REPL Mode (xf shell)","description":"## Overview\n\nAdd `xf shell` command for interactive archive exploration. REPL with command history, tab completion, persistent session state, and query refinement. Reduces friction for exploratory searches by eliminating repeated `xf` prefix and index loading overhead.\n\n## Background \u0026 Motivation\n\nPower users often run many searches in sequence when exploring their archive:\n```bash\nxf search \"meeting\" --limit 5\nxf search \"meeting\" --from 2023-01-01\nxf search \"meeting project\" --types dm\nxf list tweets --limit 10\nxf stats\n```\n\nEach command:\n- Re-loads the Tantivy index (50-100ms overhead)\n- Requires typing full command with `xf` prefix\n- Loses context from previous searches\n\nA REPL mode solves these issues:\n```\nxf\u003e search \"meeting\" --limit 5\n... results ...\nxf\u003e refine --from 2023-01-01\n... filtered results ...\nxf\u003e search \"meeting project\" --types dm\n... results ...\nxf\u003e $1  # reference first result\n... details ...\n```\n\n## UX Design\n\n### Prompt\n```\nxf\u003e _                    # Default prompt\nxf [42 results]\u003e _       # After search, show result count\nxf [dm:conv-123]\u003e _      # In DM conversation context\n```\n\n### Commands\n\n| Command | Description |\n|---------|-------------|\n| search \u003cquery\u003e [flags] | Search archive (same flags as CLI) |\n| stats [--detailed] | Show archive statistics |\n| list \u003ctype\u003e [flags] | List tweets/dms/likes/etc |\n| refine [flags] | Filter previous results |\n| more | Show next page of results |\n| show \u003cid\u003e or $N | Show details of result N |\n| export \u003cfile\u003e | Export current results to file |\n| help [command] | Show help for command |\n| quit / exit / q | Exit REPL |\n\n### Variable Substitution\n\n- `$1`, `$2`, etc. refer to result items by position\n- `$_` refers to last selected item\n- `$*` refers to all current results\n\n### History\n\n- Persisted to ~/.xf_history (1000 lines)\n- Arrow keys for navigation\n- Ctrl+R for reverse search\n- History excludes quit/exit\n\n## Technical Architecture\n\n### New Module: src/repl.rs\n\n```rust\nuse rustyline::{Editor, Config, CompletionType};\nuse rustyline::error::ReadlineError;\n\npub struct ReplState {\n    storage: Storage,\n    index: TantivyIndex,\n    last_results: Vec\u003cSearchResult\u003e,\n    last_query: Option\u003cString\u003e,\n    history_path: PathBuf,\n}\n\npub fn run(storage: Storage, index: TantivyIndex) -\u003e Result\u003c()\u003e {\n    let config = Config::builder()\n        .history_ignore_space(true)\n        .completion_type(CompletionType::List)\n        .build();\n    \n    let mut rl = Editor::\u003cReplCompleter\u003e::with_config(config)?;\n    let mut state = ReplState::new(storage, index);\n    \n    // Load history\n    let _ = rl.load_history(\u0026state.history_path);\n    \n    loop {\n        let prompt = state.format_prompt();\n        match rl.readline(\u0026prompt) {\n            Ok(line) =\u003e {\n                rl.add_history_entry(\u0026line);\n                match parse_and_execute(\u0026mut state, \u0026line) {\n                    Ok(ShouldContinue::Yes) =\u003e continue,\n                    Ok(ShouldContinue::Exit) =\u003e break,\n                    Err(e) =\u003e eprintln!(\"Error: {e}\"),\n                }\n            }\n            Err(ReadlineError::Interrupted) =\u003e continue,\n            Err(ReadlineError::Eof) =\u003e break,\n            Err(e) =\u003e return Err(e.into()),\n        }\n    }\n    \n    rl.save_history(\u0026state.history_path)?;\n    Ok(())\n}\n```\n\n## Dependencies\n\n- rustyline = \"12\" (already common in Rust CLIs)\n- No other new dependencies\n\n## Child Tasks\n\n1. Implement REPL core with rustyline\n2. Add tab completion for commands and flags\n3. Add session state and query refinement\n4. Add xf shell subcommand to CLI\n5. Add comprehensive tests\n6. Add E2E test script\n\n## Acceptance Criteria\n\n- [ ] `xf shell` launches interactive REPL\n- [ ] Commands work same as CLI equivalents\n- [ ] Tab completion for commands and flags\n- [ ] History persists across sessions\n- [ ] Variable substitution works ($1, $2, etc)\n- [ ] refine command filters previous results\n- [ ] Ctrl+C cancels current input, doesn't exit\n- [ ] Performance: no perceivable lag between commands","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-10T19:08:37.178423-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:27:38.382863-05:00","dependencies":[{"issue_id":"xf-11.3","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.1","title":"Implement REPL core with rustyline","description":"## Goal\n\nCreate the core REPL module using rustyline for readline support.\n\n## Dependencies to Add (Cargo.toml)\n\n```toml\n[dependencies]\nrustyline = \"12\"\n```\n\n## New Module: src/repl.rs\n\n### Core Types\n\n```rust\nuse rustyline::{Editor, Config, CompletionType, EditMode};\nuse rustyline::history::DefaultHistory;\nuse std::path::PathBuf;\n\n/// REPL session state\npub struct ReplSession {\n    /// Database connection\n    storage: Storage,\n    /// Tantivy search index\n    index: TantivyIndex,\n    /// Results from last search command\n    last_results: Vec\u003cSearchResult\u003e,\n    /// Last executed query string\n    last_query: Option\u003cString\u003e,\n    /// Path to history file\n    history_path: PathBuf,\n    /// Current prompt context\n    prompt_context: PromptContext,\n}\n\n#[derive(Default)]\nenum PromptContext {\n    #[default]\n    Normal,\n    WithResults(usize),\n    InConversation(String),\n}\n\nenum Command {\n    Search { query: String, args: SearchArgs },\n    Stats { detailed: bool },\n    List { target: ListTarget, limit: Option\u003cusize\u003e },\n    Refine { args: RefineArgs },\n    More,\n    Show { reference: ResultReference },\n    Export { path: PathBuf, format: OutputFormat },\n    Help { command: Option\u003cString\u003e },\n    Quit,\n}\n\nenum ResultReference {\n    Index(usize),    // $1, $2, etc\n    Last,            // $_\n    All,             // $*\n    Id(String),      // Direct ID\n}\n```\n\n### Main Loop\n\n```rust\npub fn run(storage: Storage, index: TantivyIndex) -\u003e Result\u003c()\u003e {\n    let config = Config::builder()\n        .history_ignore_space(true)\n        .history_ignore_dups(true)?\n        .completion_type(CompletionType::List)\n        .edit_mode(EditMode::Emacs)\n        .build();\n    \n    let mut rl: Editor\u003c(), DefaultHistory\u003e = Editor::with_config(config)?;\n    \n    let mut session = ReplSession {\n        storage,\n        index,\n        last_results: Vec::new(),\n        last_query: None,\n        history_path: dirs::home_dir()\n            .unwrap_or_default()\n            .join(\".xf_history\"),\n        prompt_context: PromptContext::Normal,\n    };\n    \n    // Load history (ignore errors for new users)\n    let _ = rl.load_history(\u0026session.history_path);\n    \n    println\\!(\"xf interactive mode. Type 'help' for commands, 'quit' to exit.\");\n    println\\!();\n    \n    loop {\n        let prompt = session.format_prompt();\n        \n        match rl.readline(\u0026prompt) {\n            Ok(line) =\u003e {\n                let line = line.trim();\n                if line.is_empty() {\n                    continue;\n                }\n                \n                // Don't save quit/exit to history\n                if \\!matches\\!(line, \"quit\" | \"exit\" | \"q\") {\n                    rl.add_history_entry(line)?;\n                }\n                \n                match session.execute(line) {\n                    Ok(true) =\u003e continue,  // Keep running\n                    Ok(false) =\u003e break,    // User quit\n                    Err(e) =\u003e {\n                        eprintln\\!(\"{}: {}\", \"Error\".red(), e);\n                    }\n                }\n            }\n            Err(ReadlineError::Interrupted) =\u003e {\n                // Ctrl+C - just show new prompt\n                println\\!(\"^C\");\n                continue;\n            }\n            Err(ReadlineError::Eof) =\u003e {\n                // Ctrl+D - exit\n                println\\!();\n                break;\n            }\n            Err(e) =\u003e {\n                return Err(e.into());\n            }\n        }\n    }\n    \n    // Save history\n    rl.save_history(\u0026session.history_path)?;\n    \n    println\\!(\"Goodbye\\!\");\n    Ok(())\n}\n```\n\n### Prompt Formatting\n\n```rust\nimpl ReplSession {\n    fn format_prompt(\u0026self) -\u003e String {\n        match \u0026self.prompt_context {\n            PromptContext::Normal =\u003e \"xf\u003e \".to_string(),\n            PromptContext::WithResults(n) =\u003e format\\!(\"xf [{}]\u003e \", n),\n            PromptContext::InConversation(id) =\u003e format\\!(\"xf [dm:{}]\u003e \", \u0026id[..8.min(id.len())]),\n        }\n    }\n}\n```\n\n### Command Parsing\n\n```rust\nfn parse_command(input: \u0026str) -\u003e Result\u003cCommand\u003e {\n    let parts: Vec\u003c\u0026str\u003e = input.split_whitespace().collect();\n    if parts.is_empty() {\n        return Err(anyhow\\!(\"Empty command\"));\n    }\n    \n    match parts[0] {\n        \"search\" | \"s\" =\u003e parse_search_command(\u0026parts[1..]),\n        \"stats\" =\u003e parse_stats_command(\u0026parts[1..]),\n        \"list\" | \"ls\" =\u003e parse_list_command(\u0026parts[1..]),\n        \"refine\" | \"r\" =\u003e parse_refine_command(\u0026parts[1..]),\n        \"more\" | \"m\" =\u003e Ok(Command::More),\n        \"show\" =\u003e parse_show_command(\u0026parts[1..]),\n        \"export\" =\u003e parse_export_command(\u0026parts[1..]),\n        \"help\" | \"h\" | \"?\" =\u003e parse_help_command(\u0026parts[1..]),\n        \"quit\" | \"exit\" | \"q\" =\u003e Ok(Command::Quit),\n        s if s.starts_with('$') =\u003e parse_reference(s),\n        _ =\u003e Err(anyhow\\!(\"Unknown command: {}. Type 'help' for available commands.\", parts[0])),\n    }\n}\n```\n\n## Logging\n\n- Log session start/end with tracing::info\\!\n- Log each command execution with tracing::debug\\!\n- Log errors with tracing::warn\\!\n\n## Acceptance Criteria\n\n- [ ] rustyline dependency added\n- [ ] src/repl.rs module created\n- [ ] Main loop handles Ctrl+C gracefully\n- [ ] History saved to ~/.xf_history\n- [ ] Basic commands parsed (search, quit, help)\n- [ ] Prompt changes based on context\n- [ ] cargo check passes","notes":"Started REPL core with rustyline: added dependency, new src/repl.rs with run loop, prompt context, search/stats/help/quit parsing, history support, logging, and basic result printing. Added module in lib.rs. Ran fmt/check/clippy.","status":"in_progress","priority":2,"issue_type":"task","assignee":"SageDune","created_at":"2026-01-10T19:08:52.475269-05:00","created_by":"jemanuel","updated_at":"2026-01-10T20:00:20.721294259-05:00","dependencies":[{"issue_id":"xf-11.3.1","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.2","title":"Add tab completion for REPL commands","description":"## Goal\n\nImplement tab completion for REPL commands, flags, and context-aware values.\n\n## Implementation\n\n### Rustyline Completer Trait\n\n```rust\nuse rustyline::completion::{Completer, Pair};\nuse rustyline::Context;\n\npub struct ReplCompleter {\n    commands: Vec\u003cCompletionEntry\u003e,\n    flags: HashMap\u003cString, Vec\u003cCompletionEntry\u003e\u003e,\n    type_values: Vec\u003cString\u003e,\n}\n\nstruct CompletionEntry {\n    display: String,\n    replacement: String,\n    description: Option\u003cString\u003e,\n}\n\nimpl Completer for ReplCompleter {\n    type Candidate = Pair;\n    \n    fn complete(\n        \u0026self,\n        line: \u0026str,\n        pos: usize,\n        _ctx: \u0026Context\u003c'_\u003e,\n    ) -\u003e rustyline::Result\u003c(usize, Vec\u003cPair\u003e)\u003e {\n        let (start, word) = extract_word_at_position(line, pos);\n        \n        let candidates = if start == 0 {\n            // Completing command name\n            self.complete_command(word)\n        } else if word.starts_with('-') {\n            // Completing flag\n            self.complete_flag(line, word)\n        } else if self.is_after_type_flag(line, pos) {\n            // Completing --types value\n            self.complete_type_value(word)\n        } else {\n            Vec::new()\n        };\n        \n        Ok((start, candidates))\n    }\n}\n```\n\n### Command Completions\n\n```rust\nimpl ReplCompleter {\n    pub fn new() -\u003e Self {\n        let commands = vec\\![\n            CompletionEntry::new(\"search\", \"Search the archive\"),\n            CompletionEntry::new(\"stats\", \"Show archive statistics\"),\n            CompletionEntry::new(\"list\", \"List items (tweets, dms, etc)\"),\n            CompletionEntry::new(\"refine\", \"Filter previous results\"),\n            CompletionEntry::new(\"more\", \"Show more results\"),\n            CompletionEntry::new(\"show\", \"Show item details\"),\n            CompletionEntry::new(\"export\", \"Export results to file\"),\n            CompletionEntry::new(\"help\", \"Show help\"),\n            CompletionEntry::new(\"quit\", \"Exit REPL\"),\n        ];\n        \n        let mut flags = HashMap::new();\n        flags.insert(\"search\".to_string(), vec\\![\n            CompletionEntry::new(\"--types\", \"Filter by type\"),\n            CompletionEntry::new(\"--from\", \"Start date\"),\n            CompletionEntry::new(\"--to\", \"End date\"),\n            CompletionEntry::new(\"--limit\", \"Max results\"),\n            CompletionEntry::new(\"--format\", \"Output format\"),\n            CompletionEntry::new(\"--context\", \"Show DM context\"),\n        ]);\n        flags.insert(\"list\".to_string(), vec\\![\n            CompletionEntry::new(\"--limit\", \"Max results\"),\n            CompletionEntry::new(\"--format\", \"Output format\"),\n        ]);\n        flags.insert(\"stats\".to_string(), vec\\![\n            CompletionEntry::new(\"--detailed\", \"Show detailed analytics\"),\n            CompletionEntry::new(\"--hashtags\", \"Show top hashtags\"),\n            CompletionEntry::new(\"--mentions\", \"Show top mentions\"),\n        ]);\n        \n        let type_values = vec\\![\n            \"tweet\", \"dm\", \"like\", \"follower\", \"following\",\n            \"block\", \"mute\", \"ad\",\n        ].into_iter().map(String::from).collect();\n        \n        Self { commands, flags, type_values }\n    }\n    \n    fn complete_command(\u0026self, prefix: \u0026str) -\u003e Vec\u003cPair\u003e {\n        self.commands.iter()\n            .filter(|c| c.display.starts_with(prefix))\n            .map(|c| Pair {\n                display: format\\!(\"{} - {}\", c.display, c.description.as_deref().unwrap_or(\"\")),\n                replacement: c.replacement.clone(),\n            })\n            .collect()\n    }\n    \n    fn complete_flag(\u0026self, line: \u0026str, prefix: \u0026str) -\u003e Vec\u003cPair\u003e {\n        // Determine which command we're in\n        let cmd = line.split_whitespace().next().unwrap_or(\"\");\n        \n        self.flags.get(cmd)\n            .map(|flags| {\n                flags.iter()\n                    .filter(|f| f.display.starts_with(prefix))\n                    .map(|f| Pair {\n                        display: f.display.clone(),\n                        replacement: f.replacement.clone(),\n                    })\n                    .collect()\n            })\n            .unwrap_or_default()\n    }\n    \n    fn complete_type_value(\u0026self, prefix: \u0026str) -\u003e Vec\u003cPair\u003e {\n        self.type_values.iter()\n            .filter(|t| t.starts_with(prefix))\n            .map(|t| Pair {\n                display: t.clone(),\n                replacement: t.clone(),\n            })\n            .collect()\n    }\n    \n    fn is_after_type_flag(\u0026self, line: \u0026str, pos: usize) -\u003e bool {\n        // Check if cursor is right after \"--types \" or \"--types=\"\n        let before_cursor = \u0026line[..pos];\n        before_cursor.ends_with(\"--types \") || before_cursor.ends_with(\"--types=\")\n    }\n}\n```\n\n### Word Extraction Helper\n\n```rust\nfn extract_word_at_position(line: \u0026str, pos: usize) -\u003e (usize, \u0026str) {\n    let before_cursor = \u0026line[..pos];\n    \n    // Find start of current word\n    let word_start = before_cursor\n        .rfind(|c: char| c.is_whitespace() || c == '=')\n        .map(|i| i + 1)\n        .unwrap_or(0);\n    \n    let word = \u0026before_cursor[word_start..];\n    (word_start, word)\n}\n```\n\n## UX Behavior\n\n- Tab shows completion menu if multiple options\n- Tab auto-completes if single option\n- Completions show description for context\n- Double-tab shows all options\n- Works with partial input at any position\n\n## Acceptance Criteria\n\n- [ ] Commands complete from partial input\n- [ ] Flags complete based on current command\n- [ ] --types values complete (tweet, dm, etc)\n- [ ] Descriptions shown in completion menu\n- [ ] No crash on empty input\n- [ ] Works with quotes in search queries","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:08:52.888703-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:29:57.72062-05:00","dependencies":[{"issue_id":"xf-11.3.2","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.2","depends_on_id":"xf-11.3.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.3","title":"Add REPL session state and query refinement","description":"## Goal\n\nAdd session state management for result caching, query refinement, variable substitution, and pagination.\n\n## Implementation\n\n### Session State Structure\n\n```rust\npub struct SessionState {\n    /// Cached results from last search\n    last_results: Vec\u003cSearchResult\u003e,\n    /// The query that produced last_results\n    last_query: Option\u003cParsedQuery\u003e,\n    /// Current offset for pagination\n    current_offset: usize,\n    /// Page size for 'more' command\n    page_size: usize,\n    /// Last selected/shown item\n    last_selected: Option\u003cSearchResult\u003e,\n    /// Named variables ($name = value)\n    variables: HashMap\u003cString, String\u003e,\n}\n\nstruct ParsedQuery {\n    query_string: String,\n    types: Option\u003cVec\u003cDataType\u003e\u003e,\n    from: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    to: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    limit: Option\u003cusize\u003e,\n}\n```\n\n### Refine Command\n\nThe `refine` command filters the previous result set without re-running the search.\n\n```rust\nimpl ReplSession {\n    fn cmd_refine(\u0026mut self, args: \u0026RefineArgs) -\u003e Result\u003c()\u003e {\n        if self.state.last_results.is_empty() {\n            anyhow::bail!(\"No previous results to refine. Run a search first.\");\n        }\n        \n        let mut filtered = self.state.last_results.clone();\n        \n        // Filter by date range\n        if let Some(from) = \u0026args.from {\n            filtered.retain(|r| r.created_at \u003e= *from);\n        }\n        if let Some(to) = \u0026args.to {\n            filtered.retain(|r| r.created_at \u003c= *to);\n        }\n        \n        // Filter by type\n        if let Some(types) = \u0026args.types {\n            filtered.retain(|r| types.contains(\u0026r.result_type));\n        }\n        \n        // Filter by text pattern (grep-like)\n        if let Some(pattern) = \u0026args.pattern {\n            let re = Regex::new(pattern)?;\n            filtered.retain(|r| re.is_match(\u0026r.text));\n        }\n        \n        println!(\n            \"Refined {} â†’ {} results\",\n            self.state.last_results.len(),\n            filtered.len()\n        );\n        \n        self.state.last_results = filtered;\n        self.state.current_offset = 0;\n        self.display_current_page()?;\n        \n        Ok(())\n    }\n}\n```\n\n### Pagination with 'more'\n\n```rust\nimpl ReplSession {\n    fn cmd_more(\u0026mut self) -\u003e Result\u003c()\u003e {\n        if self.state.last_results.is_empty() {\n            anyhow::bail!(\"No results to paginate.\");\n        }\n        \n        let total = self.state.last_results.len();\n        if self.state.current_offset \u003e= total {\n            println!(\"No more results (showing {} of {})\", total, total);\n            return Ok(());\n        }\n        \n        self.state.current_offset += self.state.page_size;\n        self.display_current_page()?;\n        \n        Ok(())\n    }\n    \n    fn display_current_page(\u0026self) -\u003e Result\u003c()\u003e {\n        let start = self.state.current_offset;\n        let end = (start + self.state.page_size).min(self.state.last_results.len());\n        \n        for (i, result) in self.state.last_results[start..end].iter().enumerate() {\n            let idx = start + i + 1;  // 1-indexed for user display\n            println!(\"${}: {}\", idx, format_result_oneline(result));\n        }\n        \n        let remaining = self.state.last_results.len() - end;\n        if remaining \u003e 0 {\n            println!(\"... {} more (type 'more' to see next page)\", remaining);\n        }\n        \n        Ok(())\n    }\n}\n```\n\n### Variable Substitution\n\n```rust\nimpl ReplSession {\n    /// Resolve $N, $_, $*, or $name references\n    fn resolve_reference(\u0026self, reference: \u0026str) -\u003e Result\u003cResolvedReference\u003e {\n        if !reference.starts_with('$') {\n            return Err(anyhow!(\"Not a reference: {}\", reference));\n        }\n        \n        let rest = \u0026reference[1..];\n        \n        match rest {\n            \"_\" =\u003e {\n                // Last selected\n                self.state.last_selected.clone()\n                    .ok_or_else(|| anyhow!(\"No item selected yet\"))\n                    .map(ResolvedReference::Single)\n            }\n            \"*\" =\u003e {\n                // All results\n                Ok(ResolvedReference::Multiple(self.state.last_results.clone()))\n            }\n            s if s.chars().all(|c| c.is_ascii_digit()) =\u003e {\n                // Numeric index ($1, $2, etc)\n                let idx: usize = s.parse()?;\n                if idx == 0 || idx \u003e self.state.last_results.len() {\n                    return Err(anyhow!(\n                        \"Index {} out of range (1-{})\",\n                        idx,\n                        self.state.last_results.len()\n                    ));\n                }\n                Ok(ResolvedReference::Single(self.state.last_results[idx - 1].clone()))\n            }\n            name =\u003e {\n                // Named variable\n                self.state.variables.get(name)\n                    .cloned()\n                    .ok_or_else(|| anyhow!(\"Unknown variable: ${}\", name))\n                    .map(ResolvedReference::Value)\n            }\n        }\n    }\n    \n    /// Expand variables in command line before parsing\n    fn expand_variables(\u0026self, line: \u0026str) -\u003e String {\n        let re = Regex::new(r\"\\$\\w+\").unwrap();\n        re.replace_all(line, |caps: \u0026regex::Captures| {\n            match self.resolve_reference(\u0026caps[0]) {\n                Ok(ResolvedReference::Single(r)) =\u003e r.id.clone(),\n                Ok(ResolvedReference::Value(v)) =\u003e v,\n                _ =\u003e caps[0].to_string(),\n            }\n        }).to_string()\n    }\n}\n```\n\n### Chained Operations\n\nEnable pipelines like: search | refine | export\n\n```rust\nfn execute_line(\u0026mut self, line: \u0026str) -\u003e Result\u003cbool\u003e {\n    // Check for pipe operator\n    if line.contains(\" | \") {\n        let commands: Vec\u003c\u0026str\u003e = line.split(\" | \").collect();\n        for cmd in commands {\n            self.execute_single(cmd.trim())?;\n        }\n        return Ok(true);\n    }\n    \n    self.execute_single(line)\n}\n```\n\n## Acceptance Criteria\n\n- [ ] Results cached after each search\n- [ ] 'refine' filters cached results\n- [ ] 'more' paginates through results\n- [ ] $1, $2, etc reference results by position\n- [ ] $_ references last shown item\n- [ ] $* expands to all result IDs\n- [ ] Error messages are clear for invalid references\n- [ ] Pipe operator chains commands","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:08:53.278205-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:29:58.012451-05:00","dependencies":[{"issue_id":"xf-11.3.3","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.3","depends_on_id":"xf-11.3.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.4","title":"Add xf shell subcommand to CLI","description":"## Goal\n\nAdd `xf shell` subcommand to the CLI that launches the interactive REPL.\n\n## CLI Changes (cli.rs)\n\n### Add Shell variant to Commands enum\n\n```rust\n#[derive(Subcommand)]\npub enum Commands {\n    // ... existing commands ...\n    \n    /// Launch interactive shell for exploratory searches\n    Shell(ShellArgs),\n}\n\n#[derive(Args)]\npub struct ShellArgs {\n    /// Custom prompt string (default: \"xf\u003e \")\n    #[arg(long)]\n    pub prompt: Option\u003cString\u003e,\n    \n    /// Page size for result pagination (default: 10)\n    #[arg(long, default_value = \"10\")]\n    pub page_size: usize,\n    \n    /// Disable history persistence\n    #[arg(long)]\n    pub no_history: bool,\n    \n    /// Custom history file path\n    #[arg(long)]\n    pub history_file: Option\u003cPathBuf\u003e,\n}\n```\n\n## Main.rs Integration\n\n### Wire up shell command\n\n```rust\nfn main() -\u003e Result\u003c()\u003e {\n    // ... existing setup ...\n    \n    match \u0026cli.command {\n        // ... existing commands ...\n        \n        Commands::Shell(args) =\u003e cmd_shell(\u0026cli, args),\n    }\n}\n\nfn cmd_shell(cli: \u0026Cli, args: \u0026ShellArgs) -\u003e Result\u003c()\u003e {\n    // Load storage and index\n    let storage = Storage::open(\u0026cli.db)?;\n    let index_path = cli.index.as_ref()\n        .map(PathBuf::from)\n        .unwrap_or_else(|| Storage::default_index_path());\n    let index = TantivyIndex::open(\u0026index_path)?;\n    \n    // Configure REPL\n    let config = repl::Config {\n        prompt: args.prompt.clone(),\n        page_size: args.page_size,\n        history_enabled: !args.no_history,\n        history_path: args.history_file.clone()\n            .or_else(|| dirs::home_dir().map(|d| d.join(\".xf_history\"))),\n    };\n    \n    // Run REPL\n    repl::run(storage, index, config)\n}\n```\n\n## Help Text\n\nEnsure `xf shell --help` produces clear documentation:\n\n```\nLaunch interactive shell for exploratory searches\n\nUsage: xf shell [OPTIONS]\n\nOptions:\n      --prompt \u003cPROMPT\u003e      Custom prompt string (default: \"xf\u003e \")\n      --page-size \u003cSIZE\u003e     Page size for result pagination [default: 10]\n      --no-history           Disable history persistence\n      --history-file \u003cPATH\u003e  Custom history file path\n  -h, --help                 Print help\n\nExamples:\n    xf shell                           # Start REPL with defaults\n    xf shell --page-size 25            # Show 25 results per page\n    xf shell --prompt \"search\u003e \"       # Custom prompt\n    xf shell --no-history              # Don't save command history\n\nIn the shell, use 'help' to see available commands.\n```\n\n## Module Organization\n\n```\nsrc/\nâ”œâ”€â”€ main.rs          # CLI entry point, cmd_shell\nâ”œâ”€â”€ repl.rs          # REPL core (new)\nâ”œâ”€â”€ repl/            # REPL submodules (alternative organization)\nâ”‚   â”œâ”€â”€ mod.rs\nâ”‚   â”œâ”€â”€ completer.rs\nâ”‚   â”œâ”€â”€ commands.rs\nâ”‚   â””â”€â”€ state.rs\nâ””â”€â”€ ...\n```\n\n## Acceptance Criteria\n\n- [ ] `xf shell` launches REPL\n- [ ] `xf shell --help` shows usage\n- [ ] --prompt customizes prompt string\n- [ ] --page-size affects pagination\n- [ ] --no-history disables history file\n- [ ] --history-file changes history path\n- [ ] Shell works with all database/index options from parent CLI\n- [ ] Error message if database not found","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:08:53.687242-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:30:20.962389-05:00","dependencies":[{"issue_id":"xf-11.3.4","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.4","depends_on_id":"xf-11.3.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.5","title":"Add tests for REPL mode","description":"## Goal\n\nComprehensive test coverage for the REPL mode including unit tests, integration tests, and E2E scripts.\n\n## Unit Tests (repl.rs)\n\n### Command Parsing Tests\n\n```rust\n#[test]\nfn test_parse_search_command() {\n    let cmd = parse_command(\"search \\\"hello world\\\"\").unwrap();\n    assert!(matches!(cmd, Command::Search { query, .. } if query == \"hello world\"));\n}\n\n#[test]\nfn test_parse_search_with_flags() {\n    let cmd = parse_command(\"search \\\"test\\\" --types dm --limit 10\").unwrap();\n    if let Command::Search { args, .. } = cmd {\n        assert_eq!(args.limit, Some(10));\n        assert!(args.types.contains(\u0026DataType::Dm));\n    } else {\n        panic!(\"Expected Search command\");\n    }\n}\n\n#[test]\nfn test_parse_quit_variants() {\n    assert!(matches!(parse_command(\"quit\").unwrap(), Command::Quit));\n    assert!(matches!(parse_command(\"exit\").unwrap(), Command::Quit));\n    assert!(matches!(parse_command(\"q\").unwrap(), Command::Quit));\n}\n\n#[test]\nfn test_parse_reference() {\n    let cmd = parse_command(\"$1\").unwrap();\n    assert!(matches!(cmd, Command::Show { reference: ResultReference::Index(1) }));\n    \n    let cmd = parse_command(\"$_\").unwrap();\n    assert!(matches!(cmd, Command::Show { reference: ResultReference::Last }));\n}\n\n#[test]\nfn test_parse_unknown_command() {\n    let result = parse_command(\"foobar\");\n    assert!(result.is_err());\n    assert!(result.unwrap_err().to_string().contains(\"Unknown command\"));\n}\n```\n\n### Session State Tests\n\n```rust\n#[test]\nfn test_prompt_context_normal() {\n    let session = create_test_session();\n    assert_eq!(session.format_prompt(), \"xf\u003e \");\n}\n\n#[test]\nfn test_prompt_context_with_results() {\n    let mut session = create_test_session();\n    session.prompt_context = PromptContext::WithResults(42);\n    assert_eq!(session.format_prompt(), \"xf [42]\u003e \");\n}\n\n#[test]\nfn test_refine_filters_results() {\n    let mut session = create_test_session();\n    session.last_results = vec![\n        create_result(\"2023-01-01\", \"hello\"),\n        create_result(\"2023-06-15\", \"world\"),\n        create_result(\"2023-12-01\", \"hello again\"),\n    ];\n    \n    session.execute(\"refine --from 2023-06-01\").unwrap();\n    assert_eq!(session.last_results.len(), 2);\n}\n\n#[test]\nfn test_variable_substitution() {\n    let mut session = create_test_session();\n    session.last_results = vec![\n        create_result_with_id(\"tweet-123\"),\n        create_result_with_id(\"tweet-456\"),\n    ];\n    \n    let reference = session.resolve_reference(\"$1\").unwrap();\n    assert_eq!(reference.id, \"tweet-123\");\n    \n    let reference = session.resolve_reference(\"$2\").unwrap();\n    assert_eq!(reference.id, \"tweet-456\");\n}\n```\n\n### Tab Completion Tests\n\n```rust\n#[test]\nfn test_complete_commands() {\n    let completer = ReplCompleter::new();\n    let completions = completer.complete(\"sea\", 3);\n    assert!(completions.iter().any(|c| c.display == \"search\"));\n}\n\n#[test]\nfn test_complete_flags() {\n    let completer = ReplCompleter::new();\n    let completions = completer.complete(\"search \\\"test\\\" --ty\", 20);\n    assert!(completions.iter().any(|c| c.display == \"--types\"));\n}\n\n#[test]\nfn test_complete_type_values() {\n    let completer = ReplCompleter::new();\n    let completions = completer.complete(\"search \\\"test\\\" --types d\", 25);\n    assert!(completions.iter().any(|c| c.display == \"dm\"));\n}\n```\n\n## Integration Tests\n\n### Test: REPL session execution\n\n```rust\n#[test]\nfn test_repl_session_flow() {\n    let storage = create_test_storage();\n    let index = create_test_index();\n    \n    // Simulate REPL commands\n    let mut session = ReplSession::new(storage, index);\n    \n    assert!(session.execute(\"search \\\"hello\\\"\").unwrap());\n    assert!(!session.last_results.is_empty());\n    \n    assert!(session.execute(\"stats\").unwrap());\n    \n    assert!(!session.execute(\"quit\").unwrap());  // Returns false to exit\n}\n```\n\n### Test: History persistence\n\n```rust\n#[test]\nfn test_history_persists() {\n    let temp_dir = tempdir().unwrap();\n    let history_path = temp_dir.path().join(\".xf_history\");\n    \n    // Run session 1\n    {\n        let mut session = create_session_with_history(\u0026history_path);\n        session.add_history(\"search \\\"test1\\\"\");\n        session.add_history(\"search \\\"test2\\\"\");\n        session.save_history().unwrap();\n    }\n    \n    // Run session 2\n    {\n        let session = create_session_with_history(\u0026history_path);\n        let history = session.get_history();\n        assert!(history.contains(\u0026\"search \\\"test1\\\"\".to_string()));\n        assert!(history.contains(\u0026\"search \\\"test2\\\"\".to_string()));\n    }\n}\n```\n\n## E2E Test Script: tests/e2e/repl_test.sh\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\nLOG_FILE=\"./test-output/repl_e2e_$(date +%Y%m%d_%H%M%S).log\"\nmkdir -p ./test-output\n\nlog() { echo \"[$(date +%H:%M:%S)] $*\" | tee -a \"$LOG_FILE\"; }\n\nlog \"=== REPL E2E Test Suite ===\"\n\n# Test 1: REPL starts and quits\nlog \"[TEST 1] REPL start/quit\"\necho \"quit\" | xf shell 2\u003e\u00261 | tee -a \"$LOG_FILE\"\nif [ $? -eq 0 ]; then\n    log \"[PASS] REPL quit cleanly\"\nelse\n    log \"[FAIL] REPL did not quit cleanly\"\n    exit 1\nfi\n\n# Test 2: Search command works\nlog \"[TEST 2] Search in REPL\"\nOUTPUT=$(echo -e \"search \\\"test\\\"\\nquit\" | xf shell 2\u003e\u00261)\nif echo \"$OUTPUT\" | grep -q \"result\\|found\\|match\"; then\n    log \"[PASS] Search executed in REPL\"\nelse\n    log \"[FAIL] Search did not execute\"\n    exit 1\nfi\n\n# Test 3: Help command\nlog \"[TEST 3] Help command\"\nOUTPUT=$(echo -e \"help\\nquit\" | xf shell 2\u003e\u00261)\nif echo \"$OUTPUT\" | grep -q \"search\\|stats\\|quit\"; then\n    log \"[PASS] Help displayed commands\"\nelse\n    log \"[FAIL] Help output missing expected commands\"\n    exit 1\nfi\n\n# Test 4: Unknown command error\nlog \"[TEST 4] Unknown command handling\"\nOUTPUT=$(echo -e \"foobar\\nquit\" | xf shell 2\u003e\u00261)\nif echo \"$OUTPUT\" | grep -q -i \"unknown\\|error\"; then\n    log \"[PASS] Unknown command produced error\"\nelse\n    log \"[FAIL] Unknown command did not error\"\n    exit 1\nfi\n\n# Test 5: History file created\nlog \"[TEST 5] History persistence\"\nrm -f ~/.xf_history\necho -e \"search \\\"history test\\\"\\nquit\" | xf shell 2\u003e\u00261\nif [ -f ~/.xf_history ]; then\n    log \"[PASS] History file created\"\nelse\n    log \"[FAIL] History file not created\"\n    exit 1\nfi\n\nlog \"=== All REPL E2E tests passed ===\"\n```\n\n## Logging Requirements\n\n- Log command parsing with tracing::trace!\n- Log command execution with tracing::debug!\n- Log errors with tracing::warn!\n- Log session lifecycle with tracing::info!\n\n## Acceptance Criteria\n\n- [ ] Unit tests for all command parsing\n- [ ] Unit tests for session state management\n- [ ] Unit tests for tab completion\n- [ ] Integration tests for session flow\n- [ ] E2E script validates user-facing behavior\n- [ ] All tests pass with cargo test\n- [ ] Test coverage \u003e 75% for repl.rs","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T19:09:03.5476-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:28:52.230176-05:00","dependencies":[{"issue_id":"xf-11.3.5","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.5","depends_on_id":"xf-11.3.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4","title":"xf doctor Health Check Command","description":"## Overview\n\nAdd `xf doctor` command to diagnose common archive and index issues. Validates archive structure, checks index integrity, detects orphaned records, measures search performance, and provides actionable fix suggestions. Essential for troubleshooting when searches return unexpected results.\n\n## Background \u0026 Motivation\n\nUsers encounter various issues that are hard to diagnose:\n- \"Why are some of my tweets missing from search?\"\n- \"Why is search slow?\"\n- \"Is my index corrupted?\"\n- \"Did all my data import correctly?\"\n\nA doctor command provides:\n1. Validation that data is correctly stored\n2. Detection of corruption or inconsistencies\n3. Performance baseline measurements\n4. Actionable suggestions for fixes\n\n## UX Design\n\n### Output Format\n\n```\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                       XF HEALTH CHECK                             â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nğŸ“‚ ARCHIVE STRUCTURE\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâœ“ tweets.js: 15,432 tweets parsed\nâœ“ direct-messages.js: 892 conversations, 4,521 messages\nâœ“ likes.js: 8,234 likes\nâœ“ followers.js: 1,234 accounts\nâœ“ following.js: 567 accounts\nâš  blocks.js: File not found (optional, may not exist)\nâœ“ No duplicate tweet IDs detected\n\nğŸ—„ï¸ DATABASE (SQLite)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâœ“ PRAGMA integrity_check: OK\nâœ“ FTS5 index: healthy\nâœ“ Table row counts match expected:\n    tweets: 15,432 âœ“\n    direct_messages: 4,521 âœ“\n    likes: 8,234 âœ“\nâš  3 orphaned FTS entries (no matching tweet)\n  â†’ Run 'xf reindex --rebuild-fts' to fix\n\nğŸ” SEARCH INDEX (Tantivy)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâœ“ Index version: 0.21 (compatible)\nâœ“ Segment count: 4 (healthy)\nâœ“ Document count: 28,421\nâœ“ Sample query 'test': 0.8ms (excellent)\nâš  Index size: 245MB (consider 'xf optimize')\n\nâ±ï¸ PERFORMANCE\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nIndex load time: 42ms âœ“\nSimple query (single word): 1.2ms âœ“\nComplex query (phrase + filter): 8.4ms âœ“\nFTS5 query: 2.1ms âœ“\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSUMMARY: 12 checks passed, 2 warnings, 0 errors\n\nSuggestions:\n  1. Run 'xf reindex --rebuild-fts' to fix orphaned FTS entries\n  2. Consider 'xf optimize' to reduce index size\n```\n\n### Check Categories\n\n| Category | Checks |\n|----------|--------|\n| Archive | File presence, JSON validity, duplicate IDs |\n| Database | Integrity, FTS health, table counts, orphans |\n| Index | Version, segments, document count, corruption |\n| Performance | Load time, query latency, baselines |\n\n### Exit Codes\n\n- 0: All checks pass (warnings OK)\n- 1: Errors found (data issues)\n- 2: Critical (corruption detected)\n\n## Technical Architecture\n\n### New Module: src/doctor.rs\n\n```rust\npub struct HealthCheck {\n    pub category: CheckCategory,\n    pub name: String,\n    pub status: CheckStatus,\n    pub message: String,\n    pub suggestion: Option\u003cString\u003e,\n}\n\npub enum CheckStatus {\n    Pass,\n    Warning,\n    Error,\n    Critical,\n}\n\npub enum CheckCategory {\n    Archive,\n    Database,\n    Index,\n    Performance,\n}\n\npub struct HealthReport {\n    pub checks: Vec\u003cHealthCheck\u003e,\n    pub summary: Summary,\n}\n\npub fn run_all_checks(storage: \u0026Storage, index: \u0026TantivyIndex) -\u003e Result\u003cHealthReport\u003e;\n```\n\n## Acceptance Criteria\n\n- [ ] `xf doctor` runs all health checks\n- [ ] Clear pass/warning/error indicators\n- [ ] Actionable suggestions for fixes\n- [ ] --fix flag attempts safe auto-repairs\n- [ ] --json outputs machine-readable report\n- [ ] Exit codes reflect health status\n- [ ] Performance \u003c 10 seconds on large archives","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-10T19:09:20.038868-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:30:52.286192-05:00","dependencies":[{"issue_id":"xf-11.4","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.1","title":"Implement archive structure validation","description":"## Goal\n\nImplement archive structure validation to verify the source archive is correctly formatted and complete.\n\n## Checks to Implement\n\n### 1. Required Files Presence\n\n```rust\nfn check_required_files(archive_path: \u0026Path) -\u003e Vec\u003cHealthCheck\u003e {\n    let required = [\n        (\"data/tweets.js\", true),\n        (\"data/tweets-part*.js\", true),  // or parts\n        (\"data/direct-messages.js\", false),\n        (\"data/direct-messages-group*.js\", false),\n        (\"data/like.js\", false),\n        (\"data/follower.js\", false),\n        (\"data/following.js\", false),\n        (\"data/block.js\", false),\n        (\"data/mute.js\", false),\n    ];\n    \n    let mut checks = Vec::new();\n    for (pattern, is_required) in required {\n        let exists = glob_matches_any(archive_path, pattern);\n        checks.push(HealthCheck {\n            category: CheckCategory::Archive,\n            name: format!(\"File: {}\", pattern),\n            status: if exists { \n                CheckStatus::Pass \n            } else if is_required {\n                CheckStatus::Error\n            } else {\n                CheckStatus::Warning\n            },\n            message: if exists { \n                \"Found\".into() \n            } else { \n                \"Not found\".into() \n            },\n            suggestion: if !exists \u0026\u0026 is_required {\n                Some(\"Ensure archive was fully extracted\".into())\n            } else {\n                None\n            },\n        });\n    }\n    checks\n}\n```\n\n### 2. JSON Structure Validation\n\n```rust\nfn check_json_structure(archive_path: \u0026Path) -\u003e Vec\u003cHealthCheck\u003e {\n    let files = [\"tweets.js\", \"direct-messages.js\", \"like.js\"];\n    let mut checks = Vec::new();\n    \n    for file in files {\n        let path = archive_path.join(\"data\").join(file);\n        if !path.exists() {\n            continue;\n        }\n        \n        match validate_js_wrapped_json(\u0026path) {\n            Ok((count, warnings)) =\u003e {\n                checks.push(HealthCheck {\n                    category: CheckCategory::Archive,\n                    name: format!(\"Parse: {}\", file),\n                    status: if warnings.is_empty() { CheckStatus::Pass } else { CheckStatus::Warning },\n                    message: format!(\"{} items parsed\", count),\n                    suggestion: warnings.first().cloned(),\n                });\n            }\n            Err(e) =\u003e {\n                checks.push(HealthCheck {\n                    category: CheckCategory::Archive,\n                    name: format!(\"Parse: {}\", file),\n                    status: CheckStatus::Error,\n                    message: format!(\"Parse error: {}\", e),\n                    suggestion: Some(\"Check file is not corrupted\".into()),\n                });\n            }\n        }\n    }\n    checks\n}\n\nfn validate_js_wrapped_json(path: \u0026Path) -\u003e Result\u003c(usize, Vec\u003cString\u003e)\u003e {\n    let content = fs::read_to_string(path)?;\n    \n    // Strip JS wrapper: window.YTD.tweets.part0 = [...]\n    let json_start = content.find('[')\n        .ok_or_else(|| anyhow!(\"No JSON array found\"))?;\n    let json = \u0026content[json_start..];\n    \n    // Parse and count\n    let items: Vec\u003cValue\u003e = serde_json::from_str(json)?;\n    let mut warnings = Vec::new();\n    \n    // Check for common issues\n    for (i, item) in items.iter().enumerate() {\n        if item[\"tweet\"][\"id_str\"].is_null() {\n            warnings.push(format!(\"Item {} missing id_str\", i));\n        }\n    }\n    \n    Ok((items.len(), warnings))\n}\n```\n\n### 3. Duplicate ID Detection\n\n```rust\nfn check_duplicate_ids(archive_path: \u0026Path) -\u003e HealthCheck {\n    let mut seen_ids: HashSet\u003cString\u003e = HashSet::new();\n    let mut duplicates: Vec\u003cString\u003e = Vec::new();\n    \n    // Load all tweet IDs from archive\n    if let Ok(tweets) = parse_tweets_file(archive_path) {\n        for tweet in tweets {\n            if !seen_ids.insert(tweet.id.clone()) {\n                duplicates.push(tweet.id);\n            }\n        }\n    }\n    \n    HealthCheck {\n        category: CheckCategory::Archive,\n        name: \"Duplicate Tweet IDs\".into(),\n        status: if duplicates.is_empty() { CheckStatus::Pass } else { CheckStatus::Warning },\n        message: if duplicates.is_empty() {\n            \"No duplicates detected\".into()\n        } else {\n            format!(\"{} duplicate IDs found\", duplicates.len())\n        },\n        suggestion: if !duplicates.is_empty() {\n            Some(format!(\"Duplicate IDs: {}...\", duplicates[..3.min(duplicates.len())].join(\", \")))\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 4. Timestamp Consistency\n\n```rust\nfn check_timestamp_consistency(archive_path: \u0026Path) -\u003e HealthCheck {\n    let mut issues = Vec::new();\n    \n    if let Ok(tweets) = parse_tweets_file(archive_path) {\n        for tweet in \u0026tweets {\n            // Check for future dates\n            if tweet.created_at \u003e Utc::now() {\n                issues.push(format!(\"{}: future date\", tweet.id));\n            }\n            // Check for impossibly old dates (before Twitter existed)\n            if tweet.created_at.year() \u003c 2006 {\n                issues.push(format!(\"{}: before 2006\", tweet.id));\n            }\n        }\n    }\n    \n    HealthCheck {\n        category: CheckCategory::Archive,\n        name: \"Timestamp Validity\".into(),\n        status: if issues.is_empty() { CheckStatus::Pass } else { CheckStatus::Warning },\n        message: if issues.is_empty() {\n            \"All timestamps valid\".into()\n        } else {\n            format!(\"{} timestamp issues\", issues.len())\n        },\n        suggestion: None,\n    }\n}\n```\n\n## Logging\n\n- Log each file being checked with tracing::debug!\n- Log parse errors with tracing::warn!\n- Log timing for large files with tracing::info!\n\n## Acceptance Criteria\n\n- [ ] Required files check works\n- [ ] Optional files don't cause errors\n- [ ] JSON parsing validates structure\n- [ ] Duplicate detection scans all tweets\n- [ ] Timestamp validation catches anomalies\n- [ ] All checks return HealthCheck structs\n- [ ] Performance \u003c 5s for 100k tweet archive","status":"closed","priority":1,"issue_type":"task","assignee":"MagentaFox","created_at":"2026-01-10T19:09:35.993651-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:19:26.23636315-05:00","closed_at":"2026-01-10T21:19:26.23636315-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.4.1","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.2","title":"Implement Tantivy index health check","description":"## Goal\n\nImplement Tantivy index health checks to verify the search index is valid and performant.\n\n## Checks to Implement\n\n### 1. Index Directory Existence\n\n```rust\nfn check_index_directory(index_path: \u0026Path) -\u003e HealthCheck {\n    if !index_path.exists() {\n        return HealthCheck {\n            category: CheckCategory::Index,\n            name: \"Index Directory\".into(),\n            status: CheckStatus::Error,\n            message: format!(\"Index not found at {:?}\", index_path),\n            suggestion: Some(\"Run 'xf index' to create the index\".into()),\n        };\n    }\n    \n    // Check for required Tantivy files\n    let meta_path = index_path.join(\"meta.json\");\n    if !meta_path.exists() {\n        return HealthCheck {\n            category: CheckCategory::Index,\n            name: \"Index Directory\".into(),\n            status: CheckStatus::Error,\n            message: \"Missing meta.json - index may be corrupted\".into(),\n            suggestion: Some(\"Run 'xf reindex' to rebuild\".into()),\n        };\n    }\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Index Directory\".into(),\n        status: CheckStatus::Pass,\n        message: format!(\"Found at {:?}\", index_path),\n        suggestion: None,\n    }\n}\n```\n\n### 2. Index Version Compatibility\n\n```rust\nfn check_index_version(index: \u0026TantivyIndex) -\u003e HealthCheck {\n    let meta = index.load_metas()?;\n    let index_version = meta.index_version();\n    let current_version = tantivy::version();\n    \n    let compatible = is_version_compatible(index_version, current_version);\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Index Version\".into(),\n        status: if compatible { CheckStatus::Pass } else { CheckStatus::Warning },\n        message: format!(\"Index v{} (current: v{})\", index_version, current_version),\n        suggestion: if !compatible {\n            Some(\"Consider 'xf reindex' for latest format\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 3. Segment Health\n\n```rust\nfn check_segments(index: \u0026TantivyIndex) -\u003e HealthCheck {\n    let reader = index.reader()?;\n    let searcher = reader.searcher();\n    let segment_count = searcher.segment_readers().len();\n    \n    let status = match segment_count {\n        0 =\u003e CheckStatus::Warning,\n        1..=10 =\u003e CheckStatus::Pass,\n        11..=50 =\u003e CheckStatus::Warning,\n        _ =\u003e CheckStatus::Warning,\n    };\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Segment Count\".into(),\n        status,\n        message: format!(\"{} segments\", segment_count),\n        suggestion: if segment_count \u003e 10 {\n            Some(\"Run 'xf optimize' to merge segments\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 4. Document Count Verification\n\n```rust\nfn check_document_count(index: \u0026TantivyIndex, storage: \u0026Storage) -\u003e HealthCheck {\n    let reader = index.reader()?;\n    let searcher = reader.searcher();\n    let index_count = searcher.num_docs() as i64;\n    \n    // Get expected count from database\n    let db_count: i64 = storage.conn\n        .query_row(\"SELECT COUNT(*) FROM tweets\", [], |r| r.get(0))?;\n    \n    let diff = (index_count - db_count).abs();\n    let percentage_diff = if db_count \u003e 0 {\n        (diff as f64 / db_count as f64) * 100.0\n    } else {\n        0.0\n    };\n    \n    let status = if diff == 0 {\n        CheckStatus::Pass\n    } else if percentage_diff \u003c 1.0 {\n        CheckStatus::Warning\n    } else {\n        CheckStatus::Error\n    };\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Document Count\".into(),\n        status,\n        message: format!(\"Index: {}, DB: {} (diff: {})\", index_count, db_count, diff),\n        suggestion: if diff \u003e 0 {\n            Some(\"Run 'xf reindex' to sync\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 5. Sample Query Test\n\n```rust\nfn check_sample_query(index: \u0026TantivyIndex) -\u003e HealthCheck {\n    let start = Instant::now();\n    \n    let result = index.search(\"test\", 1);  // Simple query, limit 1\n    \n    let duration = start.elapsed();\n    let duration_ms = duration.as_secs_f64() * 1000.0;\n    \n    match result {\n        Ok(_) =\u003e {\n            let status = if duration_ms \u003c 10.0 {\n                CheckStatus::Pass\n            } else if duration_ms \u003c 100.0 {\n                CheckStatus::Warning\n            } else {\n                CheckStatus::Warning\n            };\n            \n            HealthCheck {\n                category: CheckCategory::Index,\n                name: \"Sample Query\".into(),\n                status,\n                message: format!(\"{:.1}ms\", duration_ms),\n                suggestion: if duration_ms \u003e 10.0 {\n                    Some(\"Consider 'xf optimize' for faster queries\".into())\n                } else {\n                    None\n                },\n            }\n        }\n        Err(e) =\u003e HealthCheck {\n            category: CheckCategory::Index,\n            name: \"Sample Query\".into(),\n            status: CheckStatus::Error,\n            message: format!(\"Query failed: {}\", e),\n            suggestion: Some(\"Index may be corrupted. Try 'xf reindex'\".into()),\n        },\n    }\n}\n```\n\n### 6. Index Size Check\n\n```rust\nfn check_index_size(index_path: \u0026Path) -\u003e HealthCheck {\n    let size_bytes = calculate_directory_size(index_path)?;\n    let size_mb = size_bytes as f64 / (1024.0 * 1024.0);\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Index Size\".into(),\n        status: CheckStatus::Pass,  // Informational\n        message: format!(\"{:.1} MB\", size_mb),\n        suggestion: if size_mb \u003e 500.0 {\n            Some(\"Large index. Consider 'xf optimize' to reduce size\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n## Acceptance Criteria\n\n- [ ] Index directory check works\n- [ ] Version compatibility check implemented\n- [ ] Segment count reported with warnings\n- [ ] Document count compared to database\n- [ ] Sample query executes and times\n- [ ] Index size reported\n- [ ] All checks return HealthCheck structs\n- [ ] Graceful handling of corrupted indexes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:09:36.518068-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:16:18.578901895-05:00","closed_at":"2026-01-10T21:16:18.578901895-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.4.2","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.3","title":"Implement SQLite database health check","description":"Run PRAGMA integrity_check. Verify FTS5 index integrity. Check for orphaned records between tables. Report table sizes and row counts. Detect schema version mismatches.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:09:36.88663-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:00:15.217598373-05:00","closed_at":"2026-01-10T21:00:15.217598373-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.4.3","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.4","title":"Add performance benchmarks to doctor","description":"Measure index load time. Benchmark simple and complex queries. Report query latency percentiles. Compare against expected baselines. Flag performance regressions.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:09:37.267085-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:09:37.267085-05:00","dependencies":[{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.5","title":"Add doctor CLI subcommand with fix suggestions","description":"Add Doctor variant to Commands enum. Wire up all checks. Format output with emoji status indicators. Provide actionable fix commands (e.g., 'xf reindex' suggestion). Add --fix flag for auto-repair where safe.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:09:37.696048-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:09:37.696048-05:00","dependencies":[{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.6","title":"Add tests for doctor command","description":"## Goal\n\nComprehensive test coverage for the doctor command including unit tests and E2E validation.\n\n## Unit Tests (doctor.rs)\n\n### Archive Validation Tests\n\n```rust\n#[test]\nfn test_check_required_files_all_present() {\n    let archive = create_test_archive_with_files(\u0026[\n        \"data/tweets.js\",\n        \"data/like.js\",\n    ]);\n    let checks = check_required_files(\u0026archive);\n    assert!(checks.iter().all(|c| c.status == CheckStatus::Pass || c.status == CheckStatus::Warning));\n}\n\n#[test]\nfn test_check_required_files_missing_tweets() {\n    let archive = create_test_archive_with_files(\u0026[\"data/like.js\"]);\n    let checks = check_required_files(\u0026archive);\n    let tweets_check = checks.iter().find(|c| c.name.contains(\"tweets\")).unwrap();\n    assert_eq!(tweets_check.status, CheckStatus::Error);\n}\n\n#[test]\nfn test_check_json_structure_valid() {\n    let archive = create_test_archive_with_content(\n        \"data/tweets.js\",\n        \"window.YTD.tweets.part0 = [{\\\"tweet\\\":{\\\"id_str\\\":\\\"123\\\"}}]\"\n    );\n    let checks = check_json_structure(\u0026archive);\n    assert!(checks.iter().all(|c| c.status == CheckStatus::Pass));\n}\n\n#[test]\nfn test_check_json_structure_invalid() {\n    let archive = create_test_archive_with_content(\n        \"data/tweets.js\",\n        \"this is not valid json at all\"\n    );\n    let checks = check_json_structure(\u0026archive);\n    let check = checks.first().unwrap();\n    assert_eq!(check.status, CheckStatus::Error);\n}\n\n#[test]\nfn test_check_duplicate_ids_none() {\n    let archive = create_test_archive_with_tweets(vec![\n        (\"1\", \"tweet 1\"),\n        (\"2\", \"tweet 2\"),\n        (\"3\", \"tweet 3\"),\n    ]);\n    let check = check_duplicate_ids(\u0026archive);\n    assert_eq!(check.status, CheckStatus::Pass);\n}\n\n#[test]\nfn test_check_duplicate_ids_found() {\n    let archive = create_test_archive_with_tweets(vec![\n        (\"1\", \"tweet 1\"),\n        (\"1\", \"duplicate!\"),  // Same ID\n        (\"2\", \"tweet 2\"),\n    ]);\n    let check = check_duplicate_ids(\u0026archive);\n    assert_eq!(check.status, CheckStatus::Warning);\n    assert!(check.message.contains(\"1 duplicate\"));\n}\n```\n\n### Database Health Tests\n\n```rust\n#[test]\nfn test_sqlite_integrity_check_healthy() {\n    let storage = create_healthy_test_storage();\n    let check = check_sqlite_integrity(\u0026storage);\n    assert_eq!(check.status, CheckStatus::Pass);\n}\n\n#[test]\nfn test_fts5_integrity_check() {\n    let storage = create_test_storage_with_fts();\n    let check = check_fts5_integrity(\u0026storage);\n    assert_eq!(check.status, CheckStatus::Pass);\n}\n\n#[test]\nfn test_orphaned_records_detection() {\n    let storage = create_storage_with_orphans();\n    let check = check_orphaned_records(\u0026storage);\n    assert_eq!(check.status, CheckStatus::Warning);\n    assert!(check.suggestion.is_some());\n}\n\n#[test]\nfn test_table_count_mismatch() {\n    let storage = create_storage_with_count_mismatch();\n    let check = check_table_counts(\u0026storage);\n    assert_eq!(check.status, CheckStatus::Warning);\n}\n```\n\n### Index Health Tests\n\n```rust\n#[test]\nfn test_tantivy_index_healthy() {\n    let index = create_healthy_test_index();\n    let checks = check_tantivy_health(\u0026index);\n    assert!(checks.iter().all(|c| c.status == CheckStatus::Pass));\n}\n\n#[test]\nfn test_tantivy_sample_query() {\n    let index = create_test_index_with_docs();\n    let check = check_sample_query(\u0026index);\n    assert_eq!(check.status, CheckStatus::Pass);\n    assert!(check.message.contains(\"ms\"));\n}\n```\n\n### Performance Tests\n\n```rust\n#[test]\nfn test_performance_baseline() {\n    let storage = create_test_storage();\n    let index = create_test_index();\n    let checks = run_performance_checks(\u0026storage, \u0026index);\n    \n    for check in \u0026checks {\n        // All should complete (may warn if slow)\n        assert_ne!(check.status, CheckStatus::Error);\n    }\n}\n\n#[test]\nfn test_doctor_completes_under_10s() {\n    let storage = create_large_test_storage(100_000);\n    let index = create_large_test_index(100_000);\n    \n    let start = Instant::now();\n    let _ = run_all_checks(\u0026storage, \u0026index);\n    assert!(start.elapsed() \u003c Duration::from_secs(10));\n}\n```\n\n## Integration Tests\n\n```rust\n#[test]\nfn test_doctor_cli_output() {\n    let output = run_xf(\u0026[\"doctor\"]);\n    assert!(output.contains(\"ARCHIVE\"));\n    assert!(output.contains(\"DATABASE\"));\n    assert!(output.contains(\"INDEX\"));\n    assert!(output.contains(\"PERFORMANCE\"));\n}\n\n#[test]\nfn test_doctor_json_output() {\n    let output = run_xf(\u0026[\"doctor\", \"--format\", \"json\"]);\n    let json: Value = serde_json::from_str(\u0026output).unwrap();\n    assert!(json[\"checks\"].is_array());\n    assert!(json[\"summary\"].is_object());\n}\n\n#[test]\nfn test_doctor_exit_codes() {\n    // Healthy archive\n    let status = run_xf_status(\u0026[\"doctor\"]);\n    assert_eq!(status, 0);\n    \n    // With warnings (should still be 0)\n    // ... setup archive with warnings ...\n    \n    // With errors (should be 1)\n    // ... setup corrupted archive ...\n}\n```\n\n## E2E Test Script: tests/e2e/doctor_test.sh\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\nLOG_FILE=\"./test-output/doctor_e2e_$(date +%Y%m%d_%H%M%S).log\"\nmkdir -p ./test-output\n\nlog() { echo \"[$(date +%H:%M:%S)] $*\" | tee -a \"$LOG_FILE\"; }\n\nlog \"=== Doctor E2E Test Suite ===\"\n\n# Test 1: Doctor runs on healthy archive\nlog \"[TEST 1] Doctor on healthy archive\"\nif xf doctor --db test-archives/healthy/xf.db 2\u003e\u00261 | tee -a \"$LOG_FILE\" | grep -q \"SUMMARY\"; then\n    log \"[PASS] Doctor completed successfully\"\nelse\n    log \"[FAIL] Doctor did not complete\"\n    exit 1\nfi\n\n# Test 2: Check sections present\nlog \"[TEST 2] All sections present\"\nOUTPUT=$(xf doctor --db test-archives/healthy/xf.db 2\u003e\u00261)\nfor section in \"ARCHIVE\" \"DATABASE\" \"INDEX\" \"PERFORMANCE\"; do\n    if echo \"$OUTPUT\" | grep -q \"$section\"; then\n        log \"[PASS] Section $section present\"\n    else\n        log \"[FAIL] Section $section missing\"\n        exit 1\n    fi\ndone\n\n# Test 3: JSON output valid\nlog \"[TEST 3] JSON output\"\nOUTPUT=$(xf doctor --db test-archives/healthy/xf.db --format json 2\u003e\u00261)\nif echo \"$OUTPUT\" | jq . \u003e/dev/null 2\u003e\u00261; then\n    log \"[PASS] JSON output valid\"\nelse\n    log \"[FAIL] JSON output invalid\"\n    exit 1\nfi\n\n# Test 4: Performance under threshold\nlog \"[TEST 4] Performance benchmark\"\nSTART=$(date +%s)\nxf doctor --db test-archives/large/xf.db \u003e/dev/null 2\u003e\u00261\nEND=$(date +%s)\nDURATION=$((END - START))\nif [ \"$DURATION\" -le 10 ]; then\n    log \"[PASS] Completed in ${DURATION}s (under 10s)\"\nelse\n    log \"[FAIL] Took ${DURATION}s (over 10s threshold)\"\n    exit 1\nfi\n\n# Test 5: Exit code 0 for healthy\nlog \"[TEST 5] Exit code for healthy archive\"\nxf doctor --db test-archives/healthy/xf.db \u003e/dev/null 2\u003e\u00261\nif [ $? -eq 0 ]; then\n    log \"[PASS] Exit code 0 for healthy archive\"\nelse\n    log \"[FAIL] Non-zero exit code for healthy archive\"\n    exit 1\nfi\n\nlog \"=== All Doctor E2E tests passed ===\"\n```\n\n## Logging Requirements\n\n- Log each check start/end with tracing::debug!\n- Log check results with tracing::info!\n- Log failures with tracing::warn!\n- Include timing for performance checks\n\n## Acceptance Criteria\n\n- [ ] Unit tests for all check categories\n- [ ] Integration tests for CLI output\n- [ ] E2E script validates real archives\n- [ ] Performance test validates \u003c 10s\n- [ ] Exit code tests pass\n- [ ] Test coverage \u003e 80% for doctor.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:10:03.233128-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:32:10.983349-05:00","dependencies":[{"issue_id":"xf-11.4.6","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.6","depends_on_id":"xf-11.4.5","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5","title":"Natural Language Date Filtering","description":"## Overview\n\nEnable human-friendly date expressions in --from and --to flags. Parse expressions like 'last week', '3 months ago', 'January 2023', 'Q4 2022'. Drastically improves usability for temporal queries without requiring exact ISO timestamps.\n\n## Background \u0026 Motivation\n\nCurrent behavior requires ISO format:\n```bash\nxf search \"meeting\" --from 2023-01-01 --to 2023-03-31  # Works\nxf search \"meeting\" --from \"last month\"               # Error\\!\n```\n\nUsers naturally want to say:\n- \"from last week\"\n- \"since 3 months ago\"\n- \"in January 2023\"\n- \"during Q4 2022\"\n\nThis is friction that a good CLI should eliminate.\n\n## Expression Types to Support\n\n### Relative Expressions\n| Expression | Meaning |\n|------------|---------|\n| yesterday | Previous day |\n| today | Current day |\n| last week | Previous 7 days |\n| last month | Previous calendar month |\n| last year | Previous calendar year |\n| 3 days ago | 3 days before now |\n| 2 weeks ago | 14 days before now |\n| 6 months ago | 6 months before now |\n| past week | Same as \"last week\" |\n| this month | Current calendar month |\n| this year | Current calendar year |\n\n### Absolute Expressions\n| Expression | Meaning |\n|------------|---------|\n| January 2023 | 2023-01-01 to 2023-01-31 |\n| Jan 2023 | Same as above |\n| 2023-01 | Same as above |\n| March 15, 2023 | Specific date |\n| Mar 15 2023 | Same as above |\n\n### Period Expressions\n| Expression | Meaning |\n|------------|---------|\n| Q1 2024 | Jan 1 - Mar 31, 2024 |\n| Q2 2024 | Apr 1 - Jun 30, 2024 |\n| Q3 2024 | Jul 1 - Sep 30, 2024 |\n| Q4 2024 | Oct 1 - Dec 31, 2024 |\n| summer 2023 | Jun 1 - Aug 31, 2023 |\n| winter 2023 | Dec 1, 2023 - Feb 28, 2024 |\n| spring 2023 | Mar 1 - May 31, 2023 |\n| fall 2023 | Sep 1 - Nov 30, 2023 |\n\n## Technical Approach\n\n### Option A: Use chrono-english crate\n```toml\n[dependencies]\nchrono-english = \"0.1\"\n```\n\nPros: Well-tested, handles many cases\nCons: May not support all our expressions\n\n### Option B: Custom parser with nom\nBuild tailored parser for exactly our needs.\n\nPros: Full control, optimized for our use case\nCons: More code to maintain\n\n### Recommendation: Hybrid\nUse chrono-english for standard cases, add custom parsing for:\n- Quarter expressions (Q1, Q2, etc.)\n- Season expressions (summer, winter, etc.)\n- Shorthand months (Jan, Feb, etc.)\n\n## CLI Integration\n\n### Flag Behavior\n\n```rust\n// In cli.rs SearchArgs\n#[arg(long)]\npub from: Option\u003cString\u003e,  // Was Option\u003cNaiveDate\u003e\n\n#[arg(long)]\npub to: Option\u003cString\u003e,    // Was Option\u003cNaiveDate\u003e\n```\n\n### Parsing in main.rs\n\n```rust\nfn parse_date_arg(input: \u0026str, is_end: bool) -\u003e Result\u003cDateTime\u003cUtc\u003e\u003e {\n    // Try natural language first\n    if let Ok(dt) = parse_human_date(input, is_end) {\n        return Ok(dt);\n    }\n    \n    // Fall back to ISO format\n    if let Ok(date) = NaiveDate::parse_from_str(input, \"%Y-%m-%d\") {\n        let time = if is_end { \n            NaiveTime::from_hms(23, 59, 59) \n        } else { \n            NaiveTime::from_hms(0, 0, 0) \n        };\n        return Ok(date.and_time(time).and_utc());\n    }\n    \n    Err(anyhow\\!(\"Could not parse date: '{}'\", input))\n}\n```\n\n### Verbose Mode Confirmation\n\n```rust\nif cli.verbose {\n    if let Some(from) = \u0026parsed_from {\n        eprintln\\!(\"Parsed --from '{}' as {}\", original, from.format(\"%Y-%m-%d\"));\n    }\n}\n```\n\n## Timezone Handling\n\n- All calculations relative to local timezone\n- Store in UTC for consistency\n- Display in local timezone unless --utc flag\n\n## Acceptance Criteria\n\n- [ ] Relative expressions work (yesterday, last week, etc.)\n- [ ] Absolute expressions work (January 2023, etc.)\n- [ ] Quarter expressions work (Q1 2024, etc.)\n- [ ] Fallback to ISO format works\n- [ ] Verbose mode shows parsed dates\n- [ ] Invalid expressions give clear error messages\n- [ ] Timezone handling is consistent","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-10T19:10:11.839368-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:33:17.655644-05:00","dependencies":[{"issue_id":"xf-11.5","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.1","title":"Add date_parser.rs module with chrono-english","description":"## Goal\n\nCreate a new date_parser.rs module for parsing human-friendly date expressions.\n\n## Dependencies to Add (Cargo.toml)\n\n```toml\n[dependencies]\nchrono-english = \"0.1\"  # For natural language parsing\n```\n\n## New Module: src/date_parser.rs\n\n### Core Types\n\n```rust\nuse chrono::{DateTime, NaiveDate, NaiveTime, Utc, Local, Duration};\nuse chrono_english::{parse_date_string, Dialect};\n\n/// Result of parsing a date expression\n#[derive(Debug, Clone)]\npub enum ParsedDate {\n    /// A specific point in time\n    Point(DateTime\u003cUtc\u003e),\n    /// A date range (for expressions like \"January 2023\")\n    Range { start: DateTime\u003cUtc\u003e, end: DateTime\u003cUtc\u003e },\n}\n\nimpl ParsedDate {\n    /// Get the start of this date (for --from)\n    pub fn start(\u0026self) -\u003e DateTime\u003cUtc\u003e {\n        match self {\n            Self::Point(dt) =\u003e *dt,\n            Self::Range { start, .. } =\u003e *start,\n        }\n    }\n    \n    /// Get the end of this date (for --to)\n    pub fn end(\u0026self) -\u003e DateTime\u003cUtc\u003e {\n        match self {\n            Self::Point(dt) =\u003e *dt,\n            Self::Range { end, .. } =\u003e *end,\n        }\n    }\n}\n```\n\n### Main Parsing Function\n\n```rust\n/// Parse a human-readable date expression.\n///\n/// # Arguments\n/// *  - The date string to parse\n/// *  - If true, prefer end of period (for --to); otherwise start (for --from)\n///\n/// # Examples\n/// ```\n/// parse_human_date(\"yesterday\", false)?;       // Start of yesterday\n/// parse_human_date(\"last week\", true)?;        // End of last week\n/// parse_human_date(\"January 2023\", false)?;    // 2023-01-01\n/// parse_human_date(\"Q4 2022\", true)?;          // 2022-12-31\n/// ```\npub fn parse_human_date(input: \u0026str, prefer_end: bool) -\u003e Result\u003cParsedDate\u003e {\n    let input = input.trim().to_lowercase();\n    \n    // Try custom parsers first (quarters, seasons, months)\n    if let Some(parsed) = try_parse_quarter(\u0026input) {\n        return Ok(parsed);\n    }\n    if let Some(parsed) = try_parse_season(\u0026input) {\n        return Ok(parsed);\n    }\n    if let Some(parsed) = try_parse_month_year(\u0026input) {\n        return Ok(parsed);\n    }\n    \n    // Try chrono-english for natural language\n    let now = Local::now();\n    match parse_date_string(\u0026input, now, Dialect::Us) {\n        Ok(dt) =\u003e Ok(ParsedDate::Point(dt.with_timezone(\u0026Utc))),\n        Err(_) =\u003e {\n            // Try relative expressions manually\n            if let Some(parsed) = try_parse_relative(\u0026input) {\n                return Ok(parsed);\n            }\n            \n            Err(anyhow\\!(\"Could not parse date expression: '{}'\", input))\n        }\n    }\n}\n```\n\n### ISO Fallback\n\n```rust\n/// Try parsing as ISO format (YYYY-MM-DD)\npub fn try_parse_iso(input: \u0026str) -\u003e Option\u003cDateTime\u003cUtc\u003e\u003e {\n    NaiveDate::parse_from_str(input, \"%Y-%m-%d\")\n        .ok()\n        .map(|d| d.and_hms_opt(0, 0, 0).unwrap().and_utc())\n}\n\n/// Unified parser: try natural language, fall back to ISO\npub fn parse_date_flexible(input: \u0026str, prefer_end: bool) -\u003e Result\u003cDateTime\u003cUtc\u003e\u003e {\n    // Try human date first\n    if let Ok(parsed) = parse_human_date(input, prefer_end) {\n        return Ok(if prefer_end { parsed.end() } else { parsed.start() });\n    }\n    \n    // Fall back to ISO\n    try_parse_iso(input)\n        .ok_or_else(|| anyhow\\!(\"Could not parse '{}' as date\", input))\n}\n```\n\n## Logging\n\n- Log successful parses with tracing::debug\\!\n- Log fallback to ISO with tracing::trace\\!\n- Log parse failures with tracing::warn\\!\n\n## Acceptance Criteria\n\n- [ ] date_parser.rs module created\n- [ ] ParsedDate enum handles points and ranges\n- [ ] chrono-english integrated\n- [ ] ISO fallback works\n- [ ] Module exports parse_human_date and parse_date_flexible\n- [ ] Basic tests for common expressions\n- [ ] cargo check passes","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:10:25.649601-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:34:20.658091-05:00","dependencies":[{"issue_id":"xf-11.5.1","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.2","title":"Add relative date expressions support","description":"Parse expressions: 'N days/weeks/months/years ago', 'last N days/weeks/months', 'past week', 'this month'. Handle timezone awareness. Compute relative to current date at query time.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:10:26.178503-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:10:26.178503-05:00","dependencies":[{"issue_id":"xf-11.5.2","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.2","depends_on_id":"xf-11.5.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.3","title":"Add named period expressions support","description":"Parse: 'January 2023', 'Q1 2024', 'summer 2022', 'weekend', 'weekdays'. Map to date ranges. Q1=Jan-Mar, Q2=Apr-Jun, Q3=Jul-Sep, Q4=Oct-Dec. Summer=Jun-Aug, etc.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:10:26.655467-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:10:26.655467-05:00","dependencies":[{"issue_id":"xf-11.5.3","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.3","depends_on_id":"xf-11.5.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.4","title":"Integrate date parser with CLI flags","description":"Modify --from and --to argument parsing in main.rs. Try natural language first, fall back to ISO format. Display parsed date in verbose mode for user confirmation. Update help text with examples.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:10:27.16169-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:10:27.16169-05:00","dependencies":[{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.5","title":"Add comprehensive tests for date parsing","description":"## Goal\n\nComprehensive test coverage for the date parsing module with edge cases and E2E validation.\n\n## Unit Tests (date_parser.rs)\n\n### Relative Expression Tests\n\n```rust\n#[test]\nfn test_parse_yesterday() {\n    let now = Utc::now();\n    let result = parse_human_date(\"yesterday\", false).unwrap();\n    let expected = (now - Duration::days(1)).date_naive();\n    assert_eq!(result.start().date_naive(), expected);\n}\n\n#[test]\nfn test_parse_last_week() {\n    let result = parse_human_date(\"last week\", false).unwrap();\n    let now = Utc::now();\n    let week_ago = now - Duration::days(7);\n    assert!(result.start() \u003c= week_ago);\n}\n\n#[test]\nfn test_parse_3_months_ago() {\n    let result = parse_human_date(\"3 months ago\", false).unwrap();\n    let now = Utc::now();\n    // Should be approximately 90 days ago\n    let diff = now - result.start();\n    assert!(diff.num_days() \u003e= 85 \u0026\u0026 diff.num_days() \u003c= 95);\n}\n\n#[test]\nfn test_parse_this_month() {\n    let result = parse_human_date(\"this month\", false).unwrap();\n    let now = Utc::now();\n    assert_eq!(result.start().month(), now.month());\n    assert_eq!(result.start().day(), 1);\n}\n```\n\n### Absolute Expression Tests\n\n```rust\n#[test]\nfn test_parse_january_2023() {\n    let result = parse_human_date(\"January 2023\", false).unwrap();\n    match result {\n        ParsedDate::Range { start, end } =\u003e {\n            assert_eq!(start.year(), 2023);\n            assert_eq!(start.month(), 1);\n            assert_eq!(start.day(), 1);\n            assert_eq!(end.day(), 31);\n        }\n        _ =\u003e panic!(\"Expected range\"),\n    }\n}\n\n#[test]\nfn test_parse_jan_2023() {\n    // Short form\n    let result = parse_human_date(\"Jan 2023\", false).unwrap();\n    assert_eq!(result.start().month(), 1);\n}\n\n#[test]\nfn test_parse_specific_date() {\n    let result = parse_human_date(\"March 15, 2023\", false).unwrap();\n    let date = result.start().date_naive();\n    assert_eq!(date, NaiveDate::from_ymd_opt(2023, 3, 15).unwrap());\n}\n```\n\n### Quarter Expression Tests\n\n```rust\n#[test]\nfn test_parse_q1_2024() {\n    let result = parse_human_date(\"Q1 2024\", false).unwrap();\n    match result {\n        ParsedDate::Range { start, end } =\u003e {\n            assert_eq!(start, Utc.with_ymd_and_hms(2024, 1, 1, 0, 0, 0).unwrap());\n            assert_eq!(end, Utc.with_ymd_and_hms(2024, 3, 31, 23, 59, 59).unwrap());\n        }\n        _ =\u003e panic!(\"Expected range\"),\n    }\n}\n\n#[test]\nfn test_parse_q4_2022() {\n    let result = parse_human_date(\"Q4 2022\", false).unwrap();\n    if let ParsedDate::Range { start, end } = result {\n        assert_eq!(start.month(), 10);\n        assert_eq!(end.month(), 12);\n    } else {\n        panic!(\"Expected range\");\n    }\n}\n```\n\n### Season Expression Tests\n\n```rust\n#[test]\nfn test_parse_summer_2023() {\n    let result = parse_human_date(\"summer 2023\", false).unwrap();\n    if let ParsedDate::Range { start, end } = result {\n        assert_eq!(start.month(), 6);  // June\n        assert_eq!(end.month(), 8);    // August\n    } else {\n        panic!(\"Expected range\");\n    }\n}\n\n#[test]\nfn test_parse_winter_2023() {\n    // Winter spans year boundary: Dec 2023 - Feb 2024\n    let result = parse_human_date(\"winter 2023\", false).unwrap();\n    if let ParsedDate::Range { start, end } = result {\n        assert_eq!(start.year(), 2023);\n        assert_eq!(start.month(), 12);\n        assert_eq!(end.year(), 2024);\n        assert_eq!(end.month(), 2);\n    } else {\n        panic!(\"Expected range\");\n    }\n}\n```\n\n### Edge Case Tests\n\n```rust\n#[test]\nfn test_parse_leap_year() {\n    let result = parse_human_date(\"February 2024\", false).unwrap();\n    if let ParsedDate::Range { end, .. } = result {\n        assert_eq!(end.day(), 29);  // 2024 is a leap year\n    }\n}\n\n#[test]\nfn test_parse_invalid_expression() {\n    let result = parse_human_date(\"not a date\", false);\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_parse_iso_fallback() {\n    let result = parse_date_flexible(\"2023-06-15\", false).unwrap();\n    assert_eq!(result.date_naive(), NaiveDate::from_ymd_opt(2023, 6, 15).unwrap());\n}\n\n#[test]\nfn test_parse_empty_string() {\n    let result = parse_human_date(\"\", false);\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_parse_future_date() {\n    // Should work even for future dates\n    let result = parse_human_date(\"January 2030\", false);\n    assert!(result.is_ok());\n}\n\n#[test]\nfn test_prefer_end_vs_start() {\n    let start = parse_human_date(\"January 2023\", false).unwrap().start();\n    let end = parse_human_date(\"January 2023\", true).unwrap().end();\n    \n    assert_eq!(start.day(), 1);\n    assert_eq!(end.day(), 31);\n}\n```\n\n## Integration Tests\n\n```rust\n#[test]\nfn test_search_with_natural_dates() {\n    let output = run_xf(\u0026[\"search\", \"test\", \"--from\", \"last month\"]);\n    assert!(!output.contains(\"Could not parse\"));\n}\n\n#[test]\nfn test_search_with_quarter() {\n    let output = run_xf(\u0026[\"search\", \"test\", \"--from\", \"Q1 2023\", \"--to\", \"Q2 2023\"]);\n    assert!(!output.contains(\"error\"));\n}\n\n#[test]\nfn test_verbose_shows_parsed_date() {\n    let output = run_xf(\u0026[\"search\", \"test\", \"--from\", \"last week\", \"--verbose\"]);\n    assert!(output.contains(\"Parsed\") || output.contains(\"parsed\"));\n}\n```\n\n## E2E Test Script: tests/e2e/date_parsing_test.sh\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\nLOG_FILE=\"./test-output/date_parsing_e2e_$(date +%Y%m%d_%H%M%S).log\"\nmkdir -p ./test-output\n\nlog() { echo \"[$(date +%H:%M:%S)] $*\" | tee -a \"$LOG_FILE\"; }\n\nlog \"=== Date Parsing E2E Test Suite ===\"\n\n# Test expressions\nEXPRESSIONS=(\n    \"yesterday\"\n    \"last week\"\n    \"last month\"\n    \"3 months ago\"\n    \"January 2023\"\n    \"Q1 2024\"\n    \"summer 2023\"\n    \"2023-06-15\"\n)\n\nPASSED=0\nFAILED=0\n\nfor expr in \"${EXPRESSIONS[@]}\"; do\n    log \"Testing: '$expr'\"\n    if xf search \"test\" --from \"$expr\" --limit 1 2\u003e\u00261 | tee -a \"$LOG_FILE\" | grep -qv \"Could not parse\"; then\n        log \"[PASS] '$expr' parsed successfully\"\n        ((PASSED++))\n    else\n        log \"[FAIL] '$expr' failed to parse\"\n        ((FAILED++))\n    fi\ndone\n\n# Test invalid expression gives good error\nlog \"Testing: invalid expression\"\nOUTPUT=$(xf search \"test\" --from \"not a date\" 2\u003e\u00261 || true)\nif echo \"$OUTPUT\" | grep -qi \"could not parse\\|invalid\\|error\"; then\n    log \"[PASS] Invalid expression gives error message\"\n    ((PASSED++))\nelse\n    log \"[FAIL] Invalid expression should give error\"\n    ((FAILED++))\nfi\n\nlog \"\"\nlog \"=== Summary ===\"\nlog \"Passed: $PASSED\"\nlog \"Failed: $FAILED\"\n\nif [ $FAILED -eq 0 ]; then\n    log \"All date parsing tests passed!\"\n    exit 0\nelse\n    log \"Some tests failed!\"\n    exit 1\nfi\n```\n\n## Logging Requirements\n\n- Log each test case with tracing::trace!\n- Log assertion failures with full context\n- Log timing for parsing operations\n\n## Acceptance Criteria\n\n- [ ] Unit tests for all expression types\n- [ ] Edge case tests (leap years, boundaries)\n- [ ] Integration tests for CLI\n- [ ] E2E script validates common expressions\n- [ ] Error message tests\n- [ ] Test coverage \u003e 90% for date_parser.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:10:41.07647-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:34:21.05626-05:00","dependencies":[{"issue_id":"xf-11.5.5","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.5","depends_on_id":"xf-11.5.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-9rf","title":"xf: Full System Investigation, Review, and Performance Optimization","description":"Goal: Provide a self-contained, end-to-end plan to deeply understand the xf codebase, review code (including prior agent changes), identify and fix correctness/reliability issues, and run a rigorous performance investigation and optimization program.\n\nBackground/context:\n- Project root: /data/projects/xf\n- Key documents: AGENTS.md, README.md\n- Dataset provided for profiling: /data/projects/je_twitter_data/twitter-2026-01-09-b207c0562001e0a3e60faf1985feb13876dfbfe85c6831a1c73016daf6d4f2cc.zip (already extracted under /data/projects/je_twitter_data)\n- Perf workflow requirements: baseline metrics (p50/p95/p99, throughput, peak RSS), profile CPU/allocation/I/O, define equivalence oracles, rank opportunities by (Impact x Confidence)/Effort, apply minimal diffs with proof sketches, add regression guardrails.\n\nOutcome:\n- A structured set of tasks/subtasks with dependencies and detailed instructions so future work is self-contained and does not require the original plan document.\n\nNotes/constraints to preserve:\n- Follow AGENTS.md instructions (no file deletion, use apply_patch for edits, etc.).\n- Any optimization must preserve outputs for identical inputs (explicit proof sketch required).\n- Use minimal diffs: one performance lever per change, no unrelated refactors.\n- Add rollback guidance if risk exists.\n","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-10T02:23:41.616748361-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:23:41.616748361-05:00"}
{"id":"xf-9rf.1","title":"Read AGENTS.md + README and capture constraints","description":"Goal:\n- Re-read AGENTS.md and README.md in full and summarize all constraints, workflows, and project goals in a durable form.\n\nWhy:\n- All subsequent investigation, fixes, and performance work must comply with AGENTS.md instructions (e.g., no file deletion, apply_patch usage).\n\nSteps:\n1) Read /data/projects/xf/AGENTS.md carefully (end-to-end). Extract: editing constraints, testing expectations, prohibited actions, perf methodology, and any doc references to best practices.\n2) Read /data/projects/xf/README.md carefully. Extract: project purpose, build/run commands, data formats, expected usage.\n3) Record a concise summary in a working note (or issue comment) that can be referenced without re-opening the source docs.\n\nAcceptance:\n- A clear summary that enumerates all operational constraints and expected workflows.\n- Any referenced best-practice guides or doc links are listed for later lookup.\n","notes":"Summary (AGENTS.md + README):\n- Hard safety rules: never delete files; avoid destructive commands (git reset --hard, rm -rf, git clean -fd) unless user explicitly provides exact command and confirms irreversible consequences. Use safe alternatives first. Document any approved destructive action verbatim.\n- Editing discipline: no script-based code changes; modify files manually; no file proliferation (no *_v2.* style files). Use apply_patch for edits; Cargo-only toolchain; Rust 2024 nightly; unsafe forbidden; use explicit dependency versions.\n- Testing/quality gates required after substantive changes: cargo check --all-targets, cargo clippy --all-targets -- -D warnings, cargo fmt --check; run cargo test and focused tests.\n- Project semantics: xf indexes local X archive data; privacy-first (no network in core runtime); parse JS-wrapped JSON window.YTD.*; Tantivy primary search; SQLite FTS5 fallback; preserve metadata (IDs/timestamps/counts) exactly; CLI flags must be truthful; JSON output shape must remain stable.\n- README highlights: CLI provides index/search commands; supports types filters, offsets/limits, output formats (json/csv/compact); data archive layout includes data/*.js and assets/; parsing strips JS prefix; storage in SQLite + Tantivy; env vars XF_DB/XF_INDEX override storage; default storage paths by OS; performance targets and benches documented; build/test via cargo; installation via script or cargo nightly.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:24:03.88540847-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:25:28.26120732-05:00","closed_at":"2026-01-10T02:25:28.261214513-05:00","dependencies":[{"issue_id":"xf-9rf.1","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:24:03.909918163-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.2","title":"Build architecture map + core workflow understanding","description":"Goal:\n- Produce a high-level architecture map of xf: major modules, data flow, and runtime workflows.\n\nContext:\n- Requires prior understanding of AGENTS.md/README constraints.\n\nSteps:\n1) Inventory key modules in /data/projects/xf/src (main.rs, parser.rs, search.rs, storage.rs, index.rs, etc.).\n2) Trace core flows: indexing path (input archive -\u003e parse -\u003e storage/index), search path (query parse -\u003e execution -\u003e result formatting).\n3) Note data boundaries: DB (SQLite), search index (Tantivy), file I/O paths, and serialization formats.\n4) Record a module graph (imports, public APIs, and cross-module calls).\n\nAcceptance:\n- A concise but complete architecture summary that identifies the main pipeline stages, storage layers, and execution entry points.\n- List of files/functions that are hot paths or correctness-critical.\n","notes":"Architecture map (high level):\n- Entry point: src/main.rs wires CLI (Cli/Commands) to command handlers: index/search/stats/tweet/list/export/config/update/completions.\n- CLI definitions: src/cli.rs declares subcommands + many flags (types/limit/offset/sort/since/until/replies/context/fields, etc.). Some flags are not yet wired in main.rs (noted for later correctness checks).\n- Core modules (src/lib.rs): parser, model, storage (SQLite), search (Tantivy), config/logging/perf/error.\n\nIndexing flow:\n1) xf index \u003carchive\u003e (main.rs cmd_index): validate archive/data/, resolve db/index paths, optionally clear existing data.\n2) ArchiveParser (parser.rs) parses JS-wrapped JSON (window.YTD.*) with rayon; parse_manifest -\u003e ArchiveInfo; parse_tweets/likes/dms/grok/followers/etc into model types.\n3) Storage::open (storage.rs) opens SQLite, sets pragmas, migrates schema, and stores data into normalized tables + FTS5 virtual tables.\n4) SearchEngine::open (search.rs) opens/creates Tantivy index; writer adds documents for each data type (id/text/text_prefix/type/created_at/metadata) and commits.\n\nSearch flow:\n1) xf search \u003cquery\u003e (main.rs cmd_search): open SearchEngine and Storage; map DataType -\u003e search::DocType; call SearchEngine::search with limit+offset.\n2) SearchEngine::search (search.rs) uses Tantivy QueryParser (text + prefix field) and optional type filter; collects TopDocs; builds SearchResult with highlights (snippet generator) and metadata (stored JSON).\n3) main.rs formats results to json/json pretty/csv/compact/text (with highlight -\u003e ANSI conversion in print_result).\n\nData boundaries:\n- Input: archive files under \u003carchive\u003e/data/*.js with JS prefix; parser tolerates whitespace and trailing semicolons.\n- Storage: SQLite schema for tweets/likes/dms/grok + FTS5 tables; metadata in archive_info/meta tables.\n- Search index: Tantivy index under XF_INDEX; schema fields include id, text, text_prefix (prefix matching), type, created_at, metadata.\n\nHot paths / correctness-critical:\n- parser.rs: parse_js_file, parse_* per datatype; date parsing; numeric parsing; message aggregation.\n- search.rs: schema construction, index_* methods, query parsing, snippet generation.\n- storage.rs: schema/migrations + store_* insertions and FTS5 updates.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:25:50.678053409-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:27:35.671086605-05:00","closed_at":"2026-01-10T02:27:35.671094189-05:00","dependencies":[{"issue_id":"xf-9rf.2","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:25:50.679360841-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.2","depends_on_id":"xf-9rf.1","type":"blocks","created_at":"2026-01-10T02:26:00.328123138-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.3","title":"Random deep-dive file investigations + flow traces","description":"Goal:\n- Randomly sample code files and deeply trace execution flows through imports/callers to build intuitive understanding beyond the main path.\n\nSteps:\n1) Use rg/rg --files to list candidate files; pick a random subset across modules (parser, search, storage, CLI, tests, benches).\n2) For each file: identify entry points, key data structures, and how its functions are called from other modules.\n3) Trace at least 2 multi-hop flows (file A -\u003e file B -\u003e file C) with notes on inputs/outputs and invariants.\n4) Record any surprising behavior or potential correctness pitfalls discovered during tracing.\n\nAcceptance:\n- At least 5 files deeply analyzed with call/flow notes.\n- Flow traces identify caller/callee relationships and data transformations across module boundaries.\n","notes":"Deep-dive notes (random file sampling + flow traces):\n- src/main.rs: command dispatcher; cmd_index -\u003e ArchiveParser::parse_* -\u003e Storage::store_* + SearchEngine::index_* -\u003e writer.commit + reload. cmd_search -\u003e SearchEngine::search -\u003e output formatting; cmd_stats/tweet/list/export/config/update are stubs/partial (yellow warnings) in main.\n- src/parser.rs: read_data_file -\u003e parse_js_file (split on first '=') -\u003e serde_json Value -\u003e parse_* into model structs. Uses rayon par_iter for JSON arrays. parse_manifest reads manifest.js and normalizes dates/numeric fields.\n- src/search.rs: schema defines id/text/text_prefix/type/created_at/metadata. index_* builds Tantivy docs; search() builds QueryParser over text+text_prefix, optional type filters with BooleanQuery; SnippetGenerator for highlights; SearchResult assembled with metadata JSON.\n- src/storage.rs: opens SQLite with WAL + perf pragmas; migrate/create schema; store_* inserts per model and maintains FTS5 tables; get_tweet/stats queries. Data normalized; DM messages linked by conversation_id.\n- src/cli.rs: defines flags (types, limit/offset/sort, since/until, replies/context/fields). Several flags are not wired in main.rs yet (potential correctness/UX gap for later bug-hunt).\n- src/config.rs/logging.rs/perf.rs/error.rs: full-featured config/logging/perf budgets/custom errors defined but currently not integrated into main.rs (latent functionality).\n- benches/search_perf.rs: synthetic benchmarks invoke SearchEngine and Storage directly; provides baseline for search/index/storage; uses TempDir and synthetic models.\n\nMulti-hop flow traces:\n1) xf index -\u003e main.rs cmd_index -\u003e ArchiveParser::parse_tweets -\u003e model::Tweet -\u003e Storage::store_tweets (SQLite inserts + FTS5) -\u003e SearchEngine::index_tweets (Tantivy doc) -\u003e commit + reload.\n2) xf search -\u003e main.rs cmd_search -\u003e SearchEngine::search -\u003e QueryParser + TopDocs -\u003e SnippetGenerator -\u003e model::SearchResult -\u003e main.rs output formatter (json/csv/text/compact).\n\nPotential pitfalls observed (for later bug review):\n- CLI flags defined but not used (sort/since/until/replies/context/fields/threads etc.).\n- Config/logging/perf modules unused; custom error types largely bypassed by anyhow usage in main.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:27:54.059223458-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:29:46.586423572-05:00","closed_at":"2026-01-10T02:29:46.586430315-05:00","dependencies":[{"issue_id":"xf-9rf.3","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:27:54.061604543-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.3","depends_on_id":"xf-9rf.2","type":"blocks","created_at":"2026-01-10T02:28:02.980609232-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.4","title":"Review prior agent changes for correctness and regressions","description":"Goal:\n- Audit code written by other agents (not limited to latest commits) to identify bugs, inefficiencies, or regressions.\n\nSteps:\n1) Use git log/diff to locate code changes authored by agents; scan both recent and older edits.\n2) For each change: verify logic vs requirements; look for edge cases, incorrect assumptions, or performance pitfalls.\n3) Cross-check with tests/benches; add targeted tests if a bug is found.\n4) Record root-cause analysis for any issues discovered.\n\nAcceptance:\n- A documented list of reviewed changes, with issues (if any) and their root causes.\n- Any fixes are minimal, scoped, and comply with AGENTS.md editing rules.\n","notes":"Reviewed uncommitted/agent-origin diffs:\n- src/main.rs: cmd_export implemented (JSON/JSONL/CSV) + csv_escape/format_export helpers; uses Storage::get_all_* and writes to file or stdout. No correctness regressions spotted; minor risk: CSV header order uses serde_json map iteration; should be deterministic for struct field order.\n- src/storage.rs: new get_all_tweets/likes/dms/followers/following; migrate now takes \u0026self; json fields parsed with serde_json::from_str (stored values are always serialized JSON, so NULL risk low). get_all_dms drops conversation_id because DirectMessage model lacks it.\n- src/parser.rs: parse_js_file uses splitn(2,'=') and trims; parse_i64 added for sizeBytes/favorite/retweet counts; doc comments expanded. Behavior aligns with JS wrapper format and should be more tolerant.\n- src/config.rs/logging.rs/error.rs/lib.rs/cli.rs: mostly must_use/const/default/allow annotations and doc tweaks; no functional changes.\n\nPotential concerns (not confirmed bugs):\n- Export of DMs lacks conversation_id context (DirectMessage model does not carry it). If CLI intended to export conversations, model/API may need extension.\n- Several CLI flags remain unimplemented in main.rs (sort/since/until/replies/context/fields). Not introduced by these diffs, but a correctness/UX gap for later bug hunt.\n\nNo immediate regressions found that warrant code changes at this step.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:30:01.076013082-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:34:02.360639254-05:00","closed_at":"2026-01-10T02:34:02.360646728-05:00","dependencies":[{"issue_id":"xf-9rf.4","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:30:01.078025102-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.4","depends_on_id":"xf-9rf.2","type":"blocks","created_at":"2026-01-10T02:30:10.794020969-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.5","title":"Fresh-eyes bug hunt + correctness fixes","description":"Goal:\n- Conduct a careful, methodical bug hunt across the codebase and fix issues with minimal, test-backed diffs.\n\nSteps:\n1) Re-read critical paths (parser/search/storage/CLI) with fresh eyes.\n2) Identify obvious errors: unchecked assumptions, incorrect error handling, silent fallbacks.\n3) For each issue: document root cause and impact; add tests before fixing when possible.\n4) Apply minimal fix using apply_patch; no unrelated refactors.\n\nAcceptance:\n- Any fixes include root-cause notes and tests when feasible.\n- No regressions; clippy/fmt/test plan recorded.\n","notes":"Fresh-eyes pass: made stats JSON optional fields skip nulls and filtered empty items in top_counts to avoid blank hashtag/mention entries. Re-ran fmt/check/clippy.","status":"in_progress","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:34:40.255520526-05:00","created_by":"ubuntu","updated_at":"2026-01-10T19:10:08.826161079-05:00","dependencies":[{"issue_id":"xf-9rf.5","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:34:40.256783946-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.5","depends_on_id":"xf-9rf.3","type":"blocks","created_at":"2026-01-10T02:34:50.320708583-05:00","created_by":"ubuntu"}]}
