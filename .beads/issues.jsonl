{"id":"xf-10","title":"Test Issue","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T19:03:06.322242-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:03:13.179271-05:00","closed_at":"2026-01-10T19:03:13.179271-05:00","close_reason":"Test issue - deleting"}
{"id":"xf-11","title":"xf UX \u0026 Reliability Improvements","description":"## Overview\n\nUX + reliability improvements across DM context, stats analytics, REPL, doctor, and natural‚Äëlanguage dates.\n\n## Cross‚ÄëCutting Requirements\n\n- **No regression** in search/indexing results.\n- **Stable JSON schemas** for machine use.\n- **Local‚Äëonly** operations (privacy‚Äëfirst).\n\n## Testing Expectations\n\n- Every feature includes unit + integration + E2E coverage.\n- E2E scripts must log command, output, exit code, and timing.\n- Quality gates: `cargo check`, `cargo clippy -D warnings`, `cargo fmt --check`, and feature tests.\n\n## Acceptance Criteria\n\n- [ ] All five feature areas shipped with tests + docs\n- [ ] No regressions in CLI behavior\n- [ ] E2E scripts available for each feature with detailed logging\n","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-10T19:03:43.51052-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:20:13.088099482-05:00"}
{"id":"xf-11.1","title":"DM Conversation Viewer (--context flag)","description":"## Overview\n\nImplement DM conversation context for `xf search --types dm --context`, enabling full thread display with highlighted matches.\n\n## Key Behaviors\n\n- Context mode only applies to DM results.\n- Deduplicate by conversation_id.\n- Support all output formats (text/json/csv/compact).\n- Preserve privacy (local-only).\n\n## Test Strategy\n\n- Unit tests for storage + context rendering helpers.\n- Integration tests for `cmd_search` behavior and JSON schema.\n- E2E script validating context output + exit codes with detailed logs.\n\n## Acceptance Criteria\n\n- [ ] `--context` works for DM searches without errors\n- [ ] Deduplication + highlight correct for multiple matches\n- [ ] Output formats consistent and stable\n- [ ] Comprehensive tests + E2E logging\n","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-10T19:04:10.48158-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:19:19.612386498-05:00","dependencies":[{"issue_id":"xf-11.1","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.1","title":"Add get_conversation_messages to storage.rs","description":"## Goal\n\nAdd `Storage::get_conversation_messages(conversation_id)` that returns **all** DM messages for a conversation in deterministic chronological order, preserving all stored fields.\n\n## Implementation Notes\n\n- Query `direct_messages` by `conversation_id`.\n- **Ordering**: `ORDER BY created_at ASC, id ASC` to break timestamp ties deterministically.\n- **Field preservation**: include `conversation_id` in the returned struct (add to `DirectMessage` if missing) or expose it via an explicit return type so `cmd_search --context` can group reliably.\n- **Parsing**: RFC3339 timestamps ‚Üí `DateTime\u003cUtc\u003e`; malformed rows should be skipped (consistent with existing `get_all_*` patterns).\n- **Performance**: use a prepared statement; avoid extra allocations for JSON fields.\n\n## Tests (storage.rs)\n\n### Unit\n- **Chronological order**: store messages out of order and assert returned order by timestamp then id.\n- **Empty conversation**: missing `conversation_id` returns empty vec (no error).\n- **Single message**: returns exactly one with all fields intact.\n- **Field preservation**: URLs + media JSON round‚Äëtrip; `conversation_id` present and correct.\n- **Malformed JSON**: ensure fallback to empty vec for urls/media without panicking.\n\n### Integration\n- `cmd_search --types dm --context` uses this method and outputs a full conversation.\n\n### E2E Coverage\n- Covered by `tests/e2e/dm_context_test.sh` (validate grouped conversation output).\n\n## Logging\n\n- `debug!` on query start/end with conversation_id + count.\n\n## Acceptance Criteria\n\n- [ ] Method added with deterministic ordering\n- [ ] Returns empty vec for missing conversation_id\n- [ ] Preserves conversation_id + all message fields\n- [ ] Unit tests cover ordering, empty, single, field preservation\n- [ ] Integration/E2E coverage via DM context tests\n","notes":"Started implementation of --context DM viewer: added get_conversation_messages to storage.rs and context output pipeline in cmd_search; added conversation context structs, grouping by conversation_id, and text/JSON rendering; added unit test for get_conversation_messages; fixed stats JSON optional fields and filtered empty top_counts entries.","status":"in_progress","priority":1,"issue_type":"task","assignee":"SageDune","created_at":"2026-01-10T19:04:32.991463-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:26:39.30131756-05:00","dependencies":[{"issue_id":"xf-11.1.1","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.2","title":"Implement DM context display in cmd_search","description":"## Goal\n\nImplement DM conversation context output for `xf search --types dm --context` across **all** output formats, with deduplication and match highlighting.\n\n## Behavior\n\n- **Context applies only to DM results**. If `--context` is used without `--types dm`, print a single warning to stderr and fall back to normal output (no failure).\n- Group results by `conversation_id` (from metadata or returned field) and show **each conversation once**.\n- Within a conversation, mark all matched message IDs (not just first).\n- Handle long conversations with a **context window** option if needed (default: full conversation for now; allow future `--context-window N`).\n\n## Text Output\n\n- Header per conversation with date range and participant IDs.\n- Each message line shows timestamp, sender, text.\n- Matched messages are highlighted and prefixed (e.g., `‚ñ∫`).\n\n## JSON Output (stable)\n\n```json\n{\n  \"conversations\": [\n    {\n      \"conversation_id\": \"...\",\n      \"participants\": [\"...\"],\n      \"messages\": [\n        {\"id\":\"...\",\"created_at\":\"...\",\"sender_id\":\"...\",\"text\":\"...\",\"matched\":true}\n      ]\n    }\n  ]\n}\n```\n\n## CSV/Compact Output\n\n- CSV: emit one row per message with `conversation_id` + `matched` flag.\n- Compact: one line per message, include `conversation_id` prefix and `*` for matched.\n\n## Edge Cases\n\n- Missing `conversation_id` ‚Üí skip with warning.\n- Empty conversations ‚Üí skip.\n- Multiple matches in same conversation ‚Üí shown once, all matched flagged.\n\n## Tests\n\n### Unit\n- Grouping + dedup by conversation_id.\n- Highlighting: all matched IDs flagged.\n- Missing conversation_id handled gracefully.\n- Output helpers for text/compact/CSV/JSON.\n\n### Integration\n- `--context` with DM types uses conversation output.\n- `--context` without DM types warns and uses default output.\n- JSON schema matches expected.\n\n### E2E Script (tests/e2e/dm_context_test.sh)\n- Validates: header present, matched indicators, JSON schema, CSV/compact rows.\n- Logs command, output, and duration per test.\n\n## Logging Requirements\n\n- `info!` when switching into context mode.\n- `debug!` per conversation (id, message_count, matched_count).\n- `warn!` for missing conversation_id.\n\n## Acceptance Criteria\n\n- [ ] `--context` no longer errors\n- [ ] Conversations deduped; all matches highlighted\n- [ ] Text/JSON/CSV/Compact outputs supported\n- [ ] Warnings for misused `--context` and missing IDs\n- [ ] Unit + integration + E2E tests with detailed logs\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T19:05:01.194389-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:17:26.812018961-05:00","dependencies":[{"issue_id":"xf-11.1.2","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.2","depends_on_id":"xf-11.1.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.3","title":"Add unit tests for DM context functionality","description":"## Goal\n\nComprehensive test coverage for DM context, including storage, formatting, and CLI integration across all output formats.\n\n## Unit Tests\n\n### Storage\n- Chronological order (timestamp + id tiebreaker).\n- Empty conversation returns empty vec.\n- Single message and full field preservation.\n\n### Formatting Helpers\n- Text formatting marks matched messages and shows conversation header.\n- JSON formatter includes `matched` flag and stable schema.\n- CSV/compact emit per-message rows with conversation_id and matched flag.\n\n## Integration Tests\n\n- `xf search --types dm --context` returns conversation output.\n- `--context` without DM types emits a warning and falls back to default output.\n- JSON output contains `conversations` array with `matched` flags.\n- CSV/compact outputs have expected columns/prefixes.\n\n## E2E Script (tests/e2e/dm_context_test.sh)\n\n- Validates:\n  - header presence\n  - matched markers\n  - JSON schema via `jq`\n  - CSV/compact output shape\n- Logs: timestamp, command, stdout/stderr, exit code, and duration.\n\n## Logging Requirements\n\n- Unit tests log conversation_id and message_count.\n- E2E logs include timing and failure context.\n\n## Acceptance Criteria\n\n- [ ] Unit + integration + E2E coverage across all formats\n- [ ] Deterministic assertions (no reliance on wall-clock)\n- [ ] Logs are detailed and actionable on failure\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T19:06:59.983466-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:19:36.450793309-05:00","dependencies":[{"issue_id":"xf-11.1.3","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.3","depends_on_id":"xf-11.1.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.3","depends_on_id":"xf-11.1.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.1.4","title":"Update CLI help text and README for --context flag","description":"## Goal\n\nDocument `--context` for DM searches in CLI help + README with clear examples and JSON schema.\n\n## CLI Help (cli.rs)\n\n- Expand `--context` help to specify:\n  - Works only with `--types dm`\n  - Shows full conversation with matched highlights\n  - Example usage\n\n## README Updates\n\n- Feature bullet for DM context.\n- Usage example showing text output with highlight marker.\n- JSON schema snippet for `--format json`.\n\n## Tests\n\n### Integration\n- `xf search --help` includes `--context` description and example string.\n- `xf search --types dm --context` output matches documented markers.\n\n### E2E\n- Extend `tests/e2e/dm_context_test.sh` to verify the README example output still matches (sanity check).\n\n## Acceptance Criteria\n\n- [ ] `xf search --help` documents `--context` clearly\n- [ ] README includes example + JSON schema\n- [ ] Integration/E2E tests verify docs stay in sync\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:07:06.472161-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:26:54.055787475-05:00","dependencies":[{"issue_id":"xf-11.1.4","depends_on_id":"xf-11.1","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.1.4","depends_on_id":"xf-11.1.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2","title":"Enhanced Stats Dashboard (--detailed flag)","description":"## Overview\n\nUpgrade `xf stats` into a detailed analytics dashboard with temporal, engagement, and content insights.\n\n## Output \u0026 UX\n\n- Text mode: sectioned analytics, sparklines, colored emphasis.\n- JSON: stable schema for automation.\n- Performance target: \u003c2s on 100k tweets.\n\n## Required Analytics\n\n- Temporal: activity trends, day/hour distributions, longest gaps.\n- Engagement: likes/retweets totals, histograms, top tweets.\n- Content: media ratio, threads, top hashtags/mentions, avg length.\n\n## Test Strategy\n\n### Unit\n- SQL aggregation correctness for each stat.\n- Edge cases: empty archive, single tweet, missing metrics.\n\n### Integration\n- `xf stats --detailed` text output contains all sections.\n- JSON output includes `temporal`, `engagement`, `content` keys.\n\n### E2E\n- `tests/e2e/stats_detailed_test.sh` validates output and performance with logs.\n\n## Acceptance Criteria\n\n- [ ] All analytics computed correctly and quickly\n- [ ] Text + JSON outputs stable\n- [ ] Comprehensive unit/integration/E2E tests with detailed logging\n","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-01-10T19:07:22.685312-05:00","created_by":"jemanuel","updated_at":"2026-01-10T23:13:14.813224-05:00","closed_at":"2026-01-10T23:13:14.813224-05:00","close_reason":"All subtasks completed (temporal, engagement, content analytics, CLI integration, tests)","dependencies":[{"issue_id":"xf-11.2","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.1","title":"Add temporal analytics to stats command","description":"## Goal\n\nAdd temporal analytics showing when and how frequently the user posted.\n\n## Implementation\n\n### New Functions in stats_analytics.rs\n\n```rust\nuse chrono::{NaiveDate, Weekday, Duration, Timelike};\n\n/// Activity statistics over time\npub struct TemporalStats {\n    /// Tweets per day for the entire archive period\n    pub daily_counts: Vec\u003c(NaiveDate, u64)\u003e,\n    /// Tweets per hour of day (0-23), aggregated\n    pub hourly_distribution: [u64; 24],\n    /// Tweets per day of week (Mon=0, Sun=6)\n    pub dow_distribution: [u64; 7],\n    /// Longest period with no tweets\n    pub longest_gap: Duration,\n    /// The gap's start and end dates\n    pub longest_gap_range: (NaiveDate, NaiveDate),\n    /// Day with most tweets\n    pub most_active_day: (NaiveDate, u64),\n    /// Hour with most tweets overall\n    pub most_active_hour: (u8, u64),\n    /// Average tweets per day (excluding zero days)\n    pub avg_tweets_per_active_day: f64,\n}\n\npub fn compute_temporal_stats(storage: \u0026Storage) -\u003e Result\u003cTemporalStats\u003e {\n    // Use SQL for efficiency:\n    // SELECT DATE(created_at) as day, COUNT(*) as count\n    // FROM tweets GROUP BY day ORDER BY day\n    \n    // For hourly: \n    // SELECT CAST(strftime('%H', created_at) AS INTEGER) as hour, COUNT(*)\n    // FROM tweets GROUP BY hour\n    \n    // For DOW:\n    // SELECT CAST(strftime('%w', created_at) AS INTEGER) as dow, COUNT(*)\n    // FROM tweets GROUP BY dow\n}\n```\n\n### Sparkline Generation\n\n```rust\n/// Generate ASCII sparkline from values\n/// Uses: ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\npub fn sparkline(values: \u0026[u64], width: usize) -\u003e String {\n    let blocks = ['‚ñÅ', '‚ñÇ', '‚ñÉ', '‚ñÑ', '‚ñÖ', '‚ñÜ', '‚ñá', '‚ñà'];\n    let max = *values.iter().max().unwrap_or(\u00261);\n    \n    // Bucket values into 'width' buckets\n    let bucket_size = (values.len() + width - 1) / width;\n    let buckets: Vec\u003cu64\u003e = values\n        .chunks(bucket_size)\n        .map(|chunk| chunk.iter().sum::\u003cu64\u003e() / chunk.len() as u64)\n        .collect();\n    \n    buckets.iter()\n        .map(|\u0026v| {\n            let idx = ((v as f64 / max as f64) * 7.0) as usize;\n            blocks[idx.min(7)]\n        })\n        .collect()\n}\n```\n\n### Gap Detection\n\n```rust\nfn find_longest_gap(daily_counts: \u0026[(NaiveDate, u64)]) -\u003e (Duration, NaiveDate, NaiveDate) {\n    let mut max_gap = Duration::zero();\n    let mut gap_start = daily_counts[0].0;\n    let mut gap_end = daily_counts[0].0;\n    \n    for window in daily_counts.windows(2) {\n        let gap = window[1].0 - window[0].0;\n        if gap \u003e max_gap {\n            max_gap = gap;\n            gap_start = window[0].0;\n            gap_end = window[1].0;\n        }\n    }\n    (max_gap, gap_start, gap_end)\n}\n```\n\n## SQL Queries Required\n\n```sql\n-- Daily counts\nSELECT DATE(created_at) as day, COUNT(*) as count\nFROM tweets \nWHERE created_at IS NOT NULL\nGROUP BY day \nORDER BY day;\n\n-- Hourly distribution\nSELECT CAST(strftime('%H', created_at) AS INTEGER) as hour, COUNT(*) as count\nFROM tweets\nWHERE created_at IS NOT NULL\nGROUP BY hour\nORDER BY hour;\n\n-- Day of week distribution  \nSELECT CAST(strftime('%w', created_at) AS INTEGER) as dow, COUNT(*) as count\nFROM tweets\nWHERE created_at IS NOT NULL\nGROUP BY dow\nORDER BY dow;\n```\n\n## Logging\n\n- Log query execution times with tracing::debug!\n- Log total tweets processed\n- Log when sparkline is generated\n\n## Acceptance Criteria\n\n- [ ] TemporalStats struct defined in stats_analytics.rs\n- [ ] compute_temporal_stats function implemented\n- [ ] Sparkline generation works for various data ranges\n- [ ] Gap detection handles edge cases (single tweet, no tweets)\n- [ ] All SQL queries execute in \u003c 500ms on 100k tweets\n- [ ] Unit tests for sparkline and gap detection","status":"closed","priority":1,"issue_type":"task","assignee":"StormyLantern","created_at":"2026-01-10T19:08:11.877953-05:00","created_by":"jemanuel","updated_at":"2026-01-10T19:54:12.699305-05:00","closed_at":"2026-01-10T19:54:12.699305-05:00","close_reason":"Implemented temporal analytics with SQL aggregations, sparklines, gap detection, and CLI integration. All tests pass.","dependencies":[{"issue_id":"xf-11.2.1","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.2","title":"Add engagement analytics to stats command","description":"## Goal\n\nAdd engagement analytics showing how the user's tweets performed.\n\n## Implementation\n\n### New Structs in stats_analytics.rs\n\n```rust\n/// Engagement metrics for the archive\npub struct EngagementStats {\n    /// Histogram of likes: (min_likes, max_likes, count)\n    pub likes_histogram: Vec\u003cLikesBucket\u003e,\n    /// Top N tweets by total engagement (likes + retweets)\n    pub top_tweets: Vec\u003cTopTweet\u003e,\n    /// Average engagement per tweet\n    pub avg_engagement: f64,\n    /// Median engagement\n    pub median_engagement: u64,\n    /// Total likes received across all tweets\n    pub total_likes: u64,\n    /// Total retweets received\n    pub total_retweets: u64,\n    /// Engagement trend over time (monthly averages)\n    pub monthly_trend: Vec\u003c(String, f64)\u003e,  // (YYYY-MM, avg_engagement)\n}\n\n#[derive(Debug)]\npub struct LikesBucket {\n    pub range: (u64, u64),  // (min, max) inclusive\n    pub count: u64,\n}\n\n#[derive(Debug)]\npub struct TopTweet {\n    pub id: String,\n    pub text_preview: String,  // First 50 chars\n    pub created_at: DateTime\u003cUtc\u003e,\n    pub likes: u64,\n    pub retweets: u64,\n    pub total_engagement: u64,\n}\n```\n\n### Histogram Bucketing\n\n```rust\nfn compute_likes_histogram(storage: \u0026Storage) -\u003e Result\u003cVec\u003cLikesBucket\u003e\u003e {\n    // Buckets: 0, 1-5, 6-10, 11-25, 26-50, 51-100, 101-500, 500+\n    let bucket_ranges = [\n        (0, 0), (1, 5), (6, 10), (11, 25), \n        (26, 50), (51, 100), (101, 500), (501, u64::MAX)\n    ];\n    \n    // SQL: SELECT favorite_count, COUNT(*) FROM tweets GROUP BY \n    //      CASE WHEN favorite_count = 0 THEN 0\n    //           WHEN favorite_count \u003c= 5 THEN 1 ... END\n}\n```\n\n### Top Tweets Query\n\n```sql\nSELECT id, full_text, created_at, favorite_count, retweet_count,\n       (favorite_count + retweet_count) as total_engagement\nFROM tweets\nWHERE favorite_count IS NOT NULL\nORDER BY total_engagement DESC\nLIMIT 10;\n```\n\n### Engagement Trend\n\n```sql\nSELECT strftime('%Y-%m', created_at) as month,\n       AVG(favorite_count + retweet_count) as avg_engagement\nFROM tweets\nWHERE created_at IS NOT NULL\nGROUP BY month\nORDER BY month;\n```\n\n## Display Format\n\n```\nüìà ENGAGEMENT ANALYTICS\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTotal Likes: 15,432 | Total Retweets: 1,234\nAverage per Tweet: 12.3 | Median: 3\n\nLikes Distribution:\n  0     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 45%\n  1-5   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 28%\n  6-10  ‚ñà‚ñà‚ñà‚ñà 9%\n  11-25 ‚ñà‚ñà‚ñà 7%\n  26-50 ‚ñà‚ñà 5%\n  50+   ‚ñà 6%\n\nTrend (12mo): ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\nTop Performing:\n1. [523 ‚ù§Ô∏è 45 üîÅ] \"My hot take on...\" (May 12)\n2. [412 ‚ù§Ô∏è 23 üîÅ] \"Thread about...\" (Mar 1)\n```\n\n## Acceptance Criteria\n\n- [ ] EngagementStats struct implemented\n- [ ] Histogram buckets are intuitive and meaningful\n- [ ] Top tweets include preview text (truncated)\n- [ ] Trend sparkline shows monthly patterns\n- [ ] Handles tweets with NULL engagement values\n- [ ] Performance \u003c 500ms on 100k tweets","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:08:12.244568-05:00","created_by":"jemanuel","updated_at":"2026-01-10T20:35:58.174246-05:00","closed_at":"2026-01-10T20:35:58.174246-05:00","close_reason":"Implemented engagement analytics with histogram, top tweets, trends sparkline, and CLI integration. All tests pass.","dependencies":[{"issue_id":"xf-11.2.2","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.3","title":"Add content analysis to stats command","description":"## Goal\n\nAdd content analysis showing what types of content the user posts and who they interact with.\n\n## Implementation\n\n### New Structs in stats_analytics.rs\n\n```rust\n/// Content breakdown and interaction patterns\npub struct ContentStats {\n    /// Percentage of tweets with media attachments\n    pub media_ratio: f64,\n    /// Number of tweets that are part of threads\n    pub thread_count: u64,\n    /// Number of standalone tweets\n    pub standalone_count: u64,\n    /// Top hashtags with counts\n    pub top_hashtags: Vec\u003c(String, u64)\u003e,\n    /// Top mentioned users with counts\n    pub top_mentions: Vec\u003c(String, u64)\u003e,\n    /// Average tweet length in characters\n    pub avg_tweet_length: f64,\n    /// Distribution of tweet lengths (buckets)\n    pub length_distribution: Vec\u003c(String, u64)\u003e,  // (\"0-50\", count)\n    /// Tweets with links vs without\n    pub link_ratio: f64,\n    /// Reply ratio (tweets that are replies vs original)\n    pub reply_ratio: f64,\n}\n```\n\n### Hashtag/Mention Extraction\n\nOption A: Parse from stored JSON (if available)\n```rust\n// If entities JSON is stored in metadata column\nfn extract_hashtags_from_metadata(storage: \u0026Storage) -\u003e Result\u003cVec\u003c(String, u64)\u003e\u003e {\n    let rows = storage.conn.prepare(\"SELECT metadata FROM tweets\")?;\n    let mut counts: HashMap\u003cString, u64\u003e = HashMap::new();\n    for row in rows {\n        if let Ok(meta) = serde_json::from_str::\u003cValue\u003e(\u0026row.get::\u003c_, String\u003e(0)?) {\n            if let Some(hashtags) = meta[\"entities\"][\"hashtags\"].as_array() {\n                for ht in hashtags {\n                    if let Some(text) = ht[\"text\"].as_str() {\n                        *counts.entry(text.to_lowercase()).or_default() += 1;\n                    }\n                }\n            }\n        }\n    }\n    // Sort by count, take top 20\n    let mut sorted: Vec\u003c_\u003e = counts.into_iter().collect();\n    sorted.sort_by(|a, b| b.1.cmp(\u0026a.1));\n    Ok(sorted.into_iter().take(20).collect())\n}\n```\n\nOption B: Parse from tweet text using regex\n```rust\nuse regex::Regex;\n\nfn extract_hashtags_from_text(storage: \u0026Storage) -\u003e Result\u003cVec\u003c(String, u64)\u003e\u003e {\n    let hashtag_re = Regex::new(r\"#(\\w+)\")?;\n    let mut counts: HashMap\u003cString, u64\u003e = HashMap::new();\n    \n    let mut stmt = storage.conn.prepare(\"SELECT full_text FROM tweets\")?;\n    for row in stmt.query_map([], |r| r.get::\u003c_, String\u003e(0))? {\n        if let Ok(text) = row {\n            for cap in hashtag_re.captures_iter(\u0026text) {\n                let tag = cap[1].to_lowercase();\n                *counts.entry(tag).or_default() += 1;\n            }\n        }\n    }\n    // Sort and return top 20\n}\n```\n\n### Thread Detection\n\n```sql\n-- Threads are identified by in_reply_to_user_id matching own user_id\n-- and in_reply_to_status_id being non-null\nSELECT COUNT(*) FROM tweets\nWHERE in_reply_to_user_id = (SELECT user_id FROM account_info LIMIT 1)\n  AND in_reply_to_status_id IS NOT NULL;\n```\n\n### Media Detection\n\n```sql\n-- Count tweets with media\nSELECT COUNT(*) FROM tweets\nWHERE media_urls_json IS NOT NULL \n  AND media_urls_json != '[]';\n```\n\n## Display Format\n\n```\nüìù CONTENT ANALYSIS\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nContent Type:\n  Text Only:   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 65%\n  With Media:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 35%\n  With Links:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 22%\n\nTweet Type:\n  Original:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 72%\n  Replies:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 25%\n  Threads:     ‚ñà 3% (142 threads)\n\nAvg Length: 187 chars\nLength Distribution:\n  0-50:   ‚ñà‚ñà 8%\n  51-140: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 45%\n  141-280: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 42%\n  280+:   ‚ñà 5%\n\nTop Hashtags:\n  #rust (156)  #programming (89)  #tech (45)\n  #ai (34)     #opensource (28)   #webdev (22)\n\nTop Mentions:\n  @friend (156)  @colleague (89)  @brand (45)\n```\n\n## Acceptance Criteria\n\n- [ ] ContentStats struct implemented\n- [ ] Hashtag extraction works (choose Option A or B based on data)\n- [ ] Mention extraction works\n- [ ] Thread detection is accurate\n- [ ] Media/link ratios computed correctly\n- [ ] Top lists limited to 20 items\n- [ ] Performance \u003c 1s on 100k tweets","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:08:12.592229-05:00","created_by":"jemanuel","updated_at":"2026-01-10T20:43:51.924873-05:00","closed_at":"2026-01-10T20:43:51.924873-05:00","close_reason":"Closed","dependencies":[{"issue_id":"xf-11.2.3","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.4","title":"Add --detailed flag and output formatting","description":"## Goal\n\nAdd --detailed flag to CLI and wire up all analytics with proper output formatting.\n\n## CLI Changes (cli.rs)\n\n### Add flag to StatsArgs\n\n```rust\n/// Show detailed analytics including temporal patterns, engagement metrics,\n/// and content analysis. Provides comprehensive archive insights.\n#[arg(long)]\npub detailed: bool,\n```\n\n### Already existing (remove 'not implemented' error)\n\n```rust\n/// Show top hashtags with counts\n#[arg(long)]\npub hashtags: bool,\n\n/// Show top mentions with counts\n#[arg(long)]\npub mentions: bool,\n```\n\n## Main.rs Integration (cmd_stats)\n\n```rust\nfn cmd_stats(cli: \u0026Cli, args: \u0026StatsArgs) -\u003e Result\u003c()\u003e {\n    let storage = Storage::open(\u0026cli.db)?;\n    \n    // Basic stats (always computed)\n    let basic = compute_basic_stats(\u0026storage)?;\n    \n    if args.detailed {\n        // Show progress for large archives\n        if basic.total_tweets \u003e 10_000 {\n            eprintln\\!(\"Computing detailed analytics...\");\n        }\n        \n        let temporal = compute_temporal_stats(\u0026storage)?;\n        let engagement = compute_engagement_stats(\u0026storage)?;\n        let content = compute_content_stats(\u0026storage)?;\n        \n        if cli.format.is_json() {\n            print_detailed_stats_json(\u0026basic, \u0026temporal, \u0026engagement, \u0026content)?;\n        } else {\n            print_detailed_stats_text(\u0026basic, \u0026temporal, \u0026engagement, \u0026content)?;\n        }\n    } else if args.hashtags {\n        let content = compute_content_stats(\u0026storage)?;\n        print_hashtags(\u0026content.top_hashtags, \u0026cli.format)?;\n    } else if args.mentions {\n        let content = compute_content_stats(\u0026storage)?;\n        print_mentions(\u0026content.top_mentions, \u0026cli.format)?;\n    } else {\n        // Basic stats only\n        print_basic_stats(\u0026basic, \u0026cli.format)?;\n    }\n    \n    Ok(())\n}\n```\n\n## Output Formatting (stats_output.rs)\n\n### Text Format\n\n```rust\nfn print_detailed_stats_text(\n    basic: \u0026BasicStats,\n    temporal: \u0026TemporalStats,\n    engagement: \u0026EngagementStats,\n    content: \u0026ContentStats,\n) -\u003e Result\u003c()\u003e {\n    use colored::*;\n    \n    println\\!(\"{}\", \"‚ïê\".repeat(65).bright_blue());\n    println\\!(\"{}               ARCHIVE ANALYTICS\", \" \".repeat(16).on_bright_blue());\n    println\\!(\"{}\", \"‚ïê\".repeat(65).bright_blue());\n    println\\!();\n    \n    // Section: Basic Overview\n    println\\!(\"{}\", \"üìä OVERVIEW\".cyan().bold());\n    println\\!(\"{}\", \"‚îÄ\".repeat(65).dimmed());\n    // ... format basic stats\n    \n    // Section: Temporal\n    println\\!();\n    println\\!(\"{}\", \"üìÖ TEMPORAL PATTERNS\".cyan().bold());\n    println\\!(\"{}\", \"‚îÄ\".repeat(65).dimmed());\n    println\\!(\"Activity Trend: {}\", sparkline(\u0026temporal.daily_counts, 30));\n    // ... format temporal stats\n    \n    // Section: Engagement\n    println\\!();\n    println\\!(\"{}\", \"üìà ENGAGEMENT\".cyan().bold());\n    // ... format engagement stats\n    \n    // Section: Content\n    println\\!();\n    println\\!(\"{}\", \"üìù CONTENT\".cyan().bold());\n    // ... format content stats\n}\n```\n\n### JSON Format\n\n```rust\nfn print_detailed_stats_json(...) -\u003e Result\u003c()\u003e {\n    let output = json\\!({\n        \"basic\": {\n            \"total_tweets\": basic.total_tweets,\n            \"total_likes\": basic.total_likes,\n            // ...\n        },\n        \"temporal\": {\n            \"daily_counts\": temporal.daily_counts,\n            \"hourly_distribution\": temporal.hourly_distribution,\n            \"most_active_day\": temporal.most_active_day,\n            // ...\n        },\n        \"engagement\": {\n            \"total_likes\": engagement.total_likes,\n            \"top_tweets\": engagement.top_tweets,\n            // ...\n        },\n        \"content\": {\n            \"media_ratio\": content.media_ratio,\n            \"top_hashtags\": content.top_hashtags,\n            // ...\n        }\n    });\n    println\\!(\"{}\", serde_json::to_string_pretty(\u0026output)?);\n    Ok(())\n}\n```\n\n## Acceptance Criteria\n\n- [ ] --detailed flag added to CLI\n- [ ] --hashtags and --mentions work (no longer throw error)\n- [ ] Text output is colorful and well-formatted\n- [ ] JSON output includes all computed metrics\n- [ ] Progress indicator for large archives\n- [ ] Help text clearly explains --detailed\n- [ ] Output fits in 80-column terminal","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T19:08:12.922482-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:05:10.662004-05:00","closed_at":"2026-01-10T21:05:10.662004-05:00","close_reason":"Closed","dependencies":[{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.4","depends_on_id":"xf-11.2.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.5","title":"Add tests for enhanced stats analytics","description":"## Goal\n\nComprehensive test coverage for the enhanced stats analytics feature.\n\n## Unit Tests (stats_analytics.rs)\n\n### Temporal Analytics Tests\n\n```rust\n#[test]\nfn test_sparkline_generation() {\n    let values = vec![1, 5, 10, 8, 3, 1];\n    let spark = sparkline(\u0026values, 6);\n    assert_eq!(spark.chars().count(), 6);\n    // Highest value (10) should be ‚ñà\n    assert!(spark.contains('‚ñà'));\n}\n\n#[test]\nfn test_sparkline_empty_input() {\n    let values: Vec\u003cu64\u003e = vec![];\n    let spark = sparkline(\u0026values, 10);\n    assert_eq!(spark, \"\");\n}\n\n#[test]\nfn test_sparkline_single_value() {\n    let values = vec![5];\n    let spark = sparkline(\u0026values, 1);\n    assert_eq!(spark, \"‚ñà\");  // Single value is max\n}\n\n#[test]\nfn test_gap_detection_normal() {\n    let counts = vec![\n        (NaiveDate::from_ymd(2023, 1, 1), 5),\n        (NaiveDate::from_ymd(2023, 1, 5), 3),  // 4 day gap\n        (NaiveDate::from_ymd(2023, 1, 20), 2), // 15 day gap (longest)\n        (NaiveDate::from_ymd(2023, 1, 22), 1), // 2 day gap\n    ];\n    let (gap, start, end) = find_longest_gap(\u0026counts);\n    assert_eq!(gap.num_days(), 15);\n    assert_eq!(start, NaiveDate::from_ymd(2023, 1, 5));\n}\n\n#[test]\nfn test_gap_detection_single_day() {\n    let counts = vec![(NaiveDate::from_ymd(2023, 1, 1), 5)];\n    let (gap, _, _) = find_longest_gap(\u0026counts);\n    assert_eq!(gap.num_days(), 0);\n}\n\n#[test]\nfn test_hourly_distribution() {\n    let storage = create_test_storage_with_tweets(vec![\n        (\"tweet1\", \"2023-01-01T09:00:00Z\"),\n        (\"tweet2\", \"2023-01-01T09:30:00Z\"),\n        (\"tweet3\", \"2023-01-01T21:00:00Z\"),\n    ]);\n    let stats = compute_temporal_stats(\u0026storage).unwrap();\n    assert_eq!(stats.hourly_distribution[9], 2);  // 9 AM\n    assert_eq!(stats.hourly_distribution[21], 1); // 9 PM\n}\n```\n\n### Engagement Analytics Tests\n\n```rust\n#[test]\nfn test_likes_histogram_buckets() {\n    let storage = create_test_storage_with_engagement(vec![\n        (0, 0), (1, 0), (3, 0), (5, 0),  // 4 in 0-5 bucket\n        (10, 0), (15, 0),                // 2 in 6-25 bucket\n        (100, 0),                        // 1 in 51-100 bucket\n    ]);\n    let stats = compute_engagement_stats(\u0026storage).unwrap();\n    // Verify bucket counts\n}\n\n#[test]\nfn test_top_tweets_ordering() {\n    let storage = create_test_storage_with_engagement(vec![\n        (10, 5),   // total 15\n        (100, 20), // total 120 (should be first)\n        (50, 10),  // total 60\n    ]);\n    let stats = compute_engagement_stats(\u0026storage).unwrap();\n    assert_eq!(stats.top_tweets[0].total_engagement, 120);\n}\n\n#[test]\nfn test_engagement_with_nulls() {\n    // Tweets with NULL favorite_count should be handled gracefully\n    let storage = create_test_storage_with_null_engagement();\n    let stats = compute_engagement_stats(\u0026storage).unwrap();\n    // Should not panic, should skip nulls\n}\n```\n\n### Content Analytics Tests\n\n```rust\n#[test]\nfn test_hashtag_extraction() {\n    let storage = create_test_storage_with_tweets(vec![\n        \"Hello #rust #programming\",\n        \"More #rust content\",\n        \"#Tech news\",\n    ]);\n    let stats = compute_content_stats(\u0026storage).unwrap();\n    assert_eq!(stats.top_hashtags[0], (\"rust\".to_string(), 2));\n}\n\n#[test]\nfn test_media_ratio() {\n    let storage = create_test_storage_mixed_media(3, 7); // 3 with media, 7 without\n    let stats = compute_content_stats(\u0026storage).unwrap();\n    assert!((stats.media_ratio - 0.3).abs() \u003c 0.01);\n}\n\n#[test]\nfn test_thread_detection() {\n    // Create tweets where some reply to self\n    let storage = create_test_storage_with_threads();\n    let stats = compute_content_stats(\u0026storage).unwrap();\n    assert_eq!(stats.thread_count, expected_thread_count);\n}\n```\n\n## Integration Tests\n\n### Test: detailed_flag_produces_all_sections\n\n```rust\n#[test]\nfn test_detailed_flag_text_output() {\n    let output = run_xf(\u0026[\"stats\", \"--detailed\"]);\n    assert!(output.contains(\"TEMPORAL PATTERNS\"));\n    assert!(output.contains(\"ENGAGEMENT\"));\n    assert!(output.contains(\"CONTENT\"));\n}\n```\n\n### Test: detailed_json_schema\n\n```rust\n#[test]\nfn test_detailed_flag_json_output() {\n    let output = run_xf(\u0026[\"stats\", \"--detailed\", \"--format\", \"json\"]);\n    let json: Value = serde_json::from_str(\u0026output).unwrap();\n    assert!(json[\"temporal\"].is_object());\n    assert!(json[\"engagement\"].is_object());\n    assert!(json[\"content\"].is_object());\n}\n```\n\n## Edge Case Tests\n\n```rust\n#[test]\nfn test_empty_archive() {\n    let storage = create_empty_storage();\n    let stats = compute_temporal_stats(\u0026storage).unwrap();\n    assert!(stats.daily_counts.is_empty());\n}\n\n#[test]\nfn test_single_tweet_archive() {\n    // Edge case: archive with exactly one tweet\n}\n\n#[test]\nfn test_massive_archive_performance() {\n    // Generate 100k fake tweets, verify \u003c 2s execution\n    let storage = create_large_test_storage(100_000);\n    let start = Instant::now();\n    let _ = compute_temporal_stats(\u0026storage).unwrap();\n    assert!(start.elapsed() \u003c Duration::from_secs(2));\n}\n```\n\n## Logging Requirements\n\n- Each test should log setup and teardown with tracing::debug!\n- On assertion failure, log full computed state\n- Log timing for performance-sensitive tests\n\n## Acceptance Criteria\n\n- [ ] All unit tests pass\n- [ ] Edge cases covered (empty, single item, nulls)\n- [ ] Performance test validates \u003c 2s on 100k tweets\n- [ ] Integration tests verify CLI output\n- [ ] Test coverage \u003e 80% for new code","status":"closed","priority":2,"issue_type":"task","assignee":"MagentaFox","created_at":"2026-01-10T19:08:23.225389-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:40:05.421163932-05:00","closed_at":"2026-01-10T21:40:05.421163932-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.2.5","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.5","depends_on_id":"xf-11.2.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.2.6","title":"Create E2E test script for stats --detailed","description":"## Goal\n\nCreate a **reliable E2E script** for `xf stats --detailed` that validates output sections, JSON schema, and performance with detailed logging.\n\n## Script: tests/e2e/stats_detailed_test.sh\n\n### Setup\n- Use a **known fixture** archive/DB under `tests/fixtures/` (or a small synthetic dataset built by a helper) to keep runtime deterministic.\n- Respect `XF_DB` / `XF_INDEX` overrides if set.\n\n### Checks\n- Text output contains all required sections: Temporal, Engagement, Content.\n- JSON output is valid and contains `temporal`, `engagement`, `content` objects.\n- Exit code is 0 on healthy data.\n- Performance: log duration; warn if \u003e2s on fixture (do not hard‚Äëfail unless agreed).\n\n### Logging (must be verbose)\n- Timestamped entries for each test.\n- Capture command, stdout, stderr, exit code, and duration.\n- On failure, print the failing output snippet and context.\n\n## Integration Touchpoints\n\n- Script should be runnable in CI and locally with no network access.\n- Use `set -euo pipefail` and explicit trap to report final summary.\n\n## Acceptance Criteria\n\n- [ ] E2E script added and executable\n- [ ] Validates text + JSON output for `--detailed`\n- [ ] Logs command/stdout/stderr/exit code/duration\n- [ ] Uses deterministic fixture data\n- [ ] Non‚Äëzero exit on failure with actionable logs\n","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T19:26:54.052373-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:59:17.798982126-05:00","closed_at":"2026-01-10T22:59:17.798982126-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.2.6","depends_on_id":"xf-11.2","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.2.6","depends_on_id":"xf-11.2.5","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3","title":"Interactive REPL Mode (xf shell)","description":"## Overview\n\nInteractive REPL (`xf shell`) for iterative searches, refinements, and exports with history + completion.\n\n## Key Capabilities\n\n- Fast loop for `search`, `stats`, `list`, `refine`, `show`, `export`.\n- Session state with pagination and variable references.\n- Tab completion for commands/flags/values.\n- History persistence (opt‚Äëout).\n\n## Test Strategy\n\n- Unit tests for parsing, session state, completion.\n- Integration tests for REPL flow with test DB/index.\n- E2E script exercising non‚Äëinteractive REPL sessions with detailed logging.\n\n## Acceptance Criteria\n\n- [ ] `xf shell` launches REPL and accepts commands\n- [ ] State, pagination, and variable substitution work\n- [ ] Completion works and is safe\n- [ ] History persists when enabled\n- [ ] Full test coverage with logs\n","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-10T19:08:37.178423-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:20:01.333893695-05:00","dependencies":[{"issue_id":"xf-11.3","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.1","title":"Implement REPL core with rustyline","description":"## Goal\n\nImplement the **core REPL loop** using `rustyline`, providing stable command parsing, history, and error handling that mirrors CLI behavior.\n\n## Functional Requirements\n\n- `xf shell` launches a REPL that supports: `search`, `stats`, `list`, `refine`, `more`, `show`, `export`, `help`, `quit` (+ aliases).\n- Ctrl+C resets the prompt (no exit). Ctrl+D exits cleanly.\n- History file is loaded/saved when enabled; skip history for `quit/exit` commands.\n- Prompt context shows result count or DM context when relevant.\n- No network access; all data stays local.\n\n## Implementation Notes\n\n- Use `rustyline` with Emacs editing, history dedupe, and list completion mode (actual completer added in xf-11.3.2).\n- Keep parsing logic isolated and reusable for tests.\n- Avoid panics on empty/whitespace input; provide friendly errors for unknown commands.\n\n## Tests\n\n### Unit\n- Parsing for each command + alias.\n- Prompt formatting for normal/results/DM contexts.\n- History path resolution (respect `--history-file`, `--no-history`).\n- Ctrl+C/Ctrl+D handling (simulate via helper functions).\n\n### Integration\n- Non‚Äëinteractive session (`printf \"help\\nquit\\n\" | xf shell`) exits 0.\n- History file created only when enabled.\n\n### E2E Script (tests/e2e/repl_smoke_test.sh)\n- Runs a basic REPL session and validates output + exit code.\n- Logs timestamp, command, stdout/stderr, exit code, duration.\n\n## Logging\n\n- `info!` on REPL start/exit.\n- `debug!` on command parse/execute.\n- `warn!` on unknown commands or execution errors.\n\n## Acceptance Criteria\n\n- [ ] REPL loop runs with proper Ctrl+C/Ctrl+D behavior\n- [ ] History loads/saves when enabled and is skipped when disabled\n- [ ] Commands parsed + dispatched without panics\n- [ ] Unit + integration + E2E tests with detailed logs\n","notes":"REPL implementation complete: all commands (search, list, refine, more, show, export, stats, help, quit) with aliases, 35 unit tests, pagination support. Commit c41f7dc.","status":"closed","priority":2,"issue_type":"task","assignee":"ChartreuseGate","created_at":"2026-01-10T19:08:52.475269-05:00","created_by":"jemanuel","updated_at":"2026-01-11T00:10:20.759873-05:00","closed_at":"2026-01-11T00:10:20.759873-05:00","dependencies":[{"issue_id":"xf-11.3.1","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.2","title":"Add tab completion for REPL commands","description":"## Goal\n\nAdd robust tab completion for REPL commands, flags, and context‚Äëaware values with safe handling of quotes/comma lists.\n\n## Completion Rules\n\n- **Command position**: complete commands + aliases.\n- **Flag position**: flags for active command.\n- **Value position**:\n  - `--types` ‚Üí dm/tweet/like/grok/follower/following/block/mute\n  - `--format` ‚Üí text/json/json-pretty/csv/compact\n  - `list` target ‚Üí tweets/likes/dms/conversations/followers/following/blocks/mutes/files\n- Handle `--flag value`, `--flag=value`, and comma‚Äëseparated values (`--types dm,li`).\n- If cursor is inside quotes, **do not** attempt command/flag completion (avoid corrupting user input).\n\n## Implementation Notes\n\n- Use `rustyline::completion::Completer` with `Pair { display, replacement }`.\n- Keep completions deterministic, sorted, and free of duplicates.\n- `extract_word_at_position` must handle `=` and commas without panics.\n\n## Tests\n\n### Unit\n- Command completions for partial input.\n- Flag completions per command (`search`, `list`, `stats`, `export`, `refine`).\n- Value completions for `--types`, `--format`, `list` target.\n- Comma‚Äëseparated completion (e.g., `--types dm,li` suggests `like`).\n- No completions when cursor is inside quotes.\n\n### Integration\n- REPL with completer enabled; tab on empty line does not crash.\n\n### E2E Script (tests/e2e/repl_completion_test.sh)\n- Non‚Äëinteractive session that triggers completion (e.g., via `RUSTYLINE` env or scripted input if supported).\n- Validates that a completion suggestion list appears for `se\u003cTab\u003e` and `--format \u003cTab\u003e`.\n- Logs command/stdout/stderr/exit code/duration.\n\n## Logging\n\n- `trace!` when computing completion context.\n- `debug!` for candidate counts.\n\n## Acceptance Criteria\n\n- [ ] Command/flag/value completions work\n- [ ] Handles quotes and comma lists safely\n- [ ] Deterministic, sorted suggestions\n- [ ] Unit + integration + E2E tests with detailed logs\n","status":"closed","priority":2,"issue_type":"task","assignee":"ChartreuseGate","created_at":"2026-01-10T19:08:52.888703-05:00","created_by":"jemanuel","updated_at":"2026-01-11T02:44:11.790332-05:00","closed_at":"2026-01-11T02:44:11.790332-05:00","close_reason":"Implemented XfCompleter with command, list target, export format, and help topic completion. 17 unit tests added. All tests pass.","dependencies":[{"issue_id":"xf-11.3.2","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.2","depends_on_id":"xf-11.3.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.3","title":"Add REPL session state and query refinement","description":"## Goal\n\nAdd REPL session state for cached results, query refinement, pagination, and variable substitution while keeping behavior consistent with CLI filtering semantics.\n\n## Behavior\n\n- Cache `last_results`, `last_query`, `current_offset`, and `page_size` after each search.\n- `refine` filters cached results using the **same rules** as CLI (types, since/until, replies/no-replies).\n- `more` paginates using `page_size`; prints ‚Äúno more results‚Äù when exhausted.\n- Variable refs:\n  - `$1`, `$2` etc = Nth result\n  - `$_` = last selected\n  - `$*` = all result IDs\n  - `$name` = named variables (`set $name value`)\n- Pipes (`cmd1 | cmd2`) execute sequentially; each command can mutate state.\n\n## Implementation Notes\n\n- Reuse CLI formatting/output helpers where possible.\n- For date filters in `refine`, reuse `date_parser::parse_date_flexible` for parity.\n- Avoid cloning large result sets unless needed; prefer indices or references.\n\n## Tests\n\n### Unit\n- `refine` reduces result set for date/type filters.\n- `more` advances pagination; handles end‚Äëof‚Äëresults gracefully.\n- Variable resolution for `$1`, `$_`, `$*`, `$name`.\n- Pipe execution order is preserved.\n\n### Integration\n- Simulated REPL flow: `search` ‚Üí `refine` ‚Üí `more` ‚Üí `show`.\n- `refine` yields same results as running a new CLI search with same filters.\n\n### E2E Script (tests/e2e/repl_state_test.sh)\n- Runs a scripted REPL session verifying `refine` + `more` + variable selection.\n- Logs command/stdout/stderr/exit code/duration.\n\n## Logging\n\n- `debug!` on state transitions (results count, offset).\n- `warn!` on invalid references or empty cache.\n\n## Acceptance Criteria\n\n- [ ] Cached results and pagination behave deterministically\n- [ ] `refine` matches CLI semantics\n- [ ] Variable substitution works for all forms\n- [ ] Pipes execute sequentially and mutate state correctly\n- [ ] Unit + integration + E2E tests with detailed logs\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T19:08:53.278205-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:26:23.5150827-05:00","dependencies":[{"issue_id":"xf-11.3.3","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.3","depends_on_id":"xf-11.3.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.4","title":"Add xf shell subcommand to CLI","description":"## Goal\n\nExpose `xf shell` CLI subcommand that launches the REPL with configurable prompt, pagination, and history settings.\n\n## CLI Contract\n\n- `xf shell` uses global `--db` / `--index` overrides.\n- Options:\n  - `--prompt \u003cstr\u003e` (default: `xf\u003e `)\n  - `--page-size \u003cn\u003e` (default: 10)\n  - `--no-history`\n  - `--history-file \u003cpath\u003e` (overrides default `~/.xf_history`)\n\n## Behavior\n\n- Fails with a clear error if DB/index not found.\n- History writes are disabled when `--no-history` is set.\n- Prompt reflects `--prompt` in REPL output (including multi-word prompts).\n\n## Tests\n\n### Unit\n- clap parsing for `shell` and its options.\n\n### Integration\n- `xf shell --help` contains examples and options.\n- With `--no-history`, no history file is created.\n- With `--history-file`, history is written to that path.\n\n### E2E Script (tests/e2e/shell_cli.sh)\n- Starts REPL non-interactively (`echo \"quit\" | xf shell`) and validates exit code 0.\n- Verifies prompt string appears in output.\n- Logs command, output, and duration.\n\n## Logging\n\n- `info!` when shell starts and exits.\n- `debug!` with resolved config (prompt/page_size/history_path).\n\n## Acceptance Criteria\n\n- [ ] `xf shell` launches REPL and exits cleanly\n- [ ] All shell flags work as documented\n- [ ] Help text and examples are accurate\n- [ ] Unit + integration + E2E tests with detailed logs\n","status":"closed","priority":2,"issue_type":"task","assignee":"ChartreuseGate","created_at":"2026-01-10T19:08:53.687242-05:00","created_by":"jemanuel","updated_at":"2026-01-11T00:18:09.494603-05:00","closed_at":"2026-01-11T00:18:09.494603-05:00","close_reason":"Implemented shell CLI subcommand with all required options (--prompt, --page-size, --no-history, --history-file). Added 7 integration tests and E2E script. Commit 49148f6.","dependencies":[{"issue_id":"xf-11.3.4","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.4","depends_on_id":"xf-11.3.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.3.5","title":"Add tests for REPL mode","description":"## Goal\n\nComprehensive test coverage for REPL mode including unit, integration, and E2E validation with **deterministic** history paths and verbose logs.\n\n## Unit Tests (repl.rs)\n\n- Command parsing (search/stats/list/refine/more/show/export/help/quit + aliases).\n- Flag parsing inside REPL (types, format, limit, since/until).\n- Prompt formatting for each context.\n- Variable resolution ($1, $_, $* and named variables).\n- `refine` + `more` pagination behavior.\n\n## Integration Tests\n\n- Simulate REPL flow with a test storage/index (search ‚Üí refine ‚Üí more ‚Üí show).\n- History persistence uses a **temp dir** (no writes to user home).\n- Ensure `--no-history` disables file writes.\n\n## E2E Script (tests/e2e/repl_test.sh)\n\n- Run non‚Äëinteractive sessions (`printf \"search ...\\nquit\\n\" | xf shell`).\n- Validate:\n  - REPL starts and exits cleanly.\n  - Help output contains key commands.\n  - Unknown command yields error.\n  - History file created only when enabled.\n- Logs: timestamp, command, stdout/stderr, exit code, and duration.\n\n## Logging Requirements\n\n- `trace!` for parse decisions and completion contexts.\n- `debug!` for state transitions (result count, offset).\n- `warn!` for invalid refs and command errors.\n\n## Acceptance Criteria\n\n- [ ] Unit tests cover parsing, state, variables, pagination\n- [ ] Integration tests validate session flow and history\n- [ ] E2E script validates interactive behavior\n- [ ] Deterministic temp history paths (no user‚Äëhome writes)\n- [ ] All tests pass with `cargo test`\n","status":"closed","priority":3,"issue_type":"task","assignee":"ChartreuseGate","created_at":"2026-01-10T19:09:03.5476-05:00","created_by":"jemanuel","updated_at":"2026-01-11T00:25:04.856255-05:00","closed_at":"2026-01-11T00:25:04.856255-05:00","close_reason":"Added 20 unit tests (total 52) covering ReplConfig, prompt logic, edge cases. Created repl_test.sh with 13 E2E tests. All tests pass. Commit 9b0d50a.","dependencies":[{"issue_id":"xf-11.3.5","depends_on_id":"xf-11.3","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.3.5","depends_on_id":"xf-11.3.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4","title":"xf doctor Health Check Command","description":"## Overview\n\nAdd `xf doctor` to diagnose archive, database, index, and performance issues with **actionable fixes** and **stable machine-readable output**.\n\n## User Value\n\n- Explains missing data, slow search, and corruption symptoms.\n- Provides safe, local fixes (no network access).\n- Builds trust in the archive/index integrity.\n\n## Scope\n\n### Archive Checks\n- Required files present\n- JS-wrapped JSON structure validity\n- Duplicate IDs\n- Timestamp consistency\n\n### Database Checks\n- `PRAGMA integrity_check`\n- FTS5 integrity + orphan detection\n- Table counts vs. index counts\n\n### Index Checks\n- Tantivy version compatibility\n- Segment count / doc count\n- Sample query latency\n\n### Performance Checks\n- Index load time\n- Simple/complex query latency\n- FTS5 query latency\n\n## Output Requirements\n\n- Text: sectioned, colored, emoji status, summary + suggestions.\n- JSON: stable schema (checks, summary, suggestions, runtime_ms).\n- Exit codes: 0 pass/warn, 1 error, 2 critical.\n\n## Safety\n\n- `--fix` must be **safe and idempotent** only; never delete user data.\n- All operations remain local (privacy-first).\n\n## Test Strategy\n\n### Unit\n- Check logic for each category with deterministic fixtures.\n- JSON schema serialization and summary counts.\n\n### Integration\n- CLI output for text/json formats.\n- Exit codes for pass/warn/error/critical.\n\n### E2E\n- `tests/e2e/doctor_cli.sh` against healthy and intentionally broken archives.\n- Logs command, timing, exit codes, and verifies output sections.\n\n## Acceptance Criteria\n\n- [ ] `xf doctor` runs all checks in \u003c10s on large archives\n- [ ] JSON output stable and validated\n- [ ] Safe `--fix` reduces warnings without data loss\n- [ ] Comprehensive tests + E2E with detailed logs\n","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-01-10T19:09:20.038868-05:00","created_by":"jemanuel","updated_at":"2026-01-10T23:27:52.875818-05:00","closed_at":"2026-01-10T23:27:52.875818-05:00","close_reason":"All subtasks complete: archive validation, index checks, database checks, performance benchmarks, CLI integration, and tests","dependencies":[{"issue_id":"xf-11.4","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.1","title":"Implement archive structure validation","description":"## Goal\n\nImplement archive structure validation to verify the source archive is correctly formatted and complete.\n\n## Checks to Implement\n\n### 1. Required Files Presence\n\n```rust\nfn check_required_files(archive_path: \u0026Path) -\u003e Vec\u003cHealthCheck\u003e {\n    let required = [\n        (\"data/tweets.js\", true),\n        (\"data/tweets-part*.js\", true),  // or parts\n        (\"data/direct-messages.js\", false),\n        (\"data/direct-messages-group*.js\", false),\n        (\"data/like.js\", false),\n        (\"data/follower.js\", false),\n        (\"data/following.js\", false),\n        (\"data/block.js\", false),\n        (\"data/mute.js\", false),\n    ];\n    \n    let mut checks = Vec::new();\n    for (pattern, is_required) in required {\n        let exists = glob_matches_any(archive_path, pattern);\n        checks.push(HealthCheck {\n            category: CheckCategory::Archive,\n            name: format!(\"File: {}\", pattern),\n            status: if exists { \n                CheckStatus::Pass \n            } else if is_required {\n                CheckStatus::Error\n            } else {\n                CheckStatus::Warning\n            },\n            message: if exists { \n                \"Found\".into() \n            } else { \n                \"Not found\".into() \n            },\n            suggestion: if !exists \u0026\u0026 is_required {\n                Some(\"Ensure archive was fully extracted\".into())\n            } else {\n                None\n            },\n        });\n    }\n    checks\n}\n```\n\n### 2. JSON Structure Validation\n\n```rust\nfn check_json_structure(archive_path: \u0026Path) -\u003e Vec\u003cHealthCheck\u003e {\n    let files = [\"tweets.js\", \"direct-messages.js\", \"like.js\"];\n    let mut checks = Vec::new();\n    \n    for file in files {\n        let path = archive_path.join(\"data\").join(file);\n        if !path.exists() {\n            continue;\n        }\n        \n        match validate_js_wrapped_json(\u0026path) {\n            Ok((count, warnings)) =\u003e {\n                checks.push(HealthCheck {\n                    category: CheckCategory::Archive,\n                    name: format!(\"Parse: {}\", file),\n                    status: if warnings.is_empty() { CheckStatus::Pass } else { CheckStatus::Warning },\n                    message: format!(\"{} items parsed\", count),\n                    suggestion: warnings.first().cloned(),\n                });\n            }\n            Err(e) =\u003e {\n                checks.push(HealthCheck {\n                    category: CheckCategory::Archive,\n                    name: format!(\"Parse: {}\", file),\n                    status: CheckStatus::Error,\n                    message: format!(\"Parse error: {}\", e),\n                    suggestion: Some(\"Check file is not corrupted\".into()),\n                });\n            }\n        }\n    }\n    checks\n}\n\nfn validate_js_wrapped_json(path: \u0026Path) -\u003e Result\u003c(usize, Vec\u003cString\u003e)\u003e {\n    let content = fs::read_to_string(path)?;\n    \n    // Strip JS wrapper: window.YTD.tweets.part0 = [...]\n    let json_start = content.find('[')\n        .ok_or_else(|| anyhow!(\"No JSON array found\"))?;\n    let json = \u0026content[json_start..];\n    \n    // Parse and count\n    let items: Vec\u003cValue\u003e = serde_json::from_str(json)?;\n    let mut warnings = Vec::new();\n    \n    // Check for common issues\n    for (i, item) in items.iter().enumerate() {\n        if item[\"tweet\"][\"id_str\"].is_null() {\n            warnings.push(format!(\"Item {} missing id_str\", i));\n        }\n    }\n    \n    Ok((items.len(), warnings))\n}\n```\n\n### 3. Duplicate ID Detection\n\n```rust\nfn check_duplicate_ids(archive_path: \u0026Path) -\u003e HealthCheck {\n    let mut seen_ids: HashSet\u003cString\u003e = HashSet::new();\n    let mut duplicates: Vec\u003cString\u003e = Vec::new();\n    \n    // Load all tweet IDs from archive\n    if let Ok(tweets) = parse_tweets_file(archive_path) {\n        for tweet in tweets {\n            if !seen_ids.insert(tweet.id.clone()) {\n                duplicates.push(tweet.id);\n            }\n        }\n    }\n    \n    HealthCheck {\n        category: CheckCategory::Archive,\n        name: \"Duplicate Tweet IDs\".into(),\n        status: if duplicates.is_empty() { CheckStatus::Pass } else { CheckStatus::Warning },\n        message: if duplicates.is_empty() {\n            \"No duplicates detected\".into()\n        } else {\n            format!(\"{} duplicate IDs found\", duplicates.len())\n        },\n        suggestion: if !duplicates.is_empty() {\n            Some(format!(\"Duplicate IDs: {}...\", duplicates[..3.min(duplicates.len())].join(\", \")))\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 4. Timestamp Consistency\n\n```rust\nfn check_timestamp_consistency(archive_path: \u0026Path) -\u003e HealthCheck {\n    let mut issues = Vec::new();\n    \n    if let Ok(tweets) = parse_tweets_file(archive_path) {\n        for tweet in \u0026tweets {\n            // Check for future dates\n            if tweet.created_at \u003e Utc::now() {\n                issues.push(format!(\"{}: future date\", tweet.id));\n            }\n            // Check for impossibly old dates (before Twitter existed)\n            if tweet.created_at.year() \u003c 2006 {\n                issues.push(format!(\"{}: before 2006\", tweet.id));\n            }\n        }\n    }\n    \n    HealthCheck {\n        category: CheckCategory::Archive,\n        name: \"Timestamp Validity\".into(),\n        status: if issues.is_empty() { CheckStatus::Pass } else { CheckStatus::Warning },\n        message: if issues.is_empty() {\n            \"All timestamps valid\".into()\n        } else {\n            format!(\"{} timestamp issues\", issues.len())\n        },\n        suggestion: None,\n    }\n}\n```\n\n## Logging\n\n- Log each file being checked with tracing::debug!\n- Log parse errors with tracing::warn!\n- Log timing for large files with tracing::info!\n\n## Acceptance Criteria\n\n- [ ] Required files check works\n- [ ] Optional files don't cause errors\n- [ ] JSON parsing validates structure\n- [ ] Duplicate detection scans all tweets\n- [ ] Timestamp validation catches anomalies\n- [ ] All checks return HealthCheck structs\n- [ ] Performance \u003c 5s for 100k tweet archive","status":"closed","priority":1,"issue_type":"task","assignee":"MagentaFox","created_at":"2026-01-10T19:09:35.993651-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:19:26.23636315-05:00","closed_at":"2026-01-10T21:19:26.23636315-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.4.1","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.2","title":"Implement Tantivy index health check","description":"## Goal\n\nImplement Tantivy index health checks to verify the search index is valid and performant.\n\n## Checks to Implement\n\n### 1. Index Directory Existence\n\n```rust\nfn check_index_directory(index_path: \u0026Path) -\u003e HealthCheck {\n    if !index_path.exists() {\n        return HealthCheck {\n            category: CheckCategory::Index,\n            name: \"Index Directory\".into(),\n            status: CheckStatus::Error,\n            message: format!(\"Index not found at {:?}\", index_path),\n            suggestion: Some(\"Run 'xf index' to create the index\".into()),\n        };\n    }\n    \n    // Check for required Tantivy files\n    let meta_path = index_path.join(\"meta.json\");\n    if !meta_path.exists() {\n        return HealthCheck {\n            category: CheckCategory::Index,\n            name: \"Index Directory\".into(),\n            status: CheckStatus::Error,\n            message: \"Missing meta.json - index may be corrupted\".into(),\n            suggestion: Some(\"Run 'xf reindex' to rebuild\".into()),\n        };\n    }\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Index Directory\".into(),\n        status: CheckStatus::Pass,\n        message: format!(\"Found at {:?}\", index_path),\n        suggestion: None,\n    }\n}\n```\n\n### 2. Index Version Compatibility\n\n```rust\nfn check_index_version(index: \u0026TantivyIndex) -\u003e HealthCheck {\n    let meta = index.load_metas()?;\n    let index_version = meta.index_version();\n    let current_version = tantivy::version();\n    \n    let compatible = is_version_compatible(index_version, current_version);\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Index Version\".into(),\n        status: if compatible { CheckStatus::Pass } else { CheckStatus::Warning },\n        message: format!(\"Index v{} (current: v{})\", index_version, current_version),\n        suggestion: if !compatible {\n            Some(\"Consider 'xf reindex' for latest format\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 3. Segment Health\n\n```rust\nfn check_segments(index: \u0026TantivyIndex) -\u003e HealthCheck {\n    let reader = index.reader()?;\n    let searcher = reader.searcher();\n    let segment_count = searcher.segment_readers().len();\n    \n    let status = match segment_count {\n        0 =\u003e CheckStatus::Warning,\n        1..=10 =\u003e CheckStatus::Pass,\n        11..=50 =\u003e CheckStatus::Warning,\n        _ =\u003e CheckStatus::Warning,\n    };\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Segment Count\".into(),\n        status,\n        message: format!(\"{} segments\", segment_count),\n        suggestion: if segment_count \u003e 10 {\n            Some(\"Run 'xf optimize' to merge segments\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 4. Document Count Verification\n\n```rust\nfn check_document_count(index: \u0026TantivyIndex, storage: \u0026Storage) -\u003e HealthCheck {\n    let reader = index.reader()?;\n    let searcher = reader.searcher();\n    let index_count = searcher.num_docs() as i64;\n    \n    // Get expected count from database\n    let db_count: i64 = storage.conn\n        .query_row(\"SELECT COUNT(*) FROM tweets\", [], |r| r.get(0))?;\n    \n    let diff = (index_count - db_count).abs();\n    let percentage_diff = if db_count \u003e 0 {\n        (diff as f64 / db_count as f64) * 100.0\n    } else {\n        0.0\n    };\n    \n    let status = if diff == 0 {\n        CheckStatus::Pass\n    } else if percentage_diff \u003c 1.0 {\n        CheckStatus::Warning\n    } else {\n        CheckStatus::Error\n    };\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Document Count\".into(),\n        status,\n        message: format!(\"Index: {}, DB: {} (diff: {})\", index_count, db_count, diff),\n        suggestion: if diff \u003e 0 {\n            Some(\"Run 'xf reindex' to sync\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n### 5. Sample Query Test\n\n```rust\nfn check_sample_query(index: \u0026TantivyIndex) -\u003e HealthCheck {\n    let start = Instant::now();\n    \n    let result = index.search(\"test\", 1);  // Simple query, limit 1\n    \n    let duration = start.elapsed();\n    let duration_ms = duration.as_secs_f64() * 1000.0;\n    \n    match result {\n        Ok(_) =\u003e {\n            let status = if duration_ms \u003c 10.0 {\n                CheckStatus::Pass\n            } else if duration_ms \u003c 100.0 {\n                CheckStatus::Warning\n            } else {\n                CheckStatus::Warning\n            };\n            \n            HealthCheck {\n                category: CheckCategory::Index,\n                name: \"Sample Query\".into(),\n                status,\n                message: format!(\"{:.1}ms\", duration_ms),\n                suggestion: if duration_ms \u003e 10.0 {\n                    Some(\"Consider 'xf optimize' for faster queries\".into())\n                } else {\n                    None\n                },\n            }\n        }\n        Err(e) =\u003e HealthCheck {\n            category: CheckCategory::Index,\n            name: \"Sample Query\".into(),\n            status: CheckStatus::Error,\n            message: format!(\"Query failed: {}\", e),\n            suggestion: Some(\"Index may be corrupted. Try 'xf reindex'\".into()),\n        },\n    }\n}\n```\n\n### 6. Index Size Check\n\n```rust\nfn check_index_size(index_path: \u0026Path) -\u003e HealthCheck {\n    let size_bytes = calculate_directory_size(index_path)?;\n    let size_mb = size_bytes as f64 / (1024.0 * 1024.0);\n    \n    HealthCheck {\n        category: CheckCategory::Index,\n        name: \"Index Size\".into(),\n        status: CheckStatus::Pass,  // Informational\n        message: format!(\"{:.1} MB\", size_mb),\n        suggestion: if size_mb \u003e 500.0 {\n            Some(\"Large index. Consider 'xf optimize' to reduce size\".into())\n        } else {\n            None\n        },\n    }\n}\n```\n\n## Acceptance Criteria\n\n- [ ] Index directory check works\n- [ ] Version compatibility check implemented\n- [ ] Segment count reported with warnings\n- [ ] Document count compared to database\n- [ ] Sample query executes and times\n- [ ] Index size reported\n- [ ] All checks return HealthCheck structs\n- [ ] Graceful handling of corrupted indexes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:09:36.518068-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:16:18.578901895-05:00","closed_at":"2026-01-10T21:16:18.578901895-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.4.2","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.3","title":"Implement SQLite database health check","description":"Run PRAGMA integrity_check. Verify FTS5 index integrity. Check for orphaned records between tables. Report table sizes and row counts. Detect schema version mismatches.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T19:09:36.88663-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:00:15.217598373-05:00","closed_at":"2026-01-10T21:00:15.217598373-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.4.3","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.4","title":"Add performance benchmarks to doctor","description":"Measure index load time. Benchmark simple and complex queries. Report query latency percentiles. Compare against expected baselines. Flag performance regressions.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T19:09:37.267085-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:37:54.967026-05:00","closed_at":"2026-01-10T21:37:54.967026-05:00","close_reason":"Completed - added LatencyStats, benchmark functions, and run_performance_benchmarks","dependencies":[{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.4","depends_on_id":"xf-11.4.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.5","title":"Add doctor CLI subcommand with fix suggestions","description":"## Goal\n\nWire `xf doctor` into the CLI with clear output, stable JSON, actionable suggestions, and **safe** optional auto-fixes.\n\n## CLI Surface\n\n- Command: `xf doctor`\n- Flags:\n  - `--archive \u003cpath\u003e` (optional; overrides config)\n  - `--fix` (apply safe, idempotent repairs only)\n  - `--format \u003ctext|json|json-pretty\u003e` (reuse global format)\n\n## Behavior \u0026 Flow\n\n1. **Resolve archive path**\n   - Precedence: `--archive` \u003e config default (if stored) \u003e none.\n   - If none, **skip archive checks** and emit a warning + suggestion (do not fail).\n2. **Run checks**\n   - Archive checks: `check_required_files`, `check_json_structure`, `check_duplicate_ids`, `check_timestamp_consistency`.\n   - Database checks: `Storage::database_health_checks()` (integrity, FTS, orphans, table counts).\n   - Index checks: `SearchEngine::index_health_checks(\u0026Storage)`.\n   - Performance checks: `run_performance_benchmarks`.\n3. **Aggregate results**\n   - Stable ordering: category order (Archive ‚Üí Database ‚Üí Index ‚Üí Performance), then name.\n   - Summary counts: pass/warn/error/critical + total.\n4. **Output**\n   - Text: section headers, per-check emoji, summary line, then suggestions list.\n   - JSON: stable schema (see below), no color.\n5. **Exit codes**\n   - `0` = no errors/critical\n   - `1` = errors (data issues)\n   - `2` = critical/corruption detected\n\n## JSON Schema (stable)\n\n```json\n{\n  \"checks\": [\n    {\"category\":\"archive\",\"name\":\"...\",\"status\":\"pass|warning|error\",\"message\":\"...\",\"suggestion\":null}\n  ],\n  \"summary\": {\"passed\": 0, \"warnings\": 0, \"errors\": 0, \"critical\": 0, \"total\": 0},\n  \"suggestions\": [\"...\"],\n  \"runtime_ms\": 0\n}\n```\n\n## --fix Behavior (safe + idempotent only)\n\n- **Never delete** archive, DB, or index files.\n- Allowed repairs (if check indicates need):\n  - SQLite: `PRAGMA optimize`.\n  - FTS: rebuild FTS tables **only** if orphaned/invalid (new `Storage::rebuild_fts()` if needed).\n  - Tantivy: `SearchEngine::optimize()` if segment count is high (no data loss).\n- Each repair emits a `HealthCheck` entry with status `Pass/Warning` and a `message` showing the action taken.\n- Failures in fixes are reported with `status=Error` but **do not abort** remaining checks.\n\n## Tests\n\n### Unit\n- Aggregation/sorting order is deterministic.\n- Summary counts match check list.\n- `--fix` only runs for specific warning/error states.\n- JSON schema serialization (snapshot-style) and `suggestions` content.\n\n### Integration\n- `xf doctor` without archive path skips archive checks with warning.\n- `xf doctor --format json` returns schema and valid JSON.\n- Exit codes reflect simulated errors/critical.\n- `--fix` toggles repair functions (use spies/mocks or test DBs).\n\n### E2E Script (tests/e2e/doctor_cli.sh)\n- Logs command, duration, exit code, and validates section headings.\n- Validates JSON output with `jq` and required keys.\n- Runs `--fix` on a known-bad DB and asserts reduced warnings.\n\n## Logging Requirements\n\n- `info!` for start/end of each category.\n- `debug!` for per-check details and timings.\n- `warn!` for skipped categories and failed repairs.\n\n## Acceptance Criteria\n\n- [ ] `xf doctor` produces text + JSON output with stable schema\n- [ ] `--fix` only performs safe, idempotent repairs\n- [ ] Exit codes match summary severity\n- [ ] Unit + integration + E2E tests with detailed logs\n- [ ] No destructive operations or data loss\n","status":"closed","priority":2,"issue_type":"task","assignee":"PurpleDog","created_at":"2026-01-10T19:09:37.696048-05:00","created_by":"jemanuel","updated_at":"2026-01-10T23:18:10.109504-05:00","closed_at":"2026-01-10T23:18:10.109504-05:00","close_reason":"Implemented doctor CLI subcommand with text/JSON output, --fix flag, and performance benchmarks","dependencies":[{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.5","depends_on_id":"xf-11.4.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.4.6","title":"Add tests for doctor command","description":"## Goal\n\nComprehensive test coverage for `xf doctor` with deterministic fixtures and **actionable logging**.\n\n## Unit Tests (doctor.rs + helpers)\n\n### Archive Checks\n- Required file presence detection.\n- JS‚Äëwrapped JSON validity (window.YTD prefix stripping).\n- Duplicate IDs detection (warn, not crash).\n- Timestamp sanity (detect obviously bad timestamps).\n\n### Database Checks\n- `PRAGMA integrity_check` handling.\n- FTS5 integrity / orphan detection.\n- Table count mismatches (warn vs error).\n\n### Index Checks\n- Tantivy open/segment counts and doc counts.\n- Sample query latency check (warn if slow).\n\n### Performance Checks\n- Overall doctor runtime threshold on fixture data.\n- Each check returns `Pass/Warning/Error` and includes a suggestion when fixable.\n\n## Integration Tests\n\n- `xf doctor` text output includes all sections + summary.\n- `xf doctor --format json` validates stable schema.\n- Exit codes: 0 for pass/warn, 1 for error, 2 for critical.\n\n## E2E Script (tests/e2e/doctor_test.sh)\n\n- Runs against a **known fixture** archive/DB/index (healthy) and a purposely broken fixture.\n- Validates section headers, summary counts, and JSON schema via `jq`.\n- Logs timestamp, command, stdout/stderr, exit code, and duration.\n\n## Logging Requirements\n\n- `debug!` for check start/end + timing.\n- `info!` for check results.\n- `warn!` for fixable issues and `error!` for critical problems.\n\n## Acceptance Criteria\n\n- [ ] Unit tests cover each check category\n- [ ] Integration tests validate CLI output + schema\n- [ ] E2E script validates healthy + broken fixtures\n- [ ] Exit codes follow spec\n- [ ] Detailed logs included for failures\n","status":"closed","priority":2,"issue_type":"task","assignee":"PurpleDog","created_at":"2026-01-10T19:10:03.233128-05:00","created_by":"jemanuel","updated_at":"2026-01-10T23:27:32.391305-05:00","closed_at":"2026-01-10T23:27:32.391305-05:00","close_reason":"Added 17 unit tests and 4 e2e tests for doctor command. All 199 tests pass.","dependencies":[{"issue_id":"xf-11.4.6","depends_on_id":"xf-11.4","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.4.6","depends_on_id":"xf-11.4.5","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5","title":"Natural Language Date Filtering","description":"## Overview\n\nEnable human‚Äëfriendly date expressions for `--since/--until` with **clear, deterministic semantics**, local‚Äëtime interpretation, and UTC comparison.\n\n## Supported Expressions (must be documented + tested)\n\n### Absolute\n- ISO dates: `YYYY-MM-DD`, `YYYY-MM-DDTHH:MM:SSZ`, `YYYY-MM-DD HH:MM` (local).\n- Month/year: `Jan 2023`, `January 2023`, `2024-02` ‚Üí full month range.\n- Quarter: `Q1 2024`, `Q4 2022` ‚Üí quarter range.\n- Seasons: `spring 2023`, `summer 2023`, `fall 2023`, `winter 2023` (winter spans Dec‚ÄìFeb with year boundary).\n\n### Relative / Named\n- `today`, `yesterday`.\n- `last|past N days/weeks/months/years`.\n- `N days/weeks/months/years ago`.\n- `this month`, `last month`, `this year`, `last year`.\n- `weekend`, `weekdays` (range covering the most recent weekend/weekday block relative to base time).\n\n## Semantics\n\n- **Ranges:** convert to inclusive start/end instants (start of day 00:00:00 local, end 23:59:59 local), then store/compare in UTC.\n- **Point dates:** map to start of day (00:00:00 local) unless `prefer_end` is requested.\n- `--since` uses range **start**, `--until` uses range **end**.\n- Error messages must include the original input and 2‚Äì3 example formats.\n- `--verbose` echoes parsed range in UTC and local time for clarity.\n\n## Test Strategy\n\n- **Unit:** deterministic parsing via fixed base time; cover each expression type and boundary conditions (month lengths, leap years, DST transitions).\n- **Integration:** CLI parsing for `--since/--until` plus verbose output; ensure ISO fallback still works.\n- **E2E:** `tests/e2e/date_parse_test.sh` runs representative expressions, validates output + exit codes, logs command/stdout/stderr/duration.\n\n## Acceptance Criteria\n\n- [ ] Natural language expressions accepted for `--since/--until`.\n- [ ] ISO inputs remain supported with no regressions.\n- [ ] Clear error messages with examples.\n- [ ] Deterministic unit tests (no wall‚Äëclock dependency).\n- [ ] Integration + E2E tests with detailed logs.\n","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-10T19:10:11.839368-05:00","created_by":"jemanuel","updated_at":"2026-01-10T23:13:15.064574-05:00","closed_at":"2026-01-10T23:13:15.064574-05:00","close_reason":"All subtasks completed (date parser, relative dates, named periods, CLI integration, tests)","dependencies":[{"issue_id":"xf-11.5","depends_on_id":"xf-11","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.1","title":"Add date_parser.rs module with chrono-english","description":"## Goal\n\nCreate a new date_parser.rs module for parsing human-friendly date expressions.\n\n## Dependencies to Add (Cargo.toml)\n\n```toml\n[dependencies]\nchrono-english = \"0.1\"  # For natural language parsing\n```\n\n## New Module: src/date_parser.rs\n\n### Core Types\n\n```rust\nuse chrono::{DateTime, NaiveDate, NaiveTime, Utc, Local, Duration};\nuse chrono_english::{parse_date_string, Dialect};\n\n/// Result of parsing a date expression\n#[derive(Debug, Clone)]\npub enum ParsedDate {\n    /// A specific point in time\n    Point(DateTime\u003cUtc\u003e),\n    /// A date range (for expressions like \"January 2023\")\n    Range { start: DateTime\u003cUtc\u003e, end: DateTime\u003cUtc\u003e },\n}\n\nimpl ParsedDate {\n    /// Get the start of this date (for --from)\n    pub fn start(\u0026self) -\u003e DateTime\u003cUtc\u003e {\n        match self {\n            Self::Point(dt) =\u003e *dt,\n            Self::Range { start, .. } =\u003e *start,\n        }\n    }\n    \n    /// Get the end of this date (for --to)\n    pub fn end(\u0026self) -\u003e DateTime\u003cUtc\u003e {\n        match self {\n            Self::Point(dt) =\u003e *dt,\n            Self::Range { end, .. } =\u003e *end,\n        }\n    }\n}\n```\n\n### Main Parsing Function\n\n```rust\n/// Parse a human-readable date expression.\n///\n/// # Arguments\n/// *  - The date string to parse\n/// *  - If true, prefer end of period (for --to); otherwise start (for --from)\n///\n/// # Examples\n/// ```\n/// parse_human_date(\"yesterday\", false)?;       // Start of yesterday\n/// parse_human_date(\"last week\", true)?;        // End of last week\n/// parse_human_date(\"January 2023\", false)?;    // 2023-01-01\n/// parse_human_date(\"Q4 2022\", true)?;          // 2022-12-31\n/// ```\npub fn parse_human_date(input: \u0026str, prefer_end: bool) -\u003e Result\u003cParsedDate\u003e {\n    let input = input.trim().to_lowercase();\n    \n    // Try custom parsers first (quarters, seasons, months)\n    if let Some(parsed) = try_parse_quarter(\u0026input) {\n        return Ok(parsed);\n    }\n    if let Some(parsed) = try_parse_season(\u0026input) {\n        return Ok(parsed);\n    }\n    if let Some(parsed) = try_parse_month_year(\u0026input) {\n        return Ok(parsed);\n    }\n    \n    // Try chrono-english for natural language\n    let now = Local::now();\n    match parse_date_string(\u0026input, now, Dialect::Us) {\n        Ok(dt) =\u003e Ok(ParsedDate::Point(dt.with_timezone(\u0026Utc))),\n        Err(_) =\u003e {\n            // Try relative expressions manually\n            if let Some(parsed) = try_parse_relative(\u0026input) {\n                return Ok(parsed);\n            }\n            \n            Err(anyhow\\!(\"Could not parse date expression: '{}'\", input))\n        }\n    }\n}\n```\n\n### ISO Fallback\n\n```rust\n/// Try parsing as ISO format (YYYY-MM-DD)\npub fn try_parse_iso(input: \u0026str) -\u003e Option\u003cDateTime\u003cUtc\u003e\u003e {\n    NaiveDate::parse_from_str(input, \"%Y-%m-%d\")\n        .ok()\n        .map(|d| d.and_hms_opt(0, 0, 0).unwrap().and_utc())\n}\n\n/// Unified parser: try natural language, fall back to ISO\npub fn parse_date_flexible(input: \u0026str, prefer_end: bool) -\u003e Result\u003cDateTime\u003cUtc\u003e\u003e {\n    // Try human date first\n    if let Ok(parsed) = parse_human_date(input, prefer_end) {\n        return Ok(if prefer_end { parsed.end() } else { parsed.start() });\n    }\n    \n    // Fall back to ISO\n    try_parse_iso(input)\n        .ok_or_else(|| anyhow\\!(\"Could not parse '{}' as date\", input))\n}\n```\n\n## Logging\n\n- Log successful parses with tracing::debug\\!\n- Log fallback to ISO with tracing::trace\\!\n- Log parse failures with tracing::warn\\!\n\n## Acceptance Criteria\n\n- [ ] date_parser.rs module created\n- [ ] ParsedDate enum handles points and ranges\n- [ ] chrono-english integrated\n- [ ] ISO fallback works\n- [ ] Module exports parse_human_date and parse_date_flexible\n- [ ] Basic tests for common expressions\n- [ ] cargo check passes","status":"closed","priority":2,"issue_type":"task","assignee":"EmeraldLynx","created_at":"2026-01-10T19:10:25.649601-05:00","created_by":"jemanuel","updated_at":"2026-01-10T21:57:03.091927253-05:00","closed_at":"2026-01-10T21:57:03.091927253-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.5.1","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.2","title":"Add relative date expressions support","description":"Parse expressions: 'N days/weeks/months/years ago', 'last N days/weeks/months', 'past week', 'this month'. Handle timezone awareness. Compute relative to current date at query time.","status":"closed","priority":2,"issue_type":"task","assignee":"EmeraldLynx","created_at":"2026-01-10T19:10:26.178503-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:04:18.980393172-05:00","closed_at":"2026-01-10T22:04:18.980393172-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.5.2","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.2","depends_on_id":"xf-11.5.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.3","title":"Add named period expressions support","description":"Parse: 'January 2023', 'Q1 2024', 'summer 2022', 'weekend', 'weekdays'. Map to date ranges. Q1=Jan-Mar, Q2=Apr-Jun, Q3=Jul-Sep, Q4=Oct-Dec. Summer=Jun-Aug, etc.","status":"closed","priority":2,"issue_type":"task","assignee":"EmeraldLynx","created_at":"2026-01-10T19:10:26.655467-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:08:25.764000016-05:00","closed_at":"2026-01-10T22:08:25.764000016-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.5.3","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.3","depends_on_id":"xf-11.5.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.4","title":"Integrate date parser with CLI flags","description":"Modify --from and --to argument parsing in main.rs. Try natural language first, fall back to ISO format. Display parsed date in verbose mode for user confirmation. Update help text with examples.","status":"closed","priority":2,"issue_type":"task","assignee":"EmeraldLynx","created_at":"2026-01-10T19:10:27.16169-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:10:58.374983199-05:00","closed_at":"2026-01-10T22:10:58.374983199-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5.1","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5.2","type":"blocks","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.4","depends_on_id":"xf-11.5.3","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-11.5.5","title":"Add comprehensive tests for date parsing","description":"## Goal\n\nAdd comprehensive tests for the date parsing module, emphasizing **deterministic** behavior and realistic CLI flows.\n\n## Unit Tests (date_parser.rs)\n\n### Deterministic Base Time\n- Use a fixed base time via `parse_human_date_with_base`.\n- Never rely on wall‚Äëclock `now()` in assertions.\n\n### Coverage Matrix\n- **Absolute:** ISO date, RFC3339 datetime, local datetime, `Jan 2023`, `2024-02` month range, `Q1 2024`, `summer 2023`, `winter 2023` (year boundary).\n- **Relative:** `today`, `yesterday`, `last 7 days`, `3 days ago`, `this month`, `last month`, `this year`, `last year`.\n- **Named blocks:** `weekend`, `weekdays` relative to base time.\n- **Edge cases:** leap year Feb 29, month end boundaries, DST transitions (assert only on date boundaries, not hour offsets).\n- **Prefer end:** ensure `prefer_end = true` returns range end.\n\n### Error Handling\n- Invalid input yields a clear error that includes the original string.\n\n## Integration Tests (CLI)\n\n- `xf search --since \"Jan 2025\" --until \"Jan 2025\"` parses successfully.\n- `--verbose` echoes parsed timestamps.\n- Invalid expressions fail with a parse error.\n\n## E2E Script (tests/e2e/date_parse_test.sh)\n\n- Runs representative expressions.\n- Validates exit codes and verbose parse output.\n- Logs: timestamp, command, stdout/stderr, exit code, and duration.\n\n## Logging Requirements\n\n- `trace!` for parse path selection (relative vs named vs ISO fallback).\n- `debug!` for parsed start/end instants.\n- `warn!` for parse failures.\n\n## Acceptance Criteria\n\n- [ ] Deterministic unit tests for all expression classes\n- [ ] Edge cases (leap years, month boundaries, DST) covered\n- [ ] Integration tests for CLI parsing + verbose output\n- [ ] E2E script with detailed logs\n- [ ] No use of wall‚Äëclock time in assertions\n","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T19:10:41.07647-05:00","created_by":"jemanuel","updated_at":"2026-01-10T22:35:57.227581883-05:00","closed_at":"2026-01-10T22:33:46.509673387-05:00","close_reason":"Completed","dependencies":[{"issue_id":"xf-11.5.5","depends_on_id":"xf-11.5","type":"parent-child","created_at":"0001-01-01T00:00:00Z"},{"issue_id":"xf-11.5.5","depends_on_id":"xf-11.5.4","type":"blocks","created_at":"0001-01-01T00:00:00Z"}]}
{"id":"xf-9rf","title":"xf: Full System Investigation, Review, and Performance Optimization","description":"Goal:\n- Provide a self‚Äëcontained, end‚Äëto‚Äëend plan to understand the xf codebase, audit correctness/reliability, and execute a rigorous performance investigation + optimization program.\n\nScope:\n- Architecture walk‚Äëthrough (parser/storage/search/CLI/data flow).\n- Bug/risk review across critical paths.\n- Performance profiling on the provided dataset.\n\nRequired Outputs:\n- A structured task list with dependencies, estimates, and acceptance criteria.\n- Explicit equivalence oracles for correctness (before/after output checks).\n- A ranked optimization backlog using (Impact √ó Confidence) / Effort.\n- Minimal diffs: one performance lever per change, no unrelated refactors.\n- Rollback guidance for any risky change.\n\nPerformance Workflow:\n- Establish baseline metrics: p50/p95/p99 latency, throughput, peak RSS.\n- Profile CPU, allocation, and I/O hot paths.\n- Identify and validate bottlenecks with reproducible commands.\n- Add regression guardrails (bench scripts, perf test harness, or CI‚Äësafe checks).\n\nTesting \u0026 Verification:\n- Any fixes include unit/integration tests and an E2E or perf script with detailed logging.\n- E2E/perf scripts must log command, stdout/stderr, exit code, timing, and environment.\n\nConstraints:\n- Follow AGENTS.md (no file deletion, apply_patch for edits, etc.).\n- All changes remain local‚Äëonly and preserve output for identical inputs (proof sketch required).\n\nAcceptance:\n- Plan is self‚Äëcontained and executable without the original instructions.\n- Every proposed change includes test + validation strategy.\n","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-10T02:23:41.616748361-05:00","created_by":"ubuntu","updated_at":"2026-01-10T22:27:09.878997074-05:00"}
{"id":"xf-9rf.1","title":"Read AGENTS.md + README and capture constraints","description":"Goal:\n- Re-read AGENTS.md and README.md in full and summarize all constraints, workflows, and project goals in a durable form.\n\nWhy:\n- All subsequent investigation, fixes, and performance work must comply with AGENTS.md instructions (e.g., no file deletion, apply_patch usage).\n\nSteps:\n1) Read /data/projects/xf/AGENTS.md carefully (end-to-end). Extract: editing constraints, testing expectations, prohibited actions, perf methodology, and any doc references to best practices.\n2) Read /data/projects/xf/README.md carefully. Extract: project purpose, build/run commands, data formats, expected usage.\n3) Record a concise summary in a working note (or issue comment) that can be referenced without re-opening the source docs.\n\nAcceptance:\n- A clear summary that enumerates all operational constraints and expected workflows.\n- Any referenced best-practice guides or doc links are listed for later lookup.\n","notes":"Summary (AGENTS.md + README):\n- Hard safety rules: never delete files; avoid destructive commands (git reset --hard, rm -rf, git clean -fd) unless user explicitly provides exact command and confirms irreversible consequences. Use safe alternatives first. Document any approved destructive action verbatim.\n- Editing discipline: no script-based code changes; modify files manually; no file proliferation (no *_v2.* style files). Use apply_patch for edits; Cargo-only toolchain; Rust 2024 nightly; unsafe forbidden; use explicit dependency versions.\n- Testing/quality gates required after substantive changes: cargo check --all-targets, cargo clippy --all-targets -- -D warnings, cargo fmt --check; run cargo test and focused tests.\n- Project semantics: xf indexes local X archive data; privacy-first (no network in core runtime); parse JS-wrapped JSON window.YTD.*; Tantivy primary search; SQLite FTS5 fallback; preserve metadata (IDs/timestamps/counts) exactly; CLI flags must be truthful; JSON output shape must remain stable.\n- README highlights: CLI provides index/search commands; supports types filters, offsets/limits, output formats (json/csv/compact); data archive layout includes data/*.js and assets/; parsing strips JS prefix; storage in SQLite + Tantivy; env vars XF_DB/XF_INDEX override storage; default storage paths by OS; performance targets and benches documented; build/test via cargo; installation via script or cargo nightly.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:24:03.88540847-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:25:28.26120732-05:00","closed_at":"2026-01-10T02:25:28.261214513-05:00","dependencies":[{"issue_id":"xf-9rf.1","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:24:03.909918163-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.2","title":"Build architecture map + core workflow understanding","description":"Goal:\n- Produce a high-level architecture map of xf: major modules, data flow, and runtime workflows.\n\nContext:\n- Requires prior understanding of AGENTS.md/README constraints.\n\nSteps:\n1) Inventory key modules in /data/projects/xf/src (main.rs, parser.rs, search.rs, storage.rs, index.rs, etc.).\n2) Trace core flows: indexing path (input archive -\u003e parse -\u003e storage/index), search path (query parse -\u003e execution -\u003e result formatting).\n3) Note data boundaries: DB (SQLite), search index (Tantivy), file I/O paths, and serialization formats.\n4) Record a module graph (imports, public APIs, and cross-module calls).\n\nAcceptance:\n- A concise but complete architecture summary that identifies the main pipeline stages, storage layers, and execution entry points.\n- List of files/functions that are hot paths or correctness-critical.\n","notes":"Architecture map (high level):\n- Entry point: src/main.rs wires CLI (Cli/Commands) to command handlers: index/search/stats/tweet/list/export/config/update/completions.\n- CLI definitions: src/cli.rs declares subcommands + many flags (types/limit/offset/sort/since/until/replies/context/fields, etc.). Some flags are not yet wired in main.rs (noted for later correctness checks).\n- Core modules (src/lib.rs): parser, model, storage (SQLite), search (Tantivy), config/logging/perf/error.\n\nIndexing flow:\n1) xf index \u003carchive\u003e (main.rs cmd_index): validate archive/data/, resolve db/index paths, optionally clear existing data.\n2) ArchiveParser (parser.rs) parses JS-wrapped JSON (window.YTD.*) with rayon; parse_manifest -\u003e ArchiveInfo; parse_tweets/likes/dms/grok/followers/etc into model types.\n3) Storage::open (storage.rs) opens SQLite, sets pragmas, migrates schema, and stores data into normalized tables + FTS5 virtual tables.\n4) SearchEngine::open (search.rs) opens/creates Tantivy index; writer adds documents for each data type (id/text/text_prefix/type/created_at/metadata) and commits.\n\nSearch flow:\n1) xf search \u003cquery\u003e (main.rs cmd_search): open SearchEngine and Storage; map DataType -\u003e search::DocType; call SearchEngine::search with limit+offset.\n2) SearchEngine::search (search.rs) uses Tantivy QueryParser (text + prefix field) and optional type filter; collects TopDocs; builds SearchResult with highlights (snippet generator) and metadata (stored JSON).\n3) main.rs formats results to json/json pretty/csv/compact/text (with highlight -\u003e ANSI conversion in print_result).\n\nData boundaries:\n- Input: archive files under \u003carchive\u003e/data/*.js with JS prefix; parser tolerates whitespace and trailing semicolons.\n- Storage: SQLite schema for tweets/likes/dms/grok + FTS5 tables; metadata in archive_info/meta tables.\n- Search index: Tantivy index under XF_INDEX; schema fields include id, text, text_prefix (prefix matching), type, created_at, metadata.\n\nHot paths / correctness-critical:\n- parser.rs: parse_js_file, parse_* per datatype; date parsing; numeric parsing; message aggregation.\n- search.rs: schema construction, index_* methods, query parsing, snippet generation.\n- storage.rs: schema/migrations + store_* insertions and FTS5 updates.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:25:50.678053409-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:27:35.671086605-05:00","closed_at":"2026-01-10T02:27:35.671094189-05:00","dependencies":[{"issue_id":"xf-9rf.2","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:25:50.679360841-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.2","depends_on_id":"xf-9rf.1","type":"blocks","created_at":"2026-01-10T02:26:00.328123138-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.3","title":"Random deep-dive file investigations + flow traces","description":"Goal:\n- Randomly sample code files and deeply trace execution flows through imports/callers to build intuitive understanding beyond the main path.\n\nSteps:\n1) Use rg/rg --files to list candidate files; pick a random subset across modules (parser, search, storage, CLI, tests, benches).\n2) For each file: identify entry points, key data structures, and how its functions are called from other modules.\n3) Trace at least 2 multi-hop flows (file A -\u003e file B -\u003e file C) with notes on inputs/outputs and invariants.\n4) Record any surprising behavior or potential correctness pitfalls discovered during tracing.\n\nAcceptance:\n- At least 5 files deeply analyzed with call/flow notes.\n- Flow traces identify caller/callee relationships and data transformations across module boundaries.\n","notes":"Deep-dive notes (random file sampling + flow traces):\n- src/main.rs: command dispatcher; cmd_index -\u003e ArchiveParser::parse_* -\u003e Storage::store_* + SearchEngine::index_* -\u003e writer.commit + reload. cmd_search -\u003e SearchEngine::search -\u003e output formatting; cmd_stats/tweet/list/export/config/update are stubs/partial (yellow warnings) in main.\n- src/parser.rs: read_data_file -\u003e parse_js_file (split on first '=') -\u003e serde_json Value -\u003e parse_* into model structs. Uses rayon par_iter for JSON arrays. parse_manifest reads manifest.js and normalizes dates/numeric fields.\n- src/search.rs: schema defines id/text/text_prefix/type/created_at/metadata. index_* builds Tantivy docs; search() builds QueryParser over text+text_prefix, optional type filters with BooleanQuery; SnippetGenerator for highlights; SearchResult assembled with metadata JSON.\n- src/storage.rs: opens SQLite with WAL + perf pragmas; migrate/create schema; store_* inserts per model and maintains FTS5 tables; get_tweet/stats queries. Data normalized; DM messages linked by conversation_id.\n- src/cli.rs: defines flags (types, limit/offset/sort, since/until, replies/context/fields). Several flags are not wired in main.rs yet (potential correctness/UX gap for later bug-hunt).\n- src/config.rs/logging.rs/perf.rs/error.rs: full-featured config/logging/perf budgets/custom errors defined but currently not integrated into main.rs (latent functionality).\n- benches/search_perf.rs: synthetic benchmarks invoke SearchEngine and Storage directly; provides baseline for search/index/storage; uses TempDir and synthetic models.\n\nMulti-hop flow traces:\n1) xf index -\u003e main.rs cmd_index -\u003e ArchiveParser::parse_tweets -\u003e model::Tweet -\u003e Storage::store_tweets (SQLite inserts + FTS5) -\u003e SearchEngine::index_tweets (Tantivy doc) -\u003e commit + reload.\n2) xf search -\u003e main.rs cmd_search -\u003e SearchEngine::search -\u003e QueryParser + TopDocs -\u003e SnippetGenerator -\u003e model::SearchResult -\u003e main.rs output formatter (json/csv/text/compact).\n\nPotential pitfalls observed (for later bug review):\n- CLI flags defined but not used (sort/since/until/replies/context/fields/threads etc.).\n- Config/logging/perf modules unused; custom error types largely bypassed by anyhow usage in main.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:27:54.059223458-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:29:46.586423572-05:00","closed_at":"2026-01-10T02:29:46.586430315-05:00","dependencies":[{"issue_id":"xf-9rf.3","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:27:54.061604543-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.3","depends_on_id":"xf-9rf.2","type":"blocks","created_at":"2026-01-10T02:28:02.980609232-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.4","title":"Review prior agent changes for correctness and regressions","description":"Goal:\n- Audit code written by other agents (not limited to latest commits) to identify bugs, inefficiencies, or regressions.\n\nSteps:\n1) Use git log/diff to locate code changes authored by agents; scan both recent and older edits.\n2) For each change: verify logic vs requirements; look for edge cases, incorrect assumptions, or performance pitfalls.\n3) Cross-check with tests/benches; add targeted tests if a bug is found.\n4) Record root-cause analysis for any issues discovered.\n\nAcceptance:\n- A documented list of reviewed changes, with issues (if any) and their root causes.\n- Any fixes are minimal, scoped, and comply with AGENTS.md editing rules.\n","notes":"Reviewed uncommitted/agent-origin diffs:\n- src/main.rs: cmd_export implemented (JSON/JSONL/CSV) + csv_escape/format_export helpers; uses Storage::get_all_* and writes to file or stdout. No correctness regressions spotted; minor risk: CSV header order uses serde_json map iteration; should be deterministic for struct field order.\n- src/storage.rs: new get_all_tweets/likes/dms/followers/following; migrate now takes \u0026self; json fields parsed with serde_json::from_str (stored values are always serialized JSON, so NULL risk low). get_all_dms drops conversation_id because DirectMessage model lacks it.\n- src/parser.rs: parse_js_file uses splitn(2,'=') and trims; parse_i64 added for sizeBytes/favorite/retweet counts; doc comments expanded. Behavior aligns with JS wrapper format and should be more tolerant.\n- src/config.rs/logging.rs/error.rs/lib.rs/cli.rs: mostly must_use/const/default/allow annotations and doc tweaks; no functional changes.\n\nPotential concerns (not confirmed bugs):\n- Export of DMs lacks conversation_id context (DirectMessage model does not carry it). If CLI intended to export conversations, model/API may need extension.\n- Several CLI flags remain unimplemented in main.rs (sort/since/until/replies/context/fields). Not introduced by these diffs, but a correctness/UX gap for later bug hunt.\n\nNo immediate regressions found that warrant code changes at this step.\n","status":"closed","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:30:01.076013082-05:00","created_by":"ubuntu","updated_at":"2026-01-10T02:34:02.360639254-05:00","closed_at":"2026-01-10T02:34:02.360646728-05:00","dependencies":[{"issue_id":"xf-9rf.4","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:30:01.078025102-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.4","depends_on_id":"xf-9rf.2","type":"blocks","created_at":"2026-01-10T02:30:10.794020969-05:00","created_by":"ubuntu"}]}
{"id":"xf-9rf.5","title":"Fresh-eyes bug hunt + correctness fixes","description":"Goal:\n- Conduct a careful, methodical bug hunt across the codebase and fix issues with minimal, test‚Äëbacked diffs.\n\nSteps:\n1) Re‚Äëread critical paths (parser/search/storage/CLI) with fresh eyes.\n2) Identify correctness risks: unchecked assumptions, lossy conversions, silent fallbacks, bad error handling.\n3) For each issue:\n   - Document root cause, impact, and reproduction steps.\n   - Add a failing unit/integration test **before** the fix when feasible.\n4) Apply minimal fix using `apply_patch`; no unrelated refactors.\n\nTesting \u0026 Verification:\n- Add unit tests for each fix; add or extend E2E scripts when behavior changes are user‚Äëvisible.\n- E2E scripts must include detailed logs (command, stdout/stderr, exit code, timing).\n- Run `cargo check`, `cargo clippy -D warnings`, `cargo fmt --check`, and relevant tests.\n\nAcceptance:\n- All fixes include tests and root‚Äëcause notes.\n- No regressions; quality gates recorded.\n- Any behavioral changes documented with before/after examples.\n","notes":"Fresh-eyes pass: made stats JSON optional fields skip nulls and filtered empty items in top_counts to avoid blank hashtag/mention entries. Re-ran fmt/check/clippy.","status":"in_progress","priority":2,"issue_type":"task","assignee":"ubuntu","created_at":"2026-01-10T02:34:40.255520526-05:00","created_by":"ubuntu","updated_at":"2026-01-10T22:27:24.900162896-05:00","dependencies":[{"issue_id":"xf-9rf.5","depends_on_id":"xf-9rf","type":"parent-child","created_at":"2026-01-10T02:34:40.256783946-05:00","created_by":"ubuntu"},{"issue_id":"xf-9rf.5","depends_on_id":"xf-9rf.3","type":"blocks","created_at":"2026-01-10T02:34:50.320708583-05:00","created_by":"ubuntu"}]}
